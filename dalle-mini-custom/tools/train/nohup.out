WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adam', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/136 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 136/136 [00:00<00:00, 19515.73it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-c2c8bb870fb1f26c
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run sandy-wood-22
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/1llhu8ku
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220305_211333-1llhu8ku
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpc7zgpt5t/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpavwjklho/added_tokens.json. We won't load it.
loading file /tmp/tmpavwjklho/vocab.json
loading file /tmp/tmpavwjklho/merges.txt
loading file /tmp/tmpavwjklho/tokenizer.json
loading file None
loading file /tmp/tmpavwjklho/special_tokens_map.json
loading file /tmp/tmpavwjklho/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-05 21:15:32.483381: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-05 21:16:00.965595: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [01:49, 109.28s/it][A
Training...: 2it [01:51, 46.38s/it] [A
Training...: 3it [01:53, 26.27s/it][A
Training...: 4it [01:56, 16.91s/it][A
Training...: 5it [01:58, 11.66s/it][A
Training...: 6it [02:01,  8.49s/it][A
Training...: 7it [02:03,  6.60s/it][A
Training...: 8it [02:06,  5.31s/it][A
Training...: 9it [02:08,  4.38s/it][A
Training...: 10it [02:11,  3.75s/it][A
Training...: 11it [02:13,  3.32s/it][A
Training...: 12it [02:16,  3.08s/it][A
Training...: 13it [02:18,  2.86s/it][A
Training...: 14it [02:20,  2.70s/it][A
Training...: 15it [02:23,  2.59s/it][A
Training...: 16it [02:25,  2.58s/it][A
Training...: 17it [02:27,  2.51s/it][A
Training...: 18it [02:30,  2.46s/it][A
Training...: 19it [02:32,  2.42s/it][A
Training...: 20it [02:35,  2.45s/it][A
Training...: 21it [02:37,  2.42s/it][A
Training...: 22it [02:39,  2.40s/it][A
Training...: 23it [02:42,  2.38s/it][A
Training...: 24it [02:44,  2.43s/it][A
Training...: 25it [02:47,  2.41s/it][A
Training...: 26it [02:49,  2.39s/it][A
Training...: 27it [02:51,  2.38s/it][A
Training...: 28it [02:54,  2.43s/it][A
Training...: 29it [02:56,  2.40s/it][A
Training...: 30it [02:59,  2.39s/it][A
Training...: 31it [03:01,  2.37s/it][A
Training...: 32it [03:03,  2.43s/it][A
Training...: 33it [03:06,  2.40s/it][A
Training...: 34it [03:08,  2.39s/it][A
Training...: 35it [03:10,  2.38s/it][A
Training...: 36it [03:13,  2.43s/it][A
Training...: 37it [03:15,  2.41s/it][A
Training...: 38it [03:18,  2.39s/it][A
Training...: 39it [03:20,  2.38s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 40it [03:49, 10.45s/it][A
Training...: 41it [03:52,  8.06s/it][A
Training...: 42it [03:54,  6.40s/it][A
Training...: 43it [03:57,  5.25s/it][A
Training...: 44it [03:59,  4.38s/it][A
Training...: 45it [04:02,  3.77s/it][A
Training...: 46it [04:04,  3.34s/it][A
Training...: 47it [04:07,  3.11s/it][A
Training...: 48it [04:09,  2.88s/it][A
train_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.961752, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.001458, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(10.149376, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(9.04743, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.76446, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.033023, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.46377, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.2344046, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.161868, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.1844444, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.075408, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.938633, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9615717, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.000311, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.958824, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9549, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.903781, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.894009, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.855961, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8753843, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.872662, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.875169, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8627405, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.860415, dtype=float32)}
train_step here is:   2Training...: 49it [04:11,  2.72s/it][A5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8401465, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.816669, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8392944, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8136244, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.800573, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.819977, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8037252, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.794099, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8079967, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8057656, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7881165, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7901716, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7931046, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8012595, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.781406, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.783432, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7519827, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.770751, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7657175, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.772023, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.742738, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7718596, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7564178, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7753687, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7473893, dtype=float32)}
Training...: 50it [04:14,  2.61s/it][A
Training...: 51it [04:16,  2.60s/it][A
Training...: 52it [04:19,  2.52s/it][A
Training...: 53it [04:21,  2.58s/it][A
Training...: 54it [04:24,  2.51s/it][A
Training...: 55it [04:26,  2.52s/it][A
Training...: 56it [04:28,  2.47s/it][A
Training...: 57it [04:31,  2.43s/it][A
Training...: 58it [04:33,  2.41s/it][A
Training...: 59it [04:36,  2.45s/it][A
Training...: 60it [04:38,  2.42s/it][A
Training...: 61it [04:40,  2.40s/it][A
Training...: 62it [04:43,  2.39s/it][A
Training...: 63it [04:45,  2.43s/it][A
Training...: 64it [04:48,  2.41s/it][A
Training...: 65it [04:50,  2.39s/it][A
Training...: 66it [04:52,  2.38s/it][A
Training...: 67it [04:55,  2.43s/it][A
Training...: 68it [04:57,  2.41s/it][A
Training...: 69it [05:00,  2.39s/it][A
Training...: 70it [05:02,  2.38s/it][A
Training...: 71it [05:05,  2.44s/it][A
Training...: 72it [05:07,  2.41s/it][A
Training...: 73it [05:09,  2.39s/it][A
Training...: 74it [05:12,  2.38s/it][A
Training...: 75it [05:14,  2.43s/it][A
Training...: 76it [05:17,  2.41s/it][A
Training...: 77it [05:19,  2.39s/it][A
Training...: 78it [05:21,  2.38s/it][A
Training...: 79it [05:24,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 80it [05:52, 10.09s/it][A
Training...: 81it [05:54,  7.81s/it][A
Training...: 82it [05:57,  6.22s/it][A
Training...: 83it [05:59,  5.12s/it][A
Training...: 84it [06:02,  4.29s/it][A
Training...: 85it [06:04,  3.71s/it][A
Training...: 86it [06:07,  3.36s/it][A
Training...: 87it [06:09,  3.06s/it][A
Training...: 88it [06:11,  2.84s/it][A
Training...: 89it [06:14,  2.70s/it][A
Training...: 90it [06:16,  2.65s/it][A
Training...: 91it [06:19,  2.56s/it][A
Training...: 92it [06:21,  2.50s/it][A
Training...: 93it [06:23,  2.45s/it][A
Training...: 94it [06:26,  2.48s/it][A
Training...: 95it [06:28,  2.44s/it][A
Training...: 96it [06:30,  2.41s/it][A
Training...: 97it [06:33,  2.40s/it][A
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7356544, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7670493, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.752103, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7409267, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7408905, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.75125, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.758872, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.742134, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7328205, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7513137, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7454786, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734774, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7397428, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.740467, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7296844, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72339, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.731147, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7238884, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7310305, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.73246, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7277317, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.712082, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.717198, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722803, dtype=float32)}
trai
Training...: 98it [06:36,  2.55s/it][An_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7314405, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7160254, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.740878, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7440786, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7261624, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7189865, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726808, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7259564, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.732769, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722086, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716916, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.717587, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722253, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730508, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724133, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.717992, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713828, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727531, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7212873, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7215624, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.71397, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.711856, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.720701, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7353125, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7203197, dtype=float32)}
Training...: 99it [06:38,  2.49s/it][A
Training...: 100it [06:40,  2.45s/it][A
Training...: 101it [06:43,  2.42s/it][A
Training...: 102it [06:45,  2.45s/it][A
Training...: 103it [06:48,  2.42s/it][A
Training...: 104it [06:50,  2.40s/it][A
Training...: 105it [06:52,  2.39s/it][A
Training...: 106it [06:55,  2.44s/it][A
Training...: 107it [06:57,  2.41s/it][A
Training...: 108it [07:00,  2.39s/it][A
Training...: 109it [07:02,  2.38s/it][A
Training...: 110it [07:05,  2.43s/it][A
Training...: 111it [07:07,  2.41s/it][A
Training...: 112it [07:09,  2.39s/it][A
Training...: 113it [07:12,  2.38s/it][A
Training...: 114it [07:14,  2.42s/it][A
Training...: 115it [07:16,  2.40s/it][A
Training...: 116it [07:19,  2.39s/it][A
Training...: 117it [07:21,  2.38s/it][A
Training...: 118it [07:24,  2.40s/it][A
Training...: 119it [07:26,  2.39s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 120it [07:53,  9.85s/it][A
Training...: 121it [07:56,  7.80s/it][A
Training...: 122it [07:59,  6.25s/it][A
Training...: 123it [08:01,  5.08s/it][A
Training...: 124it [08:04,  4.26s/it][A
Training...: 125it [08:06,  3.69s/it][A
Training...: 126it [08:08,  3.32s/it][A
Training...: 127it [08:11,  3.03s/it][A
Training...: 128it [08:13,  2.82s/it][A
Training...: 129it [08:16,  2.71s/it][A
Training...: 130it [08:18,  2.61s/it][A
Training...: 131it [08:20,  2.53s/it][A
Training...: 132it [08:23,  2.48s/it][A
Training...: 133it [08:25,  2.47s/it][A
Training...: 134it [08:27,  2.43s/it][A
Training...: 135it [08:30,  2.41s/it][A
Training...: 136it [08:32,  2.39s/it][A
                                     [A                                          Epoch ... (1/3):   0% 0/3 [08:32<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7355714, dtype=float32)}
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728123, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7297606, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.729976, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7164392, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7115083, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7126045, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7160854, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.707484, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.737152, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7093916, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.733725, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728607, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.701902, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7198772, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7168083, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715667, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713671, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.720487, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7373476, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7350225, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7247276, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714308, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7196045, dtype=float32)}
train_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72287, dtype=float32)}
train_step here is:   124
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.708971, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726488, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722756, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.709882, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.701295, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713975, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7352867, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7220755, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7141237, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7163973, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723357, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7096176, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721385, dtype=float32)}
Epoch... (1/3 | Loss: 6.7213850021362305, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:12, 12.98s/it][A[A

Evaluating...: 4it [00:13,  2.50s/it][A[A

Evaluating...: 7it [00:13,  1.19s/it][A[A

Evaluating...: 9it [00:13,  1.24it/s][A[A

Evaluating...: 11it [00:13,  1.76it/s][A[A

Evaluating...: 13it [00:13,  2.47it/s][A[A

Evaluating...: 15it [00:13,  3.36it/s][A[A

Evaluating...: 17it [00:14,  4.03it/s][A[A

Evaluating...: 19it [00:14,  4.87it/s][A[A

Evaluating...: 21it [00:14,  5.86it/s][A[A

Evaluating...: 23it [00:14,  6.58it/s][A[A

Evaluating...: 25it [00:14,  7.33it/s][A[A

Evaluating...: 27it [00:15,  7.96it/s][A[A

Evaluating...: 29it [00:15,  9.24it/s][A[A

Evaluating...: 31it [00:15, 10.80it/s][A[A

Evaluating...: 33it [00:15,  9.26it/s][A[A

Evaluating...: 35it [00:15, 10.89it/s][A[A

Evaluating...: 37it [00:15, 12.10it/s][A[A

Evaluating...: 39it [00:15, 13.23it/s][A[A

Evaluating...: 41it [00:16, 14.04it/s][A[A

Evaluating...: 43it [00:16, 14.91it/s][A[A

Evaluating...: 45it [00:16, 13.35it/s][A[A

Evaluating...: 47it [00:16, 10.45it/s][A[A

Evaluating...: 50it [00:16, 11.75it/s][A[A

Evaluating...: 52it [00:17, 11.08it/s][A[A

Evaluating...: 54it [00:17, 11.08it/s][A[A

Evaluating...: 56it [00:17, 12.19it/s][A[A

Evaluating...: 58it [00:17, 13.21it/s][A[A

                                      [A[A                                          Epoch ... (1/3):   0% 0/3 [08:50<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 6.717380523681641):  33% 1/3 [09:18<18:36, 558.19s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (1/3 | Eval Loss: 6.717380523681641)

Training...: 1it [00:03,  3.52s/it][A
Training...: 2it [00:05,  2.83s/it][A
Training...: 3it [00:08,  2.61s/it][A
Training...: 4it [00:11,  2.71s/it][A
Training...: 5it [00:13,  2.58s/it][A
Training...: 6it [00:15,  2.50s/it][A
Training...: 7it [00:18,  2.45s/it][A
Training...: 8it [00:20,  2.48s/it][A
Training...: 9it [00:23,  2.44s/it][A
Training...: 10it [00:25,  2.41s/it][A
Training...: 11it [00:27,  2.39s/it][A
Training...: 12it [00:30,  2.45s/it][A
Training...: 13it [00:32,  2.42s/it][A
Training...: 14it [00:35,  2.40s/it][A
Training...: 15it [00:37,  2.38s/it][A
Training...: 16it [00:39,  2.44s/it][A
Training...: 17it [00:42,  2.41s/it][A
Training...: 18it [00:44,  2.39s/it][A
Training...: 19it [00:46,  2.38s/it][A
Training...: 20it [00:49,  2.42s/it][A
Training...: 21it [00:51,  2.40s/it][A
Training...: 22it [00:54,  2.39s/it][A
Training...: 23it [00:56,  2.38s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 24it [01:27, 10.94s/it][A
Training...: 25it [01:29,  8.41s/it][A
Training...: 26it [01:32,  6.59s/it][A
Training...: 27it [01:34,  5.39s/it][A
Training...: 28it [01:37,  4.54s/it][A
Training...: 29it [01:39,  3.88s/it][A
Training...: 30it [01:42,  3.42s/it][A
Training...: 31it [01:44,  3.10s/it][A
Training...: 32it [01:47,  2.93s/it][A
Training...: 33it [01:49,  2.76s/it][A
Training...: 34it [01:51,  2.64s/it][A
Training...: 35it [01:54,  2.55s/it][A
Training...: 36it [01:56,  2.55s/it][A
Training...: 37it [01:58,  2.49s/it][A
Training...: 38it [02:01,  2.45s/it][A
Training...: 39it [02:03,  2.42s/it][A
Training...: 40it [02:06,  2.46s/it][A
Training...: 41it [02:08,  2.43s/it][A
Training...: 42it [02:10,  2.40s/it][A
Training...: 43it [02:13,  2.45s/it][A
Training...: 44it [02:15,  2.42s/it][A
Training...: 45it [02:18,  2.40s/it][A
Training...: 46it [02:20,  2.38s/it][A
Training...: 47it [02:23,  2.44s/it][A
Training...: 48it [02:25,  2.41s/it][Atrain_step here is:   137
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7037306, dtype=float32)}
train_step here is:   138
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710236, dtype=float32)}
train_step here is:   139
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7021775, dtype=float32)}
train_step here is:   140
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721695, dtype=float32)}
train_step here is:   141
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.706747, dtype=float32)}
train_step here is:   142
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7375135, dtype=float32)}
train_step here is:   143
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7112713, dtype=float32)}
train_step here is:   144
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7081413, dtype=float32)}
train_step here is:   145
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726613, dtype=float32)}
train_step here is:   146
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7159767, dtype=float32)}
train_step here is:   147
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7171288, dtype=float32)}
train_step here is:   148
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7210116, dtype=float32)}
train_step here is:   149
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715482, dtype=float32)}
train_step here is:   150
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.712288, dtype=float32)}
train_step here is:   151
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7103453, dtype=float32)}
train_step here is:   152
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726911, dtype=float32)}
train_step here is:   153
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.725957, dtype=float32)}
train_step here is:   154
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7050962, dtype=float32)}
train_step here is:   155
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728201, dtype=float32)}
train_step here is:   156
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7157745, dtype=float32)}
train_step here is:   157
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.707166, dtype=float32)}
train_step here is:   158
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7153983, dtype=float32)}
train_step here is:   159
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724038, dtype=float32)}
train_step here is:   160
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7111
Training...: 49it [02:27,  2.39s/it][A71, dtype=float32)}
train_step here is:   161
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710844, dtype=float32)}
train_step here is:   162
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7081127, dtype=float32)}
train_step here is:   163
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.71432, dtype=float32)}
train_step here is:   164
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721074, dtype=float32)}
train_step here is:   165
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716085, dtype=float32)}
train_step here is:   166
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734571, dtype=float32)}
train_step here is:   167
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.70904, dtype=float32)}
train_step here is:   168
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721499, dtype=float32)}
train_step here is:   169
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7385283, dtype=float32)}
train_step here is:   170
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7120733, dtype=float32)}
train_step here is:   171
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719806, dtype=float32)}
train_step here is:   172
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.725731, dtype=float32)}
train_step here is:   173
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713845, dtype=float32)}
train_step here is:   174
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7293344, dtype=float32)}
train_step here is:   175
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724604, dtype=float32)}
train_step here is:   176
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716695, dtype=float32)}
train_step here is:   177
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.706193, dtype=float32)}
train_step here is:   178
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.742693, dtype=float32)}
train_step here is:   179
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.706008, dtype=float32)}
train_step here is:   180
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734981, dtype=float32)}
train_step here is:   181
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7216716, dtype=float32)}
train_step here is:   182
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722934, dtype=float32)}
train_step here is:   183
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7047696, dtype=float32)}
train_step here is:   184
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7182775, dtype=float32)}
train_step here is:   185
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7283707, dtype=float32)}
Training...: 50it [02:30,  2.38s/it][A
Training...: 51it [02:32,  2.43s/it][A
Training...: 52it [02:35,  2.41s/it][A
Training...: 53it [02:37,  2.39s/it][A
Training...: 54it [02:39,  2.38s/it][A
Training...: 55it [02:42,  2.43s/it][A
Training...: 56it [02:44,  2.41s/it][A
Training...: 57it [02:47,  2.39s/it][A
Training...: 58it [02:49,  2.38s/it][A
Training...: 59it [02:51,  2.43s/it][A
Training...: 60it [02:54,  2.41s/it][A
Training...: 61it [02:56,  2.39s/it][A
Training...: 62it [02:58,  2.38s/it][A
Training...: 63it [03:01,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 64it [03:32, 10.88s/it][A
Training...: 65it [03:34,  8.45s/it][A
Training...: 66it [03:37,  6.62s/it][A
Training...: 67it [03:40,  5.47s/it][A
Training...: 68it [03:42,  4.53s/it][A
Training...: 69it [03:44,  3.88s/it][A
Training...: 70it [03:47,  3.42s/it][A
Training...: 71it [03:49,  3.16s/it][A
Training...: 72it [03:51,  2.91s/it][A
Training...: 73it [03:54,  2.74s/it][A
Training...: 74it [03:56,  2.63s/it][A
Training...: 75it [03:59,  2.60s/it][A
Training...: 76it [04:01,  2.53s/it][A
Training...: 77it [04:03,  2.47s/it][A
Training...: 78it [04:06,  2.44s/it][A
Training...: 79it [04:08,  2.47s/it][A
Training...: 80it [04:11,  2.43s/it][A
Training...: 81it [04:13,  2.41s/it][A
Training...: 82it [04:15,  2.39s/it][A
Training...: 83it [04:18,  2.44s/it][A
Training...: 84it [04:20,  2.41s/it][A
Training...: 85it [04:23,  2.39s/it][A
Training...: 86it [04:25,  2.44s/it][A
Training...: 87it [04:27,  2.42s/it][A
Training...: 88it [04:30,  2.40s/it][A
Training...: 89it [04:32,  2.38s/it][A
Training...: 90it [04:35,  2.43s/it][A
Training...: 91it [04:37,  2.41s/it][A
Training...: 92it [04:39,  2.39s/it][A
Training...: 93it [04:42,  2.38s/it][A
Training...: 94it [04:44,  2.43s/it][A
Training...: 95it [04:47,  2.40s/it][A
Training...: 96it [04:49,  2.39s/it][A
Training...: 97it [04:51,  2.38s/it][A
train_step here is:   186
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730606, dtype=float32)}
train_step here is:   187
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7176437, dtype=float32)}
train_step here is:   188
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723733, dtype=float32)}
train_step here is:   189
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714939, dtype=float32)}
train_step here is:   190
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7110124, dtype=float32)}
train_step here is:   191
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.708296, dtype=float32)}
train_step here is:   192
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7231245, dtype=float32)}
train_step here is:   193
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.711918, dtype=float32)}
train_step here is:   194
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7068653, dtype=float32)}
train_step here is:   195
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7075214, dtype=float32)}
train_step here is:   196
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.733182, dtype=float32)}
train_step here is:   197
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7166758, dtype=float32)}
train_step here is:   198
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7189703, dtype=float32)}
train_step here is:   199
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724308, dtype=float32)}
train_step here is:   200
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714156, dtype=float32)}
train_step here is:   201
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7188377, dtype=float32)}
train_step here is:   202
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7127266, dtype=float32)}
train_step here is:   203
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710768, dtype=float32)}
train_step here is:   204
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721758, dtype=float32)}
train_step here is:   205
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710759, dtype=float32)}
train_step here is:   206
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7218337, dtype=float32)}
train_step here is:   207
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7146196, dtype=float32)}
train_step here is:   208
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730066, dtype=float32)}
train_step here is:   209
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724
Training...: 98it [04:54,  2.43s/it][A936, dtype=float32)}
train_step here is:   210
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.703287, dtype=float32)}
train_step here is:   211
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.702715, dtype=float32)}
train_step here is:   212
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719285, dtype=float32)}
train_step here is:   213
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7186556, dtype=float32)}
train_step here is:   214
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734022, dtype=float32)}
train_step here is:   215
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7223797, dtype=float32)}
train_step here is:   216
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726009, dtype=float32)}
train_step here is:   217
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7085385, dtype=float32)}
train_step here is:   218
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7091913, dtype=float32)}
train_step here is:   219
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7144103, dtype=float32)}
train_step here is:   220
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734641, dtype=float32)}
train_step here is:   221
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722792, dtype=float32)}
train_step here is:   222
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.701956, dtype=float32)}
train_step here is:   223
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7255354, dtype=float32)}
train_step here is:   224
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723809, dtype=float32)}
train_step here is:   225
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7237473, dtype=float32)}
train_step here is:   226
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7107425, dtype=float32)}
train_step here is:   227
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7187223, dtype=float32)}
train_step here is:   228
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.70903, dtype=float32)}
train_step here is:   229
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7224727, dtype=float32)}
train_step here is:   230
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7092857, dtype=float32)}
train_step here is:   231
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7244396, dtype=float32)}
train_step here is:   232
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.725214, dtype=float32)}
train_step here is:   233
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727661, dtype=float32)}
train_step here is:   234
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716259, dtype=float32)}
Training...: 99it [04:57,  2.53s/it][A
Training...: 100it [04:59,  2.48s/it][A
Training...: 101it [05:01,  2.44s/it][A
Training...: 102it [05:04,  2.47s/it][A
Training...: 103it [05:06,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 104it [05:36, 10.47s/it][A
Training...: 105it [05:38,  8.22s/it][A
Training...: 106it [05:41,  6.52s/it][A
Training...: 107it [05:44,  5.32s/it][A
Training...: 108it [05:46,  4.45s/it][A
Training...: 109it [05:48,  3.82s/it][A
Training...: 110it [05:51,  3.44s/it][A
Training...: 111it [05:53,  3.11s/it][A
Training...: 112it [05:56,  2.88s/it][A
Training...: 113it [05:58,  2.72s/it][A
Training...: 114it [06:00,  2.66s/it][A
Training...: 115it [06:03,  2.57s/it][A
Training...: 116it [06:05,  2.50s/it][A
Training...: 117it [06:07,  2.46s/it][A
Training...: 118it [06:10,  2.46s/it][A
Training...: 119it [06:12,  2.43s/it][A
Training...: 120it [06:15,  2.40s/it][A
Training...: 121it [06:17,  2.39s/it][A
Training...: 122it [06:19,  2.41s/it][A
Training...: 123it [06:22,  2.39s/it][A
Training...: 124it [06:24,  2.38s/it][A
Training...: 125it [06:27,  2.37s/it][A
Training...: 126it [06:29,  2.40s/it][A
Training...: 127it [06:31,  2.38s/it][A
Training...: 128it [06:34,  2.37s/it][A
Training...: 129it [06:36,  2.40s/it][A
Training...: 130it [06:38,  2.39s/it][A
Training...: 131it [06:41,  2.38s/it][A
Training...: 132it [06:43,  2.37s/it][A
Training...: 133it [06:46,  2.40s/it][A
Training...: 134it [06:48,  2.38s/it][A
Training...: 135it [06:50,  2.37s/it][A
Training...: 136it [06:53,  2.37s/it][A
                                     [A                                                                                 Epoch... (1/3 | Eval Loss: 6.717380523681641):  33% 1/3 [16:11<18:36, 558.19s/it]

Evaluating...: 0it [00:00, ?it/s][A[A
train_step here is:   235
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730074, dtype=float32)}
train_step here is:   236
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.71898, dtype=float32)}
train_step here is:   237
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.709436, dtype=float32)}
train_step here is:   238
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7284365, dtype=float32)}
train_step here is:   239
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7089996, dtype=float32)}
train_step here is:   240
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7158556, dtype=float32)}
train_step here is:   241
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730336, dtype=float32)}
train_step here is:   242
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7094564, dtype=float32)}
train_step here is:   243
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.704055, dtype=float32)}
train_step here is:   244
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7296505, dtype=float32)}
train_step here is:   245
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724256, dtype=float32)}
train_step here is:   246
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.720416, dtype=float32)}
train_step here is:   247
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7292604, dtype=float32)}
train_step here is:   248
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734379, dtype=float32)}
train_step here is:   249
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.718026, dtype=float32)}
train_step here is:   250
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.737308, dtype=float32)}
train_step here is:   251
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7338037, dtype=float32)}
train_step here is:   252
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721154, dtype=float32)}
train_step here is:   253
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.71784, dtype=float32)}
train_step here is:   254
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7091217, dtype=float32)}
train_step here is:   255
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7132277, dtype=float32)}
train_step here is:   256
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7279744, dtype=float32)}
train_step here is:   257
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7323275, dtype=float32)}
train_step here is:   258
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734909, dtype=float32)}
train_step here is:   259
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.729551, dtype=float32)}
train_step here is:   260
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7194715, dtype=float32)}
train_step here is:   261
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7387514, dtype=float32)}
train_step here is:   262
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7265778, dtype=float32)}
train_step here is:   263
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7108727, dtype=float32)}
train_step here is:   264
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714793, dtype=float32)}
train_step here is:   265
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7344766, dtype=float32)}
train_step here is:   266
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.725055, dtype=float32)}
train_step here is:   267
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7227154, dtype=float32)}
train_step here is:   268
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72344, dtype=float32)}
train_step here is:   269
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726637, dtype=float32)}
train_step here is:   270
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7325134, dtype=float32)}
train_step here is:   271
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7314935, dtype=float32)}
train_step here is:   272
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7282386, dtype=float32)}
Epoch... (2/3 | Loss: 6.728238582611084, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  3.77it/s][A[A

Evaluating...: 3it [00:00,  7.94it/s][A[A

Evaluating...: 4it [00:00,  7.53it/s][A[A

Evaluating...: 6it [00:00, 10.41it/s][A[A

Evaluating...: 8it [00:00, 12.39it/s][A[A

Evaluating...: 10it [00:00, 13.58it/s][A[A

Evaluating...: 12it [00:01, 14.55it/s][A[A

Evaluating...: 14it [00:01, 15.29it/s][A[A

Evaluating...: 16it [00:01, 10.93it/s][A[A

Evaluating...: 19it [00:01, 13.27it/s][A[A

Evaluating...: 21it [00:01, 14.10it/s][A[A

Evaluating...: 23it [00:01, 11.42it/s][A[A

Evaluating...: 25it [00:02, 10.01it/s][A[A

Evaluating...: 27it [00:02,  9.59it/s][A[A

Evaluating...: 29it [00:02,  9.12it/s][A[A

Evaluating...: 31it [00:02, 10.40it/s][A[A

Evaluating...: 33it [00:03,  6.07it/s][A[A

Evaluating...: 35it [00:03,  7.64it/s][A[A

Evaluating...: 37it [00:03,  9.08it/s][A[A

Evaluating...: 39it [00:03, 10.41it/s][A[A

Evaluating...: 41it [00:04, 10.54it/s][A[A

Evaluating...: 43it [00:04,  9.80it/s][A[A

Evaluating...: 45it [00:04,  9.26it/s][A[A

Evaluating...: 47it [00:04,  7.99it/s][A[A

Evaluating...: 50it [00:05, 10.39it/s][A[A

Evaluating...: 52it [00:05, 11.53it/s][A[A

Evaluating...: 54it [00:05, 12.71it/s][A[A

Evaluating...: 56it [00:05, 13.64it/s][A[A

Evaluating...: 58it [00:05, 14.28it/s][A[A

                                      [A[A                                                                                 Epoch... (1/3 | Eval Loss: 6.717380523681641):  33% 1/3 [16:17<18:36, 558.19s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 6.72103214263916):  67% 2/3 [16:42<08:11, 491.21s/it] 
Training...: 0it [00:00, ?it/s][AEpoch... (2/3 | Eval Loss: 6.72103214263916)

Training...: 1it [00:03,  3.35s/it][A
Training...: 2it [00:05,  2.76s/it][A
Training...: 3it [00:08,  2.69s/it][A
Training...: 4it [00:10,  2.63s/it][A
Training...: 5it [00:13,  2.53s/it][A
Training...: 6it [00:15,  2.47s/it][A
Training...: 7it [00:17,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 8it [00:48, 11.40s/it][A
Training...: 9it [00:51,  8.63s/it][A
Training...: 10it [00:53,  6.69s/it][A
Training...: 11it [00:56,  5.45s/it][A
Training...: 12it [00:58,  4.58s/it][A
Training...: 13it [01:00,  3.90s/it][A
Training...: 14it [01:03,  3.43s/it][A
Training...: 15it [01:05,  3.10s/it][A
Training...: 16it [01:08,  2.94s/it][A
Training...: 17it [01:10,  2.76s/it][A
Training...: 18it [01:12,  2.64s/it][A
Training...: 19it [01:15,  2.55s/it][A
Training...: 20it [01:17,  2.55s/it][A
Training...: 21it [01:20,  2.49s/it][A
Training...: 22it [01:22,  2.45s/it][A
Training...: 23it [01:24,  2.42s/it][A
Training...: 24it [01:27,  2.46s/it][A
Training...: 25it [01:29,  2.42s/it][A
Training...: 26it [01:32,  2.41s/it][A
Training...: 27it [01:34,  2.39s/it][A
Training...: 28it [01:36,  2.44s/it][A
Training...: 29it [01:39,  2.41s/it][A
Training...: 30it [01:41,  2.39s/it][A
Training...: 31it [01:44,  2.38s/it][A
Training...: 32it [01:46,  2.43s/it][A
Training...: 33it [01:48,  2.41s/it][A
Training...: 34it [01:51,  2.39s/it][A
Training...: 35it [01:53,  2.38s/it][A
Training...: 36it [01:56,  2.54s/it][A
Training...: 37it [01:58,  2.48s/it][A
Training...: 38it [02:01,  2.44s/it][A
Training...: 39it [02:03,  2.42s/it][A
Training...: 40it [02:06,  2.46s/it][A
Training...: 41it [02:08,  2.42s/it][A
Training...: 42it [02:10,  2.40s/it][A
Training...: 43it [02:13,  2.45s/it][A
Training...: 44it [02:15,  2.42s/it][A
Training...: 45it [02:18,  2.40s/it][A
Training...: 46it [02:20,  2.38s/it][A
Training...: 47it [02:23,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 48it [02:50, 10.00s/it][Atrain_step here is:   273
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7146063, dtype=float32)}
train_step here is:   274
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7195344, dtype=float32)}
train_step here is:   275
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.735881, dtype=float32)}
train_step here is:   276
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7388067, dtype=float32)}
train_step here is:   277
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7247586, dtype=float32)}
train_step here is:   278
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710395, dtype=float32)}
train_step here is:   279
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7391887, dtype=float32)}
train_step here is:   280
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727771, dtype=float32)}
train_step here is:   281
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.718463, dtype=float32)}
train_step here is:   282
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7126713, dtype=float32)}
train_step here is:   283
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728345, dtype=float32)}
train_step here is:   284
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7109556, dtype=float32)}
train_step here is:   285
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716912, dtype=float32)}
train_step here is:   286
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734168, dtype=float32)}
train_step here is:   287
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714486, dtype=float32)}
train_step here is:   288
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713393, dtype=float32)}
train_step here is:   289
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.707597, dtype=float32)}
train_step here is:   290
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721692, dtype=float32)}
train_step here is:   291
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7362957, dtype=float32)}
train_step here is:   292
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722028, dtype=float32)}
train_step here is:   293
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.712907, dtype=float32)}
train_step here is:   294
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7174644, dtype=float32)}
train_step here is:   295
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7182164, dtype=float32)}
train_step here is:   296
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.709483
Training...: 49it [02:53,  7.77s/it][A, dtype=float32)}
train_step here is:   297
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7198057, dtype=float32)}
train_step here is:   298
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.717319, dtype=float32)}
train_step here is:   299
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728157, dtype=float32)}
train_step here is:   300
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7043023, dtype=float32)}
train_step here is:   301
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7161894, dtype=float32)}
train_step here is:   302
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723673, dtype=float32)}
train_step here is:   303
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7074804, dtype=float32)}
train_step here is:   304
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7108417, dtype=float32)}
train_step here is:   305
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7101183, dtype=float32)}
train_step here is:   306
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7112527, dtype=float32)}
train_step here is:   307
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7235165, dtype=float32)}
train_step here is:   308
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7186155, dtype=float32)}
train_step here is:   309
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723566, dtype=float32)}
train_step here is:   310
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727117, dtype=float32)}
train_step here is:   311
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726522, dtype=float32)}
train_step here is:   312
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7380857, dtype=float32)}
train_step here is:   313
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.718802, dtype=float32)}
train_step here is:   314
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7185106, dtype=float32)}
train_step here is:   315
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721884, dtype=float32)}
train_step here is:   316
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722706, dtype=float32)}
train_step here is:   317
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7363787, dtype=float32)}
train_step here is:   318
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7317185, dtype=float32)}
train_step here is:   319
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728638, dtype=float32)}
train_step here is:   320
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.735029, dtype=float32)}
train_step here is:   321
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7320576, dtype=float32)}
Training...: 50it [02:55,  6.14s/it][A
Training...: 51it [02:58,  5.15s/it][A
Training...: 52it [03:00,  4.31s/it][A
Training...: 53it [03:03,  3.72s/it][A
Training...: 54it [03:05,  3.31s/it][A
Training...: 55it [03:07,  3.08s/it][A
Training...: 56it [03:10,  2.86s/it][A
Training...: 57it [03:12,  2.71s/it][A
Training...: 58it [03:15,  2.60s/it][A
Training...: 59it [03:17,  2.58s/it][A
Training...: 60it [03:19,  2.51s/it][A
Training...: 61it [03:22,  2.46s/it][A
Training...: 62it [03:24,  2.43s/it][A
Training...: 63it [03:27,  2.46s/it][A
Training...: 64it [03:29,  2.43s/it][A
Training...: 65it [03:31,  2.40s/it][A
Training...: 66it [03:34,  2.39s/it][A
Training...: 67it [03:36,  2.44s/it][A
Training...: 68it [03:39,  2.41s/it][A
Training...: 69it [03:41,  2.39s/it][A
Training...: 70it [03:43,  2.38s/it][A
Training...: 71it [03:46,  2.43s/it][A
Training...: 72it [03:48,  2.41s/it][A
Training...: 73it [03:51,  2.39s/it][A
Training...: 74it [03:53,  2.38s/it][A
Training...: 75it [03:55,  2.43s/it][A
Training...: 76it [03:58,  2.40s/it][A
Training...: 77it [04:00,  2.39s/it][A
Training...: 78it [04:03,  2.38s/it][A
Training...: 79it [04:05,  2.43s/it][A
Training...: 80it [04:07,  2.40s/it][A
Training...: 81it [04:10,  2.39s/it][A
Training...: 82it [04:12,  2.38s/it][A
Training...: 83it [04:15,  2.43s/it][A
Training...: 84it [04:17,  2.40s/it][A
Training...: 85it [04:19,  2.39s/it][A
Training...: 86it [04:22,  2.55s/it][A
Training...: 87it [04:25,  2.49s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 88it [04:56, 11.07s/it][A
Training...: 89it [04:59,  8.61s/it][A
Training...: 90it [05:01,  6.80s/it][A
Training...: 91it [05:04,  5.54s/it][A
Training...: 92it [05:06,  4.58s/it][A
Training...: 93it [05:08,  3.91s/it][A
Training...: 94it [05:11,  3.51s/it][A
Training...: 95it [05:13,  3.16s/it][A
Training...: 96it [05:16,  2.91s/it][A
Training...: 97it [05:18,  2.74s/it][A
train_step here is:   322
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716595, dtype=float32)}
train_step here is:   323
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721572, dtype=float32)}
train_step here is:   324
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7207246, dtype=float32)}
train_step here is:   325
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710804, dtype=float32)}
train_step here is:   326
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7260685, dtype=float32)}
train_step here is:   327
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724906, dtype=float32)}
train_step here is:   328
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7301445, dtype=float32)}
train_step here is:   329
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.720299, dtype=float32)}
train_step here is:   330
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719725, dtype=float32)}
train_step here is:   331
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7249355, dtype=float32)}
train_step here is:   332
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72432, dtype=float32)}
train_step here is:   333
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7186985, dtype=float32)}
train_step here is:   334
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724848, dtype=float32)}
train_step here is:   335
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726984, dtype=float32)}
train_step here is:   336
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7149434, dtype=float32)}
train_step here is:   337
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723835, dtype=float32)}
train_step here is:   338
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.714718, dtype=float32)}
train_step here is:   339
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7198553, dtype=float32)}
train_step here is:   340
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7284255, dtype=float32)}
train_step here is:   341
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7134385, dtype=float32)}
train_step here is:   342
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.709976, dtype=float32)}
train_step here is:   343
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.720313, dtype=float32)}
train_step here is:   344
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722506, dtype=float32)}
train_step here is:   345
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7246695, dtype=float32)}
train_step here is:   346
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7155814, dtype=float32)}
train_step here is:   347
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719167, dtype=float32)}
train_step here is:   348
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7237134, dtype=float32)}
train_step here is:   349
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7119236, dtype=float32)}
train_step here is:   350
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728167, dtype=float32)}
train_step here is:   351
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7204394, dtype=float32)}
train_step here is:   352
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7141914, dtype=float32)}
train_step here is:   353
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.707719, dtype=float32)}
train_step here is:   354
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7270727, dtype=float32)}
train_step here is:   355
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7209272, dtype=float32)}
train_step here is:   356
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72904, dtype=float32)}
train_step here is:   357
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713921, dtype=float32)}
train_step here is:   358
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.73958, dtype=float32)}
train_step here is:   359
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.711487, dtype=float32)}
train_step here is:   360
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7340875, dtype=float32)}
train_step here is:   361
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.718664, dtype=float32)}
train_step here is:   362
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7170253, dtype=float32)}
train_step here is:   363
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7029314, dtype=float32)}
train_step here is:   364
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7274127, dtype=float32)}
train_step here is:   365
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7183895, dtype=float32)}
train_step here is:   366
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.71731, dtype=float32)}
train_step here is:   367
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722493, dtype=float32)}
train_step here is:   368
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7263975, dtype=float32)}
train_step here is:   369
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.733064, dtype=float32)}
train_step here is:   370
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724903, dtype=float32)}
Training...: 98it [05:21,  2.69s/it][A
Training...: 99it [05:23,  2.59s/it][A
Training...: 100it [05:25,  2.52s/it][A
Training...: 101it [05:28,  2.47s/it][A
Training...: 102it [05:30,  2.48s/it][A
Training...: 103it [05:33,  2.44s/it][A
Training...: 104it [05:35,  2.42s/it][A
Training...: 105it [05:37,  2.40s/it][A
Training...: 106it [05:40,  2.45s/it][A
Training...: 107it [05:42,  2.42s/it][A
Training...: 108it [05:44,  2.40s/it][A
Training...: 109it [05:47,  2.39s/it][A
Training...: 110it [05:49,  2.44s/it][A
Training...: 111it [05:52,  2.41s/it][A
Training...: 112it [05:54,  2.39s/it][A
Training...: 113it [05:56,  2.38s/it][A
Training...: 114it [05:59,  2.42s/it][A
Training...: 115it [06:01,  2.40s/it][A
Training...: 116it [06:04,  2.39s/it][A
Training...: 117it [06:06,  2.38s/it][A
Training...: 118it [06:08,  2.40s/it][A
Training...: 119it [06:11,  2.39s/it][A
Training...: 120it [06:13,  2.38s/it][A
Training...: 121it [06:16,  2.37s/it][A
Training...: 122it [06:18,  2.40s/it][A
Training...: 123it [06:20,  2.38s/it][A
Training...: 124it [06:23,  2.37s/it][A
Training...: 125it [06:25,  2.37s/it][A
Training...: 126it [06:28,  2.39s/it][A
Training...: 127it [06:30,  2.38s/it][A


train_step here is:   371
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.718013, dtype=float32)}
train_step here is:   372
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.713114, dtype=float32)}
train_step here is:   373
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7128615, dtype=float32)}
train_step here is:   374
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.716072, dtype=float32)}
train_step here is:   375
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727025, dtype=float32)}
train_step here is:   376
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7137547, dtype=float32)}
train_step here is:   377
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.711339, dtype=float32)}
train_step here is:   378
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7279553, dtype=float32)}
train_step here is:   379
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7111583, dtype=float32)}
train_step here is:   380
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.733342, dtype=float32)}
train_step here is:   381
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723285, dtype=float32)}
train_step here is:   382
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7175255, dtype=float32)}
train_step here is:   383
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710122, dtype=float32)}
train_step here is:   384
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722228, dtype=float32)}
train_step here is:   385
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.724908, dtype=float32)}
train_step here is:   386
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727977, dtype=float32)}
train_step here is:   387
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7103586, dtype=float32)}
train_step here is:   388
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727249, dtype=float32)}
train_step here is:   389
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7174425, dtype=float32)}
train_step here is:   390
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715472, dtype=float32)}
train_step here is:   391
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715761, dtype=float32)}
train_step here is:   392
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715495, dtype=float32)}
train_step here is:   393
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719395, dtype=float32)}
train_step here is:   394
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7210264, dtype=float32)}
train_step here is:   395
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727401, dtype=float32)}
train_step here is:   396
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.733617, dtype=float32)}
train_step here is:   397
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7156687, dtype=float32)}
train_step here is:   398
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.709991, dtype=float32)}
train_step here is:   399
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722845, dtype=float32)}
train_step here is:   400
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7158246, dtype=float32)}
Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  3.87it/s][A[A

Evaluating...: 4it [00:00,  9.37it/s][A[A

Evaluating...: 5it [00:00,  8.33it/s][A[A

Evaluating...: 6it [00:00,  8.17it/s][A[A

Evaluating...: 7it [00:00,  8.28it/s][A[A

Evaluating...: 8it [00:00,  8.29it/s][A[A

Evaluating...: 9it [00:01,  8.00it/s][A[A

Evaluating...: 10it [00:01,  7.90it/s][A[A

Evaluating...: 11it [00:01,  8.30it/s][A[A

Evaluating...: 13it [00:01, 10.44it/s][A[A

Evaluating...: 15it [00:01,  6.46it/s][A[A

Evaluating...: 16it [00:02,  5.73it/s][A[A

Evaluating...: 19it [00:02,  8.78it/s][A[A

Evaluating...: 21it [00:02, 10.32it/s][A[A

Evaluating...: 23it [00:02, 10.69it/s][A[A

Evaluating...: 25it [00:02,  9.73it/s][A[A

Evaluating...: 27it [00:03,  9.12it/s][A[A

Evaluating...: 29it [00:03,  8.74it/s][A[A

Evaluating...: 30it [00:03,  8.54it/s][A[A

Evaluating...: 32it [00:03,  7.86it/s][A[A

Evaluating...: 35it [00:03, 10.48it/s][A[A

Evaluating...: 37it [00:04, 11.71it/s][A[A

Evaluating...: 39it [00:04, 12.84it/s][A[A

Evaluating...: 41it [00:04, 13.71it/s][A[A

Evaluating...: 43it [00:04, 14.47it/s][A[A

Evaluating...: 45it [00:04, 15.15it/s][A[A

Evaluating...: 47it [00:04, 11.92it/s][A[A

Evaluating...: 49it [00:04, 12.76it/s][A[A

Evaluating...: 51it [00:05, 10.86it/s][A[A

Evaluating...: 53it [00:05,  9.79it/s][A[A

Evaluating...: 55it [00:05,  9.18it/s][A[A

Evaluating...: 57it [00:05, 10.38it/s][A[A

Evaluating...: 59it [00:05, 11.65it/s][A[A

                                      [A[A                                                                                
                                     [AEpoch... (2/3 | Eval Loss: 6.72103214263916):  67% 2/3 [23:21<08:11, 491.21s/it]
Training...: 127it [06:38,  2.38s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json

Training...: 128it [07:05, 12.34s/it][A
Training...: 129it [07:08,  9.44s/it][A
Training...: 130it [07:10,  7.31s/it][A
Training...: 131it [07:13,  5.89s/it][A
Training...: 132it [07:15,  4.83s/it][A
Training...: 133it [07:18,  4.12s/it][A
Training...: 134it [07:20,  3.59s/it][A
Training...: 135it [07:23,  3.21s/it][A
Training...: 136it [07:25,  2.95s/it][A
                                     [A                                                                                Epoch... (3/3 | Eval Loss: 6.718649864196777):  67% 2/3 [24:07<08:11, 491.21s/it]

Evaluating...: 0it [00:00, ?it/s][A[AEpoch... (3/3 | Eval Loss: 6.718649864196777)
train_step here is:   401
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7240753, dtype=float32)}
train_step here is:   402
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7292485, dtype=float32)}
train_step here is:   403
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728361, dtype=float32)}
train_step here is:   404
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7261095, dtype=float32)}
train_step here is:   405
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723411, dtype=float32)}
train_step here is:   406
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7297516, dtype=float32)}
train_step here is:   407
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7228117, dtype=float32)}
train_step here is:   408
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7368035, dtype=float32)}
Epoch... (3/3 | Loss: 6.7368035316467285, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  4.49it/s][A[A

Evaluating...: 3it [00:00,  8.82it/s][A[A

Evaluating...: 4it [00:00,  8.58it/s][A[A

Evaluating...: 6it [00:00, 11.59it/s][A[A

Evaluating...: 8it [00:00, 13.07it/s][A[A

Evaluating...: 10it [00:00, 13.96it/s][A[A

Evaluating...: 12it [00:00, 14.88it/s][A[A

Evaluating...: 14it [00:01, 15.47it/s][A[A

Evaluating...: 16it [00:01, 11.01it/s][A[A

Evaluating...: 19it [00:01, 13.51it/s][A[A

Evaluating...: 21it [00:01, 14.23it/s][A[A

Evaluating...: 23it [00:01, 11.50it/s][A[A

Evaluating...: 25it [00:02,  9.99it/s][A[A

Evaluating...: 27it [00:02,  9.47it/s][A[A

Evaluating...: 29it [00:02,  8.97it/s][A[A

Evaluating...: 31it [00:02, 10.24it/s][A[A

Evaluating...: 33it [00:03,  9.00it/s][A[A

Evaluating...: 35it [00:03, 10.66it/s][A[A

Evaluating...: 37it [00:03, 11.89it/s][A[A

Evaluating...: 39it [00:03, 12.98it/s][A[A

Evaluating...: 41it [00:03, 13.79it/s][A[A

Evaluating...: 43it [00:03, 14.59it/s][A[A

Evaluating...: 45it [00:03, 15.09it/s][A[A

Evaluating...: 47it [00:04, 11.97it/s][A[A

Evaluating...: 49it [00:04, 11.74it/s][A[A

Evaluating...: 51it [00:04, 10.51it/s][A[A

Evaluating...: 53it [00:04,  9.36it/s][A[A

Evaluating...: 55it [00:04,  9.93it/s][A[A

Evaluating...: 57it [00:05, 11.33it/s][A[A

Evaluating...: 59it [00:05, 12.46it/s][A[A

                                      [A[A                                                                                 Epoch... (3/3 | Eval Loss: 6.718649864196777):  67% 2/3 [24:13<08:11, 491.21s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 6.719422340393066): 100% 3/3 [24:40<00:00, 485.06s/it]Epoch... (3/3 | Eval Loss: 6.719422340393066): 100% 3/3 [24:40<00:00, 493.42s/it]Epoch... (3/3 | Eval Loss: 6.719422340393066)

wandb: Waiting for W&B process to finish, PID 1470782... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.04MB uploaded (0.00MB deduped)wandb: \ 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: | 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: / 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: - 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: \ 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: | 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss ▁█▃▅
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss █▄▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/samples ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████
wandb:            train/step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████
wandb:            train/time ▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████
wandb:   train/time_per_step ▇▁▁▁▃▁▁▃▁▁▁▃▁█▁▁▃▁▁▃▁▁▁▁▁▁▁▃▁▁▁▃▁▁▃▁▁▁▁▄
wandb: 
wandb: Run summary:
wandb:             eval/loss 6.71942
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 6.7368
wandb:         train/samples 104448
wandb:            train/step 408
wandb:            train/time 1445.56409
wandb:   train/time_per_step 6.58469
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced sandy-wood-22: https://wandb.ai/gxucla/dalle-mini/runs/1llhu8ku
wandb: Find logs at: ./wandb/run-20220305_211333-1llhu8ku/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch1', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adam', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=1, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/136 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 136/136 [00:00<00:00, 20273.86it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-c2c8bb870fb1f26c
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run vivid-sea-23
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/37qchfi2
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_135137-37qchfi2
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpk35t71z2/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmp6lusyxgi/added_tokens.json. We won't load it.
loading file /tmp/tmp6lusyxgi/vocab.json
loading file /tmp/tmp6lusyxgi/merges.txt
loading file /tmp/tmp6lusyxgi/tokenizer.json
loading file None
loading file /tmp/tmp6lusyxgi/special_tokens_map.json
loading file /tmp/tmp6lusyxgi/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/1):   0% 0/1 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 13:53:39.979702: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 13:54:05.688568: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [01:49, 109.54s/it][A
Training...: 2it [01:51, 46.48s/it] [A
Training...: 3it [01:54, 26.33s/it][A
Training...: 4it [01:56, 16.94s/it][A
Training...: 5it [01:59, 11.68s/it][A
Training...: 6it [02:01,  8.51s/it][A
Training...: 7it [02:03,  6.49s/it][A
Training...: 8it [02:06,  5.23s/it][A
Training...: 9it [02:08,  4.33s/it][A
Training...: 10it [02:11,  3.82s/it][A
Training...: 11it [02:13,  3.37s/it][A
Training...: 12it [02:16,  3.12s/it][A
Training...: 13it [02:18,  2.89s/it][A
Training...: 14it [02:20,  2.72s/it][A
Training...: 15it [02:23,  2.61s/it][A
Training...: 16it [02:25,  2.59s/it][A
Training...: 17it [02:28,  2.52s/it][A
Training...: 18it [02:30,  2.47s/it][A
Training...: 19it [02:32,  2.43s/it][A
Training...: 20it [02:35,  2.46s/it][A
Training...: 21it [02:37,  2.43s/it][A
Training...: 22it [02:40,  2.40s/it][A
Training...: 23it [02:42,  2.38s/it][A
Training...: 24it [02:45,  2.43s/it][A
Training...: 25it [02:47,  2.41s/it][A
Training...: 26it [02:49,  2.39s/it][A
Training...: 27it [02:52,  2.38s/it][A
Training...: 28it [02:54,  2.43s/it][A
Training...: 29it [02:56,  2.40s/it][A
Training...: 30it [02:59,  2.39s/it][A
Training...: 31it [03:01,  2.38s/it][A
Training...: 32it [03:04,  2.43s/it][A
Training...: 33it [03:06,  2.40s/it][A
Training...: 34it [03:08,  2.39s/it][A
Training...: 35it [03:11,  2.37s/it][A
Training...: 36it [03:13,  2.43s/it][A
Training...: 37it [03:16,  2.40s/it][A
Training...: 38it [03:18,  2.39s/it][A
Training...: 39it [03:20,  2.38s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch1/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch1/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch1/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch1/special_tokens_map.json

Training...: 40it [03:44,  8.77s/it][A
Training...: 41it [03:47,  6.89s/it][A
Training...: 42it [03:49,  5.57s/it][A
Training...: 43it [03:52,  4.67s/it][A
Training...: 44it [03:54,  3.98s/it][A
Training...: 45it [03:56,  3.49s/it][A
Training...: 46it [03:59,  3.15s/it][A
Training...: 47it [04:01,  2.97s/it][A
Training...: 48it [04:04,  2.78s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.8618493, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.9503193, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(9.81534, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(9.203737, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(9.431974, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.579729, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.9771743, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.3501906, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.0855246, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.230388, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.223752, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.066672, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.0461426, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.046582, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.004383, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9736633, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9660673, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9445395, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9274044, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9175572, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8979354, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.899216, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.920783, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9068084, dtype=float32)}
train_step h
ere is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.9007893, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8699317, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.862116, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8498244, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.834953, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8353677, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.841216, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.816485, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8099833, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.807175, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8072863, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7775583, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.8067656, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7958226, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.789311, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.791775, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.797304, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.78483, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7820063, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.754487, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.743133, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7473745, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.766516, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.766431, dtype=float32)}
train_step here iTraining...: 49it [04:06,  2.65s/it][As:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.768642, dtype=float32)}
Training...: 50it [04:08,  2.56s/it][A
Training...: 51it [04:11,  2.56s/it][A
Training...: 52it [04:13,  2.50s/it][A
Training...: 53it [04:16,  2.46s/it][A
Training...: 54it [04:18,  2.53s/it][A
Training...: 55it [04:21,  2.54s/it][A
Training...: 56it [04:23,  2.48s/it][A
Training...: 57it [04:25,  2.44s/it][A
Training...: 58it [04:28,  2.41s/it][A
Training...: 59it [04:30,  2.45s/it][A
Training...: 60it [04:33,  2.42s/it][A
Training...: 61it [04:35,  2.40s/it][A
Training...: 62it [04:37,  2.39s/it][A
Training...: 63it [04:40,  2.43s/it][A
Training...: 64it [04:42,  2.41s/it][A
Training...: 65it [04:45,  2.39s/it][A
Training...: 66it [04:47,  2.38s/it][A
Training...: 67it [04:50,  2.43s/it][A
Training...: 68it [04:52,  2.41s/it][A
Training...: 69it [04:54,  2.39s/it][A
Training...: 70it [04:57,  2.38s/it][A
Training...: 71it [04:59,  2.43s/it][A
Training...: 72it [05:02,  2.40s/it][A
Training...: 73it [05:04,  2.39s/it][A
Training...: 74it [05:06,  2.38s/it][A
Training...: 75it [05:09,  2.43s/it][A
Training...: 76it [05:11,  2.41s/it][A
Training...: 77it [05:13,  2.39s/it][A
Training...: 78it [05:16,  2.38s/it][A
Training...: 79it [05:18,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch1/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch1/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch1/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch1/special_tokens_map.json

Training...: 80it [05:43,  9.00s/it][A
Training...: 81it [05:45,  7.06s/it][A
Training...: 82it [05:48,  5.68s/it][A
Training...: 83it [05:50,  4.75s/it][A
Training...: 84it [05:53,  4.03s/it][A
Training...: 85it [05:55,  3.52s/it][A
Training...: 86it [05:58,  3.23s/it][A
Training...: 87it [06:00,  2.97s/it][A
Training...: 88it [06:02,  2.78s/it][A
Training...: 89it [06:05,  2.65s/it][A
Training...: 90it [06:07,  2.62s/it][A
Training...: 91it [06:09,  2.54s/it][A
Training...: 92it [06:12,  2.48s/it][A
Training...: 93it [06:14,  2.44s/it][A
Training...: 94it [06:17,  2.48s/it][A
Training...: 95it [06:19,  2.44s/it][A
Training...: 96it [06:21,  2.42s/it][A
Training...: 97it [06:24,  2.40s/it][A

train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.764146, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.757126, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.764791, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7521152, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.722299, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7430363, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.736605, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7336583, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7417097, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7385883, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.727502, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7496095, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.743872, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.723344, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7380385, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.734548, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7399616, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7431555, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7324047, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7357836, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7245803, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.717947, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7351294, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.739291, dtype=float32)}
train_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7145967, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.732352, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7351527, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7232633, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7424617, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7233934, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.72896, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7058353, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715525, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7190423, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7348223, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.715296, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.719874, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.721203, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.730322, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7128124, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.732279, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7380505, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.726215, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.710218, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.729835, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7321095, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.728099, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7276173, dtype=float32)}
traiTraining...: 98it [06:26,  2.45s/it][An_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.7373085, dtype=float32)}
Training...: 99it [06:29,  2.42s/it][A
Training...: 100it [06:31,  2.50s/it][A
Training...: 101it [06:34,  2.45s/it][A
Training...: 102it [06:36,  2.47s/it][A
Training...: 103it [06:39,  2.44s/it][A
Training...: 104it [06:41,  2.41s/it][A
Training...: 105it [06:43,  2.39s/it][A/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch1_shampoo', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='distributed_shampoo', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=1, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/136 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 136/136 [00:00<00:00, 14531.65it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-c2c8bb870fb1f26c
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run dauntless-frost-24
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/1bnvj18f
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_142020-1bnvj18f
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmp28uxx342/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

2022-03-06 14:20:25.752129: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2124] Execution of replica 0 failed: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: out of memory

Traceback (most recent call last):
  File "train.py", line 1246, in <module>
    main()
  File "train.py", line 552, in main
    gradient_checkpointing=False,
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/utils.py", line 34, in from_pretrained
    pretrained_model_name_or_path, *model_args, **kwargs
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/modeling.py", line 500, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/modeling.py", line 358, in __init__
    random_params = self.init_weights(self.key, input_shape)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/transformers/models/bart/modeling_flax_bart.py", line 948, in init_weights
    params_rng, dropout_rng = jax.random.split(rng)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/random.py", line 188, in split
    return _return_prng_keys(wrapped, _split(key, num))
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/random.py", line 174, in _split
    return key._split(num)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/prng.py", line 191, in _split
    return PRNGKeyArray(self.impl, self.impl.split(self._keys, num))
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/prng.py", line 445, in threefry_split
    return _threefry_split(key, int(num))  # type: ignore
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/traceback_util.py", line 165, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/api.py", line 430, in cache_miss
    donated_invars=donated_invars, inline=inline)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/core.py", line 1690, in bind
    return call_bind(self, fun, *args, **params)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/core.py", line 1702, in call_bind
    outs = top_trace.process_call(primitive, fun, tracers, params)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/core.py", line 601, in process_call
    return primitive.impl(f, *tracers, **params)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/dispatch.py", line 145, in _xla_call_impl
    out = compiled_fun(*args)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/dispatch.py", line 444, in _execute_compiled
    out_bufs = compiled.execute(input_bufs)
jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: out of memory

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train.py", line 1246, in <module>
    main()
  File "train.py", line 552, in main
    gradient_checkpointing=False,
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/utils.py", line 34, in from_pretrained
    pretrained_model_name_or_path, *model_args, **kwargs
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/modeling.py", line 500, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/dalle_mini/model/modeling.py", line 358, in __init__
    random_params = self.init_weights(self.key, input_shape)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/transformers/models/bart/modeling_flax_bart.py", line 948, in init_weights
    params_rng, dropout_rng = jax.random.split(rng)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/random.py", line 188, in split
    return _return_prng_keys(wrapped, _split(key, num))
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/random.py", line 174, in _split
    return key._split(num)
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/prng.py", line 191, in _split
    return PRNGKeyArray(self.impl, self.impl.split(self._keys, num))
  File "/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/_src/prng.py", line 445, in threefry_split
    return _threefry_split(key, int(num))  # type: ignore
RuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: out of memory
wandb: Waiting for W&B process to finish, PID 1645156... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced dauntless-frost-24: https://wandb.ai/gxucla/dalle-mini/runs/1bnvj18f
wandb: Find logs at: ./wandb/run-20220306_142020-1bnvj18f/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch1_shampoo', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='distributed_shampoo', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=1, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/136 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 136/136 [00:00<00:00, 15378.25it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-c2c8bb870fb1f26c
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run dainty-snow-25
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/2pphwot6
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_142137-2pphwot6
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpjpc7k7ad/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpuxc2vexp/added_tokens.json. We won't load it.
loading file /tmp/tmpuxc2vexp/vocab.json
loading file /tmp/tmpuxc2vexp/merges.txt
loading file /tmp/tmpuxc2vexp/tokenizer.json
loading file None
loading file /tmp/tmpuxc2vexp/special_tokens_map.json
loading file /tmp/tmpuxc2vexp/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544
/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")

Epoch ... (1/1):   0% 0/1 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch1_shampoo', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=10, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=2)
WARNING:datasets.builder:Using custom data configuration small_encoded_data-5111b80596de76c2
INFO:__main__:Local TPUs: 2
INFO:__main__:Global TPUs: 2
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run earnest-voice-26
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/2dku93u2
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_144541-2dku93u2
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpr3qakelh/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmp3c3x5hd0/added_tokens.json. We won't load it.
loading file /tmp/tmp3c3x5hd0/vocab.json
loading file /tmp/tmp3c3x5hd0/merges.txt
loading file /tmp/tmp3c3x5hd0/tokenizer.json
loading file None
loading file /tmp/tmp3c3x5hd0/special_tokens_map.json
loading file /tmp/tmp3c3x5hd0/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 10
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 2
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 64
INFO:__main__:  Model parameters = 773,580,544
/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch10_small', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=10, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=2)
WARNING:datasets.builder:Using custom data configuration small_encoded_data-5111b80596de76c2
INFO:__main__:Local TPUs: 2
INFO:__main__:Global TPUs: 2
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run electric-dragon-27
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/1nymnuel
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_144647-1nymnuel
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpg7v1humi/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmp9teior8h/added_tokens.json. We won't load it.
loading file /tmp/tmp9teior8h/vocab.json
loading file /tmp/tmp9teior8h/merges.txt
loading file /tmp/tmp9teior8h/tokenizer.json
loading file None
loading file /tmp/tmp9teior8h/special_tokens_map.json
loading file /tmp/tmp9teior8h/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 10
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 2
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 64
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/10):   0% 0/10 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A
Training...: 1it [02:43, 163.53s/it][A
Training...: 2it [02:45, 68.20s/it] [A
Training...: 3it [02:46, 37.74s/it][A
Training...: 4it [02:47, 23.41s/it][A
                                   [A                                            train_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.14152, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.9873962, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.0840063, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.9499073, dtype=float32)}
Epoch... (1/10 | Loss: 5.949907302856445, Learning Rate: 0.0005000000237487257)
Epoch ... (1/10):   0% 0/10 [02:47<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.62s/it][A[A

Evaluating...: 3it [00:12,  3.31s/it][A[A

Evaluating...: 5it [00:12,  1.64s/it][A[A

Evaluating...: 7it [00:12,  1.03it/s][A[A

Evaluating...: 9it [00:13,  1.38it/s][A[A

Evaluating...: 11it [00:13,  2.04it/s][A[A

                                      [A[A                                            Epoch ... (1/10):   0% 0/10 [03:01<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (1/10 | Eval Loss: 5.853641510009766):  10% 1/10 [03:06<27:59, 186.60s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (1/10 | Eval Loss: 5.853641510009766)

Training...: 1it [00:01,  1.58s/it][A
Training...: 2it [00:03,  1.52s/it][A
Training...: 3it [00:04,  1.51s/it][A
Training...: 4it [00:06,  1.49s/it][A
                                   [A                                                                                   train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.948883, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7490993, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8601284, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7703505, dtype=float32)}
Epoch... (2/10 | Loss: 5.770350456237793, Learning Rate: 0.0005000000237487257)
Epoch... (1/10 | Eval Loss: 5.853641510009766):  10% 1/10 [03:12<27:59, 186.60s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 2it [00:00, 14.98it/s][A[A

Evaluating...: 4it [00:00, 12.40it/s][A[A

Evaluating...: 6it [00:00, 11.89it/s][A[A

Evaluating...: 8it [00:00, 10.47it/s][A[A

Evaluating...: 10it [00:00, 12.40it/s][A[A

Evaluating...: 12it [00:00, 13.26it/s][A[A

                                      [A[A                                                                                   Epoch... (1/10 | Eval Loss: 5.853641510009766):  10% 1/10 [03:13<27:59, 186.60s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (2/10 | Eval Loss: 5.726149559020996):  20% 2/10 [03:20<11:18, 84.76s/it] Epoch... (2/10 | Eval Loss: 5.726149559020996)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:01,  1.53s/it][A
Training...: 2it [00:03,  1.52s/it][A
Training...: 3it [00:04,  1.49s/it][A
Training...: 4it [00:05,  1.48s/it][A
                                   [A                                                                                  Epoch... (2/10 | Eval Loss: 5.726149559020996):  20% 2/10 [03:26<11:18, 84.76s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.67048, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.728463, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7546453, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6397276, dtype=float32)}
Epoch... (3/10 | Loss: 5.639727592468262, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 15.55it/s][A[A

Evaluating...: 4it [00:00, 16.75it/s][A[A

Evaluating...: 6it [00:00, 17.08it/s][A[A

Evaluating...: 8it [00:00, 16.74it/s][A[A

Evaluating...: 10it [00:00, 16.62it/s][A[A

Evaluating...: 12it [00:00, 17.03it/s][A[A

                                      [A[A                                                                                  Epoch... (2/10 | Eval Loss: 5.726149559020996):  20% 2/10 [03:26<11:18, 84.76s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (3/10 | Eval Loss: 5.643712997436523):  30% 3/10 [03:33<06:04, 52.11s/it]
Epoch... (3/10 | Eval Loss: 5.643712997436523)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:01,  1.52s/it][A
Training...: 2it [00:02,  1.48s/it][A
Training...: 3it [00:04,  1.49s/it][A
Training...: 4it [00:05,  1.48s/it][A
                                   [A                                                                                  Epoch... (3/10 | Eval Loss: 5.643712997436523):  30% 3/10 [03:39<06:04, 52.11s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.49123, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6350803, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5668573, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6654534, dtype=float32)}
Epoch... (4/10 | Loss: 5.6654534339904785, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 15.65it/s][A[A

Evaluating...: 4it [00:00, 16.85it/s][A[A

Evaluating...: 6it [00:00, 16.86it/s][A[A

Evaluating...: 8it [00:00, 16.43it/s][A[A

Evaluating...: 10it [00:00, 14.83it/s][A[A

Evaluating...: 12it [00:00, 12.87it/s][A[A

                                      [A[A                                                                                  Epoch... (3/10 | Eval Loss: 5.643712997436523):  30% 3/10 [03:40<06:04, 52.11s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (4/10 | Eval Loss: 5.676398754119873):  40% 4/10 [03:46<03:40, 36.80s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (4/10 | Eval Loss: 5.676398754119873)

Training...: 1it [00:01,  1.53s/it][A
Training...: 2it [00:02,  1.49s/it][A
Training...: 3it [00:04,  1.49s/it][A
Training...: 4it [00:05,  1.48s/it][A
                                   [A                                                                                  Epoch... (4/10 | Eval Loss: 5.676398754119873):  40% 4/10 [03:52<03:40, 36.80s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6414366, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5159245, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.502471, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.52398, dtype=float32)}
Epoch... (5/10 | Loss: 5.523980140686035, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 16.82it/s][A[A

Evaluating...: 4it [00:00, 12.31it/s][A[A

Evaluating...: 6it [00:00, 11.02it/s][A[A

Evaluating...: 8it [00:00, 11.69it/s][A[A

Evaluating...: 10it [00:00, 13.15it/s][A[A

Evaluating...: 12it [00:00, 14.02it/s][A[A

                                      [A[A                                                                                  Epoch... (4/10 | Eval Loss: 5.676398754119873):  40% 4/10 [03:53<03:40, 36.80s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (5/10 | Eval Loss: 5.572726249694824):  50% 5/10 [04:00<02:21, 28.36s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (5/10 | Eval Loss: 5.572726249694824)

Training...: 1it [00:01,  1.53s/it][A
Training...: 2it [00:02,  1.49s/it][A
Training...: 3it [00:04,  1.49s/it][A
Training...: 4it [00:05,  1.48s/it][A
                                   [A                                                                                  Epoch... (5/10 | Eval Loss: 5.572726249694824):  50% 5/10 [04:06<02:21, 28.36s/it]train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4945154, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.413639, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6727557, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5208864, dtype=float32)}
Epoch... (6/10 | Loss: 5.520886421203613, Learning Rate: 0.0005000000237487257)


Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 2it [00:00, 15.87it/s][A[A

Evaluating...: 4it [00:00, 17.09it/s][A[A

Evaluating...: 6it [00:00, 16.96it/s][A[A

Evaluating...: 8it [00:00, 16.90it/s][A[A

Evaluating...: 10it [00:00, 16.51it/s][A[A

Evaluating...: 12it [00:00, 16.68it/s][A[A

                                      [A[A                                                                                  Epoch... (5/10 | Eval Loss: 5.572726249694824):  50% 5/10 [04:06<02:21, 28.36s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (6/10 | Eval Loss: 5.687436103820801):  60% 6/10 [04:13<01:32, 23.23s/it]Epoch... (6/10 | Eval Loss: 5.687436103820801)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:01,  1.56s/it][A
Training...: 2it [00:03,  1.51s/it][A
Training...: 3it [00:04,  1.50s/it][A
Training...: 4it [00:05,  1.49s/it][A
                                   [A                                                                                  Epoch... (6/10 | Eval Loss: 5.687436103820801):  60% 6/10 [04:19<01:32, 23.23s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4849787, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4674296, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5735645, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.61209, dtype=float32)}
Epoch... (7/10 | Loss: 5.612090110778809, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 16.49it/s][A[A

Evaluating...: 4it [00:00, 13.21it/s][A[A

Evaluating...: 6it [00:00, 10.64it/s][A[A

Evaluating...: 8it [00:00,  9.97it/s][A[A

Evaluating...: 10it [00:00,  9.70it/s][A[A

Evaluating...: 12it [00:01, 10.50it/s][A[A

                                      [A[A                                                                                  Epoch... (6/10 | Eval Loss: 5.687436103820801):  60% 6/10 [04:20<01:32, 23.23s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (7/10 | Eval Loss: 5.650382995605469):  70% 7/10 [04:27<01:00, 20.14s/it]
Epoch... (7/10 | Eval Loss: 5.650382995605469)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:01,  1.53s/it][A
Training...: 2it [00:03,  1.76s/it][A
Training...: 3it [00:04,  1.62s/it][A
Training...: 4it [00:06,  1.56s/it][A
                                   [A                                                                                  Epoch... (7/10 | Eval Loss: 5.650382995605469):  70% 7/10 [04:33<01:00, 20.14s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5339203, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.442693, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.2513046, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.431926, dtype=float32)}
Epoch... (8/10 | Loss: 5.4319257736206055, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 16.15it/s][A[A

Evaluating...: 4it [00:00, 16.84it/s][A[A

Evaluating...: 6it [00:00, 17.25it/s][A[A

Evaluating...: 8it [00:00, 16.42it/s][A[A

Evaluating...: 10it [00:00, 16.59it/s][A[A

Evaluating...: 12it [00:00, 16.66it/s][A[A

                                      [A[A                                                                                  Epoch... (7/10 | Eval Loss: 5.650382995605469):  70% 7/10 [04:34<01:00, 20.14s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (8/10 | Eval Loss: 5.571867942810059):  80% 8/10 [04:40<00:36, 18.07s/it]Epoch... (8/10 | Eval Loss: 5.571867942810059)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:01,  1.52s/it][A
Training...: 2it [00:02,  1.49s/it][A
Training...: 3it [00:04,  1.49s/it][A
Training...: 4it [00:05,  1.48s/it][A
                                   [A                                                                                  Epoch... (8/10 | Eval Loss: 5.571867942810059):  80% 8/10 [04:46<00:36, 18.07s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.3642817, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.244322, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.224823, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.408868, dtype=float32)}
Epoch... (9/10 | Loss: 5.408867835998535, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 16.05it/s][A[A

Evaluating...: 4it [00:00, 13.42it/s][A[A

Evaluating...: 6it [00:00, 10.36it/s][A[A

Evaluating...: 8it [00:00, 10.05it/s][A[A

Evaluating...: 10it [00:00,  9.67it/s][A[A

Evaluating...: 12it [00:01, 10.25it/s][A[A

                                      [A[A                                                                                  Epoch... (8/10 | Eval Loss: 5.571867942810059):  80% 8/10 [04:47<00:36, 18.07s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (9/10 | Eval Loss: 5.628340721130371):  90% 9/10 [04:54<00:16, 16.60s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (9/10 | Eval Loss: 5.628340721130371)

Training...: 1it [00:01,  1.52s/it][A
Training...: 2it [00:02,  1.49s/it][A
Training...: 3it [00:04,  1.49s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json

Training...: 4it [00:12,  3.95s/it][A
                                   [A                                                                                  train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.3527975, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.499882, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.182217, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.199323, dtype=float32)}
Epoch... (10/10 | Loss: 5.1993231773376465, Learning Rate: 0.0005000000237487257)
Epoch... (9/10 | Eval Loss: 5.628340721130371):  90% 9/10 [05:06<00:16, 16.60s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 2it [00:00, 14.42it/s][A[A

Evaluating...: 4it [00:00, 11.56it/s][A[A

Evaluating...: 6it [00:00, 13.79it/s][A[A

Evaluating...: 8it [00:00, 14.71it/s][A[A

Evaluating...: 10it [00:00, 15.47it/s][A[A

Evaluating...: 12it [00:00, 15.89it/s][A[A

                                      [A[A                                                                                  Epoch... (9/10 | Eval Loss: 5.628340721130371):  90% 9/10 [05:07<00:16, 16.60s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch10_small/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch10_small/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch10_small/special_tokens_map.json
Epoch... (10/10 | Eval Loss: 5.611915588378906): 100% 10/10 [05:12<00:00, 17.11s/it]Epoch... (10/10 | Eval Loss: 5.611915588378906): 100% 10/10 [05:12<00:00, 31.24s/it]Epoch... (10/10 | Eval Loss: 5.611915588378906)

wandb: Waiting for W&B process to finish, PID 1652503... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▅▃▄▁▄▃▁▂▂
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss █▆▆▅▅▄▄▄▅▃▃▃▁▁
wandb:         train/samples ▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███
wandb:            train/step ▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███
wandb:            train/time ▁▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████
wandb:   train/time_per_step ██▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.61192
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 5.19932
wandb:         train/samples 2560
wandb:            train/step 40
wandb:            train/time 298.60016
wandb:   train/time_per_step 3.34113
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced electric-dragon-27: https://wandb.ai/gxucla/dalle-mini/runs/1nymnuel
wandb: Find logs at: ./wandb/run-20220306_144647-1nymnuel/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model_lr4_epoch13_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=1, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/136 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 136/136 [00:00<00:00, 14808.17it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-03a8f3605fc9c263
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run blooming-totem-28
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/oivvhmsh
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_150614-oivvhmsh
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmp7kiltecf/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpy6b00cqp/added_tokens.json. We won't load it.
loading file /tmp/tmpy6b00cqp/vocab.json
loading file /tmp/tmpy6b00cqp/merges.txt
loading file /tmp/tmpy6b00cqp/tokenizer.json
loading file None
loading file /tmp/tmpy6b00cqp/special_tokens_map.json
loading file /tmp/tmpy6b00cqp/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/1):   0% 0/1 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 15:09:41.881597: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 15:10:05.342934: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:13, 193.43s/it][A
Training...: 2it [03:15, 81.05s/it] [A
Training...: 3it [03:18, 45.13s/it][A
Training...: 4it [03:21, 28.51s/it][A
Training...: 5it [03:23, 19.08s/it][A
Training...: 6it [03:25, 13.40s/it][A
Training...: 7it [03:28,  9.80s/it][A
Training...: 8it [03:30,  7.50s/it][A
Training...: 9it [03:33,  5.90s/it][A
Training...: 10it [03:35,  4.81s/it][A
Training...: 11it [03:38,  4.06s/it][A
Training...: 12it [03:40,  3.61s/it][A
Training...: 13it [03:42,  3.24s/it][A
Training...: 14it [03:45,  2.98s/it][A
Training...: 15it [03:47,  2.79s/it][A
Training...: 16it [03:50,  2.73s/it][A
Training...: 17it [03:52,  2.62s/it][A
Training...: 18it [03:55,  2.55s/it][A
Training...: 19it [03:57,  2.49s/it][A
Training...: 20it [03:59,  2.51s/it][A
Training...: 21it [04:02,  2.47s/it][A
Training...: 22it [04:04,  2.45s/it][A
Training...: 23it [04:07,  2.42s/it][A
Training...: 24it [04:09,  2.47s/it][A
Training...: 25it [04:12,  2.44s/it][A
Training...: 26it [04:14,  2.42s/it][A
Training...: 27it [04:16,  2.41s/it][A
Training...: 28it [04:19,  2.46s/it][A
Training...: 29it [04:21,  2.43s/it][A
Training...: 30it [04:24,  2.42s/it][A
Training...: 31it [04:26,  2.40s/it][A
Training...: 32it [04:29,  2.46s/it][A
Training...: 33it [04:31,  2.43s/it][A
Training...: 34it [04:33,  2.41s/it][A
Training...: 35it [04:36,  2.40s/it][A
Training...: 36it [04:38,  2.46s/it][A
Training...: 37it [04:41,  2.43s/it][A
Training...: 38it [04:43,  2.41s/it][A
Training...: 39it [04:45,  2.40s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/special_tokens_map.json

Training...: 40it [04:54,  4.31s/it][A
Training...: 41it [04:57,  3.74s/it][A
Training...: 42it [04:59,  3.34s/it][A
Training...: 43it [05:02,  3.11s/it][A
Training...: 44it [05:04,  2.89s/it][A
Training...: 45it [05:06,  2.74s/it][A
Training...: 46it [05:09,  2.63s/it][A
Training...: 47it [05:11,  2.61s/it][A
Training...: 48it [05:14,  2.54s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.974384, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.9310594, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.149728, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.910558, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.0182357, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8158016, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8037715, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.712622, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.711743, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7690763, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.745832, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.694449, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6119585, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6924386, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.690011, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6726832, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.610544, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5550833, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.629792, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.66804, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.645901, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.590864, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6139517, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6384797, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5633335, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6590843, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6613417, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.653404, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.731522, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7409678, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6305065, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.63672, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.566618, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.610487, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5439796, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6112695, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.531621, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.537345, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.584373, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6077185, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.60379, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6310153, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6662335, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.542485, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.59542, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5680714, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.608396, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.564268, dtype=float32)}
train_step here is:  
 49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.572186, dtype=float32)}Training...: 49it [05:16,  2.49s/it][A
Training...: 50it [05:18,  2.46s/it][A
Training...: 51it [05:21,  2.49s/it][A
Training...: 52it [05:23,  2.46s/it][A
Training...: 53it [05:26,  2.44s/it][A
Training...: 54it [05:28,  2.42s/it][A
Training...: 55it [05:31,  2.47s/it][A
Training...: 56it [05:33,  2.44s/it][A
Training...: 57it [05:35,  2.42s/it][A
Training...: 58it [05:38,  2.41s/it][A
Training...: 59it [05:40,  2.46s/it][A
Training...: 60it [05:43,  2.43s/it][A
Training...: 61it [05:45,  2.42s/it][A
Training...: 62it [05:48,  2.41s/it][A
Training...: 63it [05:50,  2.45s/it][A
Training...: 64it [05:52,  2.43s/it][A
Training...: 65it [05:55,  2.42s/it][A
Training...: 66it [05:57,  2.40s/it][A
Training...: 67it [06:00,  2.45s/it][A
Training...: 68it [06:02,  2.43s/it][A
Training...: 69it [06:05,  2.42s/it][A
Training...: 70it [06:07,  2.41s/it][A
Training...: 71it [06:10,  2.46s/it][A
Training...: 72it [06:12,  2.44s/it][A
Training...: 73it [06:14,  2.42s/it][A
Training...: 74it [06:17,  2.41s/it][A
Training...: 75it [06:19,  2.46s/it][A
Training...: 76it [06:22,  2.43s/it][A
Training...: 77it [06:24,  2.42s/it][A
Training...: 78it [06:26,  2.40s/it][A
Training...: 79it [06:29,  2.45s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/special_tokens_map.json

Training...: 80it [06:39,  4.82s/it][A
Training...: 81it [06:42,  4.10s/it][A
Training...: 82it [06:44,  3.58s/it][A
Training...: 83it [06:47,  3.28s/it][A
Training...: 84it [06:49,  3.01s/it][A
Training...: 85it [06:51,  2.82s/it][A
Training...: 86it [06:54,  2.75s/it][A
Training...: 87it [06:56,  2.63s/it][A
Training...: 88it [06:59,  2.56s/it][A
Training...: 89it [07:01,  2.50s/it][A
Training...: 90it [07:04,  2.53s/it][A
Training...: 91it [07:06,  2.48s/it][A
Training...: 92it [07:08,  2.45s/it][A
Training...: 93it [07:11,  2.43s/it][A
Training...: 94it [07:13,  2.47s/it][A
Training...: 95it [07:16,  2.44s/it][A
Training...: 96it [07:18,  2.43s/it][A
Training...: 97it [07:21,  2.41s/it][A

train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.648642, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6557293, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5599203, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.536457, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.56793, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.599658, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.575159, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.567456, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.495686, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6347985, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.573761, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5560975, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5869765, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.455343, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.521632, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4792833, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.528619, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4990625, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5660653, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.611293, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.576351, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5742035, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6393704, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.558745, dtype=float32)}
traiTraining...: 98it [07:24,  2.60s/it][An_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5158553, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.601745, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.581824, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5683756, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5693207, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.562353, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.595491, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.670144, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.528952, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5796013, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5827255, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5574226, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4789, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.474476, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.550787, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4793644, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.609154, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.530442, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.535591, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.574302, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5316525, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5536118, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.553474, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.525875, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.504181, dtype=float32)}
Training...: 99it [07:26,  2.53s/it][A
Training...: 100it [07:28,  2.49s/it][A
Training...: 101it [07:31,  2.45s/it][A
Training...: 102it [07:33,  2.49s/it][A
Training...: 103it [07:36,  2.45s/it][A
Training...: 104it [07:38,  2.44s/it][A
Training...: 105it [07:40,  2.42s/it][A
Training...: 106it [07:43,  2.47s/it][A
Training...: 107it [07:45,  2.44s/it][A
Training...: 108it [07:48,  2.42s/it][A
Training...: 109it [07:50,  2.41s/it][A
Training...: 110it [07:53,  2.47s/it][A
Training...: 111it [07:55,  2.44s/it][A
Training...: 112it [07:58,  2.43s/it][A
Training...: 113it [08:00,  2.41s/it][A
Training...: 114it [08:02,  2.46s/it][A
Training...: 115it [08:05,  2.43s/it][A
Training...: 116it [08:07,  2.42s/it][A
Training...: 117it [08:10,  2.41s/it][A
Training...: 118it [08:12,  2.43s/it][A
Training...: 119it [08:14,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/special_tokens_map.json

Training...: 120it [08:25,  4.77s/it][A
Training...: 121it [08:27,  4.06s/it][A
Training...: 122it [08:30,  3.59s/it][A
Training...: 123it [08:32,  3.24s/it][A
Training...: 124it [08:34,  2.98s/it][A
Training...: 125it [08:37,  2.80s/it][A
Training...: 126it [08:39,  2.71s/it][A
Training...: 127it [08:42,  2.61s/it][A
Training...: 128it [08:44,  2.54s/it][A
Training...: 129it [08:47,  2.53s/it][A
Training...: 130it [08:49,  2.49s/it][A
Training...: 131it [08:51,  2.45s/it][A
Training...: 132it [08:54,  2.43s/it][A
Training...: 133it [08:56,  2.45s/it][A
Training...: 134it [08:59,  2.43s/it][A
Training...: 135it [09:01,  2.41s/it][A
Training...: 136it [09:03,  2.40s/it][A
                                     [A                                          Epoch ... (1/1):   0% 0/1 [09:03<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.506094, dtype=float32)}
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.557574, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.54383, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6135263, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.626993, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.512933, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.552944, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5443163, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6074824, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.526711, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5986643, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5826073, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5541925, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4978223, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5160503, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5188746, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5452075, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.545657, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.532992, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.480177, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.523905, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.563341, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5077586, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.495081, dtype=float32)}
train_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.56558, dtype=float32)}
train_step here is:   124
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4597197, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.522274, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5257664, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5423717, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.531761, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5098763, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5390296, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.536467, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5599704, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.492735, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.493028, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5211906, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4944153, dtype=float32)}
Epoch... (1/1 | Loss: 5.494415283203125, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:12, 12.87s/it][A[A

Evaluating...: 3it [00:12,  3.38s/it][A[A

Evaluating...: 5it [00:13,  1.67s/it][A[A

Evaluating...: 7it [00:13,  1.01it/s][A[A

Evaluating...: 9it [00:13,  1.57it/s][A[A

Evaluating...: 11it [00:13,  2.29it/s][A[A

Evaluating...: 13it [00:13,  3.21it/s][A[A

Evaluating...: 15it [00:13,  4.18it/s][A[A

Evaluating...: 17it [00:14,  4.74it/s][A[A

Evaluating...: 19it [00:14,  5.64it/s][A[A

Evaluating...: 21it [00:14,  6.36it/s][A[A

Evaluating...: 23it [00:14,  7.14it/s][A[A

Evaluating...: 25it [00:14,  8.58it/s][A[A

Evaluating...: 27it [00:14, 10.10it/s][A[A

Evaluating...: 29it [00:15, 11.35it/s][A[A

Evaluating...: 31it [00:15, 12.61it/s][A[A

Evaluating...: 33it [00:15, 10.06it/s][A[A

Evaluating...: 35it [00:15, 11.58it/s][A[A

Evaluating...: 37it [00:15, 12.71it/s][A[A

Evaluating...: 39it [00:15, 13.65it/s][A[A

Evaluating...: 41it [00:16, 11.17it/s][A[A

Evaluating...: 43it [00:16, 10.53it/s][A[A

Evaluating...: 45it [00:16,  9.33it/s][A[A

Evaluating...: 47it [00:16,  9.00it/s][A[A

Evaluating...: 49it [00:16, 10.36it/s][A[A

Evaluating...: 51it [00:17, 11.64it/s][A[A

                                      [A[A                                          Epoch ... (1/1):   0% 0/1 [09:21<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model_lr4_epoch13_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model_lr4_epoch13_adafactor/special_tokens_map.json
Epoch... (1/1 | Eval Loss: 5.500514507293701): 100% 1/1 [09:27<00:00, 567.76s/it]Epoch... (1/1 | Eval Loss: 5.500514507293701): 100% 1/1 [09:27<00:00, 567.76s/it]
Epoch... (1/1 | Eval Loss: 5.500514507293701)
wandb: Waiting for W&B process to finish, PID 1660889... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.04MB uploaded (0.00MB deduped)wandb: \ 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: | 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: / 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: - 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: \ 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: | 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: / 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb: - 0.04MB of 0.04MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss ▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss █▅▇▄▅▃▃▄▄▃▃▃▂▁
wandb:         train/samples ▁▂▂▃▃▄▄▅▅▆▆▇▇███
wandb:            train/step ▁▂▂▃▃▄▄▅▅▆▆▇▇███
wandb:            train/time ▁▄▄▄▅▅▅▆▆▆▇▇▇███
wandb:   train/time_per_step █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.50051
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 5.49442
wandb:         train/samples 34816
wandb:            train/step 136
wandb:            train/time 541.45435
wandb:   train/time_per_step 2.40106
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced blooming-totem-28: https://wandb.ai/gxucla/dalle-mini/runs/oivvhmsh
wandb: Find logs at: ./wandb/run-20220306_150614-oivvhmsh/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr4_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
WARNING:datasets.builder:Using custom data configuration small_encoded_data-5111b80596de76c2
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run pleasant-firebrand-29
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/6444wd8s
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_160946-6444wd8s
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpegrep0_x/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpgq51gibr/added_tokens.json. We won't load it.
loading file /tmp/tmpgq51gibr/vocab.json
loading file /tmp/tmpgq51gibr/merges.txt
loading file /tmp/tmpgq51gibr/tokenizer.json
loading file None
loading file /tmp/tmpgq51gibr/special_tokens_map.json
loading file /tmp/tmpgq51gibr/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544
/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")

Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 16:13:13.698505: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 16:13:36.952455: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:12, 192.17s/it][A
                                    [Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.008719, dtype=float32)}
Epoch... (1/3 | Loss: 8.008719444274902, Learning Rate: 0.0005000000237487257)
                                          Epoch ... (1/3):   0% 0/3 [03:12<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.73s/it][A[A

Evaluating...: 3it [00:12,  3.34s/it][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [03:25<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 7.808119773864746):  33% 1/3 [03:32<07:05, 212.82s/it]
Epoch... (1/3 | Eval Loss: 7.808119773864746)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:02,  2.47s/it][A
                                   [A                                                                                 Epoch... (1/3 | Eval Loss: 7.808119773864746):  33% 1/3 [03:35<07:05, 212.82s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.008719, dtype=float32)}
Epoch... (2/3 | Loss: 8.008719444274902, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  9.96it/s][A[A

Evaluating...: 3it [00:00, 15.83it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 7.808119773864746):  33% 1/3 [03:35<07:05, 212.82s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 6.132516860961914):  67% 2/3 [03:43<01:33, 93.67s/it] Epoch... (2/3 | Eval Loss: 6.132516860961914)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:02,  2.43s/it][A
                                   [A                                                                                Epoch... (2/3 | Eval Loss: 6.132516860961914):  67% 2/3 [03:45<01:33, 93.67s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.189269, dtype=float32)}
Epoch... (3/3 | Loss: 6.189269065856934, Learning Rate: 0.0005000000237487257)


Evaluating...: 2it [00:00, 13.91it/s][A[A

                                     [A[A                                                                                Epoch... (2/3 | Eval Loss: 6.132516860961914):  67% 2/3 [03:45<01:33, 93.67s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.8489508628845215): 100% 3/3 [03:54<00:00, 56.32s/it]Epoch... (3/3 | Eval Loss: 5.8489508628845215): 100% 3/3 [03:54<00:00, 78.32s/it]Epoch... (3/3 | Eval Loss: 5.8489508628845215)

wandb: Waiting for W&B process to finish, PID 1680349... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▂▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁
wandb:            train/loss ██▁
wandb:         train/samples ▁▃▃▃▆▆▆██
wandb:            train/step ▁▃▃▃▆▆▆██
wandb:            train/time ▁▁▁▁█████
wandb:   train/time_per_step ██▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.84895
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 6.18927
wandb:         train/samples 768
wandb:            train/step 3
wandb:            train/time 223.15312
wandb:   train/time_per_step 10.22691
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced pleasant-firebrand-29: https://wandb.ai/gxucla/dalle-mini/runs/6444wd8s
wandb: Find logs at: ./wandb/run-20220306_160946-6444wd8s/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr4_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 149/149 [00:00<00:00, 13636.29it/s]
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 88246.50it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-71d0035c557750de
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run elated-darkness-30
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/28iajpko
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_161834-28iajpko
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmppmv86tz8/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpf4hxnuuy/added_tokens.json. We won't load it.
loading file /tmp/tmpf4hxnuuy/vocab.json
loading file /tmp/tmpf4hxnuuy/merges.txt
loading file /tmp/tmpf4hxnuuy/tokenizer.json
loading file None
loading file /tmp/tmpf4hxnuuy/special_tokens_map.json
loading file /tmp/tmpf4hxnuuy/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544
/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")

Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 16:22:02.728443: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 16:22:27.946146: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:13, 193.28s/it][A
Training...: 2it [03:16, 81.25s/it] [A
Training...: 3it [03:18, 45.23s/it][A
Training...: 4it [03:21, 28.40s/it][A
Training...: 5it [03:23, 19.02s/it][A
Training...: 6it [03:25, 13.36s/it][A
Training...: 7it [03:28,  9.77s/it][A
Training...: 8it [03:30,  7.49s/it][A
Training...: 9it [03:33,  5.89s/it][A
Training...: 10it [03:35,  4.81s/it][A
Training...: 11it [03:37,  4.06s/it][A
Training...: 12it [03:40,  3.61s/it][A
Training...: 13it [03:42,  3.24s/it][A
Training...: 14it [03:45,  2.98s/it][A
Training...: 15it [03:47,  2.80s/it][A
Training...: 16it [03:50,  2.73s/it][A
Training...: 17it [03:52,  2.62s/it][A
Training...: 18it [03:55,  2.55s/it][A
Training...: 19it [03:57,  2.49s/it][A
Training...: 20it [03:59,  2.52s/it][A
Training...: 21it [04:02,  2.48s/it][A
Training...: 22it [04:04,  2.45s/it][A
Training...: 23it [04:07,  2.43s/it][A
Training...: 24it [04:09,  2.47s/it][A
Training...: 25it [04:12,  2.44s/it][A
Training...: 26it [04:14,  2.42s/it][A
Training...: 27it [04:16,  2.40s/it][A
Training...: 28it [04:19,  2.46s/it][A
Training...: 29it [04:21,  2.43s/it][A
Training...: 30it [04:24,  2.42s/it][A
Training...: 31it [04:26,  2.40s/it][A
Training...: 32it [04:29,  2.45s/it][A
Training...: 33it [04:31,  2.43s/it][A
Training...: 34it [04:33,  2.41s/it][A
Training...: 35it [04:36,  2.40s/it][A
Training...: 36it [04:38,  2.46s/it][A
Training...: 37it [04:41,  2.44s/it][A
Training...: 38it [04:43,  2.42s/it][A
Training...: 39it [04:45,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 40it [04:54,  4.37s/it][A
Training...: 41it [04:57,  3.79s/it][A
Training...: 42it [04:59,  3.38s/it][A
Training...: 43it [05:02,  3.14s/it][A
Training...: 44it [05:04,  2.91s/it][A
Training...: 45it [05:07,  2.75s/it][A
Training...: 46it [05:09,  2.64s/it][A
Training...: 47it [05:11,  2.62s/it][A
Training...: 48it [05:14,  2.54s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.126101, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.20076, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.2349644, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7824535, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.859421, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8968973, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.843046, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.747263, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7105217, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7431307, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7904105, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.695624, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.739373, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.690564, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6804657, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6774926, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6456985, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.561322, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6945887, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6821985, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.682765, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6334715, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.626272, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.616811, dtype=float32)}
train_step he
Training...: 49it [05:16,  2.50s/it][Are is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6475396, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6226525, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5557947, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.626437, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8184595, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.663208, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.556176, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.529734, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.559614, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5333343, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.598166, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.569662, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.52731, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.541135, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6117263, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.590495, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5989723, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6232586, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.603683, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5375643, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.53483, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.668345, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6941366, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.587551, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.620036, dtype=float32)}
Training...: 50it [05:19,  2.59s/it][A
Training...: 51it [05:22,  2.59s/it][A
Training...: 52it [05:24,  2.53s/it][A
Training...: 53it [05:26,  2.48s/it][A
Training...: 54it [05:29,  2.45s/it][A
Training...: 55it [05:31,  2.49s/it][A
Training...: 56it [05:34,  2.46s/it][A
Training...: 57it [05:36,  2.43s/it][A
Training...: 58it [05:38,  2.41s/it][A
Training...: 59it [05:41,  2.47s/it][A
Training...: 60it [05:43,  2.44s/it][A
Training...: 61it [05:46,  2.42s/it][A
Training...: 62it [05:48,  2.41s/it][A
Training...: 63it [05:51,  2.45s/it][A
Training...: 64it [05:53,  2.43s/it][A
Training...: 65it [05:56,  2.41s/it][A
Training...: 66it [05:58,  2.41s/it][A
Training...: 67it [06:01,  2.47s/it][A
Training...: 68it [06:03,  2.44s/it][A
Training...: 69it [06:05,  2.42s/it][A
Training...: 70it [06:08,  2.41s/it][A
Training...: 71it [06:10,  2.46s/it][A
Training...: 72it [06:13,  2.44s/it][A
Training...: 73it [06:15,  2.42s/it][A
Training...: 74it [06:17,  2.41s/it][A
Training...: 75it [06:20,  2.46s/it][A
Training...: 76it [06:22,  2.43s/it][A
Training...: 77it [06:25,  2.41s/it][A
Training...: 78it [06:27,  2.40s/it][A
Training...: 79it [06:30,  2.45s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 80it [06:40,  4.81s/it][A
Training...: 81it [06:42,  4.09s/it][A
Training...: 82it [06:45,  3.58s/it][A
Training...: 83it [06:47,  3.28s/it][A
Training...: 84it [06:50,  3.01s/it][A
Training...: 85it [06:52,  2.82s/it][A
Training...: 86it [06:55,  2.75s/it][A
Training...: 87it [06:57,  2.63s/it][A
Training...: 88it [06:59,  2.56s/it][A
Training...: 89it [07:02,  2.51s/it][A
Training...: 90it [07:04,  2.53s/it][A
Training...: 91it [07:07,  2.48s/it][A
Training...: 92it [07:09,  2.45s/it][A
Training...: 93it [07:11,  2.43s/it][A
Training...: 94it [07:14,  2.47s/it][A
Training...: 95it [07:16,  2.44s/it][A
Training...: 96it [07:19,  2.42s/it][A
Training...: 97it [07:22,  2.53s/it][A

train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5251713, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.604341, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6715517, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6392064, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.623453, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5439906, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5449915, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5292835, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5865936, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.586969, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.455412, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6144104, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.564727, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5965075, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5567756, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.530214, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.522666, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5805206, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.59451, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.557601, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.572833, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5873246, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.470791, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.584058, dtype=float32)}
trTraining...: 98it [07:24,  2.55s/it][Aain_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5360746, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4999647, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.54683, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.472498, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.59303, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.603089, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6558046, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5883303, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5356407, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4900713, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5939245, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5841675, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6040154, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5155563, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5684795, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5313916, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4822803, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5141916, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.565412, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5767407, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4980693, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.52838, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.596689, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4958544, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.528869, dtype=float32)}
Training...: 99it [07:27,  2.50s/it][A
Training...: 100it [07:29,  2.46s/it][A
Training...: 101it [07:31,  2.44s/it][A
Training...: 102it [07:34,  2.48s/it][A
Training...: 103it [07:36,  2.45s/it][A
Training...: 104it [07:39,  2.43s/it][A
Training...: 105it [07:41,  2.41s/it][A
Training...: 106it [07:44,  2.46s/it][A
Training...: 107it [07:46,  2.43s/it][A
Training...: 108it [07:48,  2.42s/it][A
Training...: 109it [07:51,  2.40s/it][A
Training...: 110it [07:53,  2.46s/it][A
Training...: 111it [07:56,  2.43s/it][A
Training...: 112it [07:58,  2.42s/it][A
Training...: 113it [08:00,  2.41s/it][A
Training...: 114it [08:03,  2.46s/it][A
Training...: 115it [08:05,  2.43s/it][A
Training...: 116it [08:08,  2.42s/it][A
Training...: 117it [08:10,  2.40s/it][A
Training...: 118it [08:13,  2.45s/it][A
Training...: 119it [08:15,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 120it [08:25,  4.81s/it][A
Training...: 121it [08:28,  4.09s/it][A
Training...: 122it [08:30,  3.63s/it][A
Training...: 123it [08:33,  3.27s/it][A
Training...: 124it [08:35,  3.00s/it][A
Training...: 125it [08:38,  2.81s/it][A
Training...: 126it [08:40,  2.76s/it][A
Training...: 127it [08:43,  2.64s/it][A
Training...: 128it [08:45,  2.56s/it][A
Training...: 129it [08:47,  2.54s/it][A
Training...: 130it [08:50,  2.49s/it][A
Training...: 131it [08:52,  2.46s/it][A
Training...: 132it [08:55,  2.43s/it][A
Training...: 133it [08:57,  2.45s/it][A
Training...: 134it [08:59,  2.43s/it][A
Training...: 135it [09:02,  2.41s/it][A
Training...: 136it [09:04,  2.40s/it][A
Training...: 137it [09:07,  2.43s/it][A
Training...: 138it [09:09,  2.41s/it][A
Training...: 139it [09:11,  2.40s/it][A
Training...: 140it [09:14,  2.39s/it][A
Training...: 141it [09:16,  2.42s/it][A
Training...: 142it [09:19,  2.41s/it][A
Training...: 143it [09:21,  2.40s/it][A
Training...: 144it [09:23,  2.39s/it][A
Training...: 145it [09:26,  2.54s/it][A
Training...: 146it [09:29,  2.49s/it][A
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.488517, dtype=float32)}
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4912815, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5088954, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4915137, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5366, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.534408, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.499389, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5676227, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5397654, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5594406, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5661125, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.532532, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5578313, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.542468, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5533667, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.552425, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5116215, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5301857, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5426426, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5799913, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.609945, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.580798, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5081615, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5325713, dtype=float32)}
train_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.511736, dtype=float32)}
train_step here is:   124
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.503002, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.515449, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.502585, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.533137, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.570035, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5988283, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5532875, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5467334, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5399094, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.500248, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5655184, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.547488, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5887017, dtype=float32)}
train_step here is:   137
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5995693, dtype=float32)}
train_step here is:   138
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5442934, dtype=float32)}
train_step here is:   139
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5280623, dtype=float32)}
train_step here is:   140
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5034027, dtype=float32)}
train_step here is:   141
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5620365, dtype=float32)}
train_step here is:   142
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.542902, dtype=float32)}
train_step here is:   143
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.533821, dtype=float32)}
train_step here is:   144
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.520677, dtype=float32)}
train_step here is:   145
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4698195, dtype=float32)}
train_step here is:   146
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4898853, dtype=float32)}
train_step here is:   147
the learning rate here is:   
Training...: 147it [09:31,  2.46s/it][A
Training...: 148it [09:34,  2.44s/it][A
Training...: 149it [09:36,  2.44s/it][A
                                     [A                                          Epoch ... (1/3):   0% 0/3 [09:36<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A{'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5147557, dtype=float32)}
train_step here is:   148
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.551188, dtype=float32)}
train_step here is:   149
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4850206, dtype=float32)}
Epoch... (1/3 | Loss: 5.485020637512207, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:12, 12.58s/it][A[A

Evaluating...: 3it [00:12,  3.30s/it][A[A

Evaluating...: 5it [00:12,  1.63s/it][A[A

Evaluating...: 7it [00:12,  1.03it/s][A[A

Evaluating...: 9it [00:13,  1.60it/s][A[A

Evaluating...: 11it [00:13,  2.34it/s][A[A

Evaluating...: 13it [00:13,  3.28it/s][A[A

Evaluating...: 15it [00:13,  4.42it/s][A[A

Evaluating...: 17it [00:13,  4.97it/s][A[A

Evaluating...: 19it [00:13,  5.57it/s][A[A

Evaluating...: 21it [00:14,  4.56it/s][A[A

Evaluating...: 23it [00:14,  5.92it/s][A[A

Evaluating...: 25it [00:14,  7.44it/s][A[A

Evaluating...: 27it [00:14,  8.88it/s][A[A

Evaluating...: 29it [00:15, 10.25it/s][A[A

Evaluating...: 31it [00:15, 11.74it/s][A[A

Evaluating...: 33it [00:15,  9.46it/s][A[A

Evaluating...: 35it [00:15, 10.93it/s][A[A

Evaluating...: 37it [00:15, 10.97it/s][A[A

Evaluating...: 39it [00:16, 10.13it/s][A[A

Evaluating...: 41it [00:16,  9.17it/s][A[A

Evaluating...: 43it [00:16,  9.01it/s][A[A

Evaluating...: 44it [00:16,  8.53it/s][A[A

Evaluating...: 46it [00:16, 10.31it/s][A[A

Evaluating...: 48it [00:17,  8.24it/s][A[A

Evaluating...: 50it [00:17, 10.03it/s][A[A

Evaluating...: 52it [00:17, 11.25it/s][A[A

Evaluating...: 54it [00:17, 12.42it/s][A[A

Evaluating...: 56it [00:17, 13.48it/s][A[A

Evaluating...: 58it [00:17, 14.27it/s][A[A

Evaluating...: 60it [00:17, 14.83it/s][A[A

Evaluating...: 62it [00:18, 11.90it/s][A[A

Evaluating...: 64it [00:18, 10.91it/s][A[A

Evaluating...: 66it [00:18,  9.65it/s][A[A

                                      [A[A                                          Epoch ... (1/3):   0% 0/3 [09:55<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 5.555107116699219):  33% 1/3 [10:03<20:07, 603.67s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (1/3 | Eval Loss: 5.555107116699219)

Training...: 1it [00:03,  3.04s/it][A
Training...: 2it [00:05,  2.65s/it][A
Training...: 3it [00:07,  2.55s/it][A
Training...: 4it [00:10,  2.56s/it][A
Training...: 5it [00:12,  2.49s/it][A
Training...: 6it [00:15,  2.45s/it][A
Training...: 7it [00:17,  2.43s/it][A
Training...: 8it [00:20,  2.47s/it][A
Training...: 9it [00:22,  2.44s/it][A
Training...: 10it [00:24,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 11it [00:35,  4.85s/it][A
Training...: 12it [00:37,  4.16s/it][A
Training...: 13it [00:40,  3.62s/it][A
Training...: 14it [00:42,  3.26s/it][A
Training...: 15it [00:44,  2.99s/it][A
Training...: 16it [00:47,  2.86s/it][A
Training...: 17it [00:49,  2.72s/it][A
Training...: 18it [00:52,  2.62s/it][A
Training...: 19it [00:54,  2.54s/it][A
Training...: 20it [00:57,  2.55s/it][A
Training...: 21it [00:59,  2.50s/it][A
Training...: 22it [01:02,  2.46s/it][A
Training...: 23it [01:04,  2.43s/it][A
Training...: 24it [01:06,  2.48s/it][A
Training...: 25it [01:09,  2.45s/it][A
Training...: 26it [01:11,  2.43s/it][A
Training...: 27it [01:14,  2.41s/it][A
Training...: 28it [01:16,  2.46s/it][A
Training...: 29it [01:19,  2.44s/it][A
Training...: 30it [01:21,  2.56s/it][A
Training...: 31it [01:24,  2.50s/it][A
Training...: 32it [01:26,  2.52s/it][A
Training...: 33it [01:29,  2.48s/it][A
Training...: 34it [01:31,  2.45s/it][A
Training...: 35it [01:33,  2.43s/it][A
Training...: 36it [01:36,  2.47s/it][A
Training...: 37it [01:38,  2.44s/it][A
Training...: 38it [01:41,  2.42s/it][A
Training...: 39it [01:43,  2.41s/it][A
Training...: 40it [01:46,  2.47s/it][A
Training...: 41it [01:48,  2.44s/it][A
Training...: 42it [01:51,  2.42s/it][A
Training...: 43it [01:53,  2.47s/it][A
Training...: 44it [01:55,  2.44s/it][A
Training...: 45it [01:58,  2.42s/it][A
Training...: 46it [02:00,  2.41s/it][A
Training...: 47it [02:03,  2.47s/it][A
Training...: 48it [02:05,  2.44s/it][Atrain_step here is:   150
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5327644, dtype=float32)}
train_step here is:   151
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5219584, dtype=float32)}
train_step here is:   152
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.546049, dtype=float32)}
train_step here is:   153
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4712887, dtype=float32)}
train_step here is:   154
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.450816, dtype=float32)}
train_step here is:   155
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.449343, dtype=float32)}
train_step here is:   156
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4650526, dtype=float32)}
train_step here is:   157
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5033307, dtype=float32)}
train_step here is:   158
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4684057, dtype=float32)}
train_step here is:   159
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4824495, dtype=float32)}
train_step here is:   160
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5610027, dtype=float32)}
train_step here is:   161
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5294323, dtype=float32)}
train_step here is:   162
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.563176, dtype=float32)}
train_step here is:   163
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5394907, dtype=float32)}
train_step here is:   164
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.493863, dtype=float32)}
train_step here is:   165
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5680733, dtype=float32)}
train_step here is:   166
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4561043, dtype=float32)}
train_step here is:   167
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4334345, dtype=float32)}
train_step here is:   168
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5095844, dtype=float32)}
train_step here is:   169
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.533919, dtype=float32)}
train_step here is:   170
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.513906, dtype=float32)}
train_step here is:   171
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5233545, dtype=float32)}
train_step here is:   172
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.521919, dtype=float32)}
train_step here is:   173
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.483075, dtype=float32)}
train_step here is:   174
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4645667, dtype=float32)}
train_step here is:   175
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4652033, dtype=float32)}
train_step here is:   176
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.48691, dtype=float32)}
train_step here is:   177
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.507906, dtype=float32)}
train_step here is:   178
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5678315, dtype=float32)}
train_step here is:   179
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.503358, dtype=float32)}
train_step here is:   180
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4847546, dtype=float32)}
train_step here is:   181
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5087695, dtype=float32)}
train_step here is:   182
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4840007, dtype=float32)}
train_step here is:   183
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.545437, dtype=float32)}
train_step here is:   184
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5911655, dtype=float32)}
train_step here is:   185
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.485194, dtype=float32)}
train_step here is:   186
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5190454, dtype=float32)}
train_step here is:   187
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.53598, dtype=float32)}
train_step here is:   188
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4822035, dtype=float32)}
train_step here is:   189
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.506933, dtype=float32)}
train_step here is:   190
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.565387, dtype=float32)}
train_step here is:   191
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4616756, dtype=float32)}
train_step here is:   192
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.554937, dtype=float32)}
train_step here is:   193
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4429264, dtype=float32)}
train_step here is:   194
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.483747, dtype=float32)}
train_step here is:   195
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4289193, dtype=float32)}
train_step here is:   196
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5359526, dtype=float32)}
train_step here is:   197
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.410842, dtype=float32)}
train_step here is:   198
the learning rate here is:   
Training...: 49it [02:08,  2.42s/it][A
Training...: 50it [02:10,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 51it [02:21,  4.90s/it][A
Training...: 52it [02:23,  4.15s/it][A
Training...: 53it [02:25,  3.62s/it][A
Training...: 54it [02:28,  3.26s/it][A
Training...: 55it [02:30,  3.06s/it][A
Training...: 56it [02:33,  2.85s/it][A
Training...: 57it [02:35,  2.71s/it][A
Training...: 58it [02:38,  2.61s/it][A
Training...: 59it [02:40,  2.60s/it][A
Training...: 60it [02:43,  2.54s/it][A
Training...: 61it [02:45,  2.49s/it][A
Training...: 62it [02:47,  2.46s/it][A
Training...: 63it [02:50,  2.49s/it][A
Training...: 64it [02:52,  2.45s/it][A
Training...: 65it [02:55,  2.43s/it][A
Training...: 66it [02:57,  2.41s/it][A
Training...: 67it [03:00,  2.46s/it][A
Training...: 68it [03:02,  2.44s/it][A
Training...: 69it [03:04,  2.42s/it][A
Training...: 70it [03:07,  2.41s/it][A
Training...: 71it [03:09,  2.48s/it][A
Training...: 72it [03:12,  2.45s/it][A
Training...: 73it [03:14,  2.42s/it][A
Training...: 74it [03:16,  2.41s/it][A
Training...: 75it [03:19,  2.46s/it][A
Training...: 76it [03:21,  2.44s/it][A
Training...: 77it [03:24,  2.42s/it][A
Training...: 78it [03:26,  2.41s/it][A
Training...: 79it [03:29,  2.59s/it][A
Training...: 80it [03:32,  2.53s/it][A
Training...: 81it [03:34,  2.48s/it][A
Training...: 82it [03:36,  2.45s/it][A
Training...: 83it [03:39,  2.49s/it][A
Training...: 84it [03:41,  2.45s/it][A
Training...: 85it [03:44,  2.43s/it][A
Training...: 86it [03:46,  2.47s/it][A
Training...: 87it [03:49,  2.44s/it][A
Training...: 88it [03:51,  2.42s/it][A
Training...: 89it [03:53,  2.41s/it][A
Training...: 90it [03:56,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 91it [04:06,  4.86s/it][A
Training...: 92it [04:09,  4.12s/it][A
Training...: 93it [04:11,  3.60s/it][A
Training...: 94it [04:14,  3.30s/it][A
Training...: 95it [04:16,  3.02s/it][A
Training...: 96it [04:19,  2.83s/it][A
{'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4790387, dtype=float32)}
train_step here is:   199
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.466708, dtype=float32)}
train_step here is:   200
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.510891, dtype=float32)}
train_step here is:   201
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4860744, dtype=float32)}
train_step here is:   202
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.477454, dtype=float32)}
train_step here is:   203
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.545519, dtype=float32)}
train_step here is:   204
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.470292, dtype=float32)}
train_step here is:   205
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5529833, dtype=float32)}
train_step here is:   206
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5095644, dtype=float32)}
train_step here is:   207
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4913673, dtype=float32)}
train_step here is:   208
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5068164, dtype=float32)}
train_step here is:   209
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5006056, dtype=float32)}
train_step here is:   210
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5608053, dtype=float32)}
train_step here is:   211
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5486684, dtype=float32)}
train_step here is:   212
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4532557, dtype=float32)}
train_step here is:   213
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4264393, dtype=float32)}
train_step here is:   214
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5195246, dtype=float32)}
train_step here is:   215
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.466017, dtype=float32)}
train_step here is:   216
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5388913, dtype=float32)}
train_step here is:   217
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5145383, dtype=float32)}
train_step here is:   218
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.538841, dtype=float32)}
train_step here is:   219
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.555258, dtype=float32)}
train_step here is:   220
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.494217, dtype=float32)}
train_step here is:   221
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.464333, dtype=float32)}
train_step here is:   222
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.485741, dtype=float32)}
train_step here is:   223
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4505014, dtype=float32)}
train_step here is:   224
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.479253, dtype=float32)}
train_step here is:   225
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5090876, dtype=float32)}
train_step here is:   226
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.469253, dtype=float32)}
train_step here is:   227
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4911084, dtype=float32)}
train_step here is:   228
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.527451, dtype=float32)}
train_step here is:   229
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.501104, dtype=float32)}
train_step here is:   230
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.493352, dtype=float32)}
train_step here is:   231
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5154667, dtype=float32)}
train_step here is:   232
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4832497, dtype=float32)}
train_step here is:   233
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.519373, dtype=float32)}
train_step here is:   234
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.486305, dtype=float32)}
train_step here is:   235
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.516959, dtype=float32)}
train_step here is:   236
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4626484, dtype=float32)}
train_step here is:   237
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5187583, dtype=float32)}
train_step here is:   238
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4580436, dtype=float32)}
train_step here is:   239
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4567986, dtype=float32)}
train_step here is:   240
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5126734, dtype=float32)}
train_step here is:   241
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.3979626, dtype=float32)}
train_step here is:   242
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.589293, dtype=float32)}
train_step here is:   243
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4272103, dtype=float32)}
train_step here is:   244
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5266623, dtype=float32)}
train_step here is:   245
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.437678, dtype=float32)}
train_step Training...: 97it [04:21,  2.70s/it][Ahere is:   246
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4972253, dtype=float32)}
Training...: 98it [04:24,  2.67s/it][A
Training...: 99it [04:26,  2.58s/it][A
Training...: 100it [04:28,  2.52s/it][A
Training...: 101it [04:31,  2.48s/it][A
Training...: 102it [04:33,  2.50s/it][A
Training...: 103it [04:36,  2.46s/it][A
Training...: 104it [04:38,  2.44s/it][A
Training...: 105it [04:40,  2.42s/it][A
Training...: 106it [04:43,  2.47s/it][A
Training...: 107it [04:45,  2.44s/it][A
Training...: 108it [04:48,  2.42s/it][A
Training...: 109it [04:50,  2.41s/it][A
Training...: 110it [04:53,  2.46s/it][A
Training...: 111it [04:55,  2.44s/it][A
Training...: 112it [04:57,  2.42s/it][A
Training...: 113it [05:00,  2.41s/it][A
Training...: 114it [05:02,  2.46s/it][A
Training...: 115it [05:05,  2.43s/it][A
Training...: 116it [05:07,  2.42s/it][A
Training...: 117it [05:10,  2.40s/it][A
Training...: 118it [05:12,  2.46s/it][A
Training...: 119it [05:14,  2.43s/it][A
Training...: 120it [05:17,  2.41s/it][A
Training...: 121it [05:19,  2.40s/it][A
Training...: 122it [05:22,  2.45s/it][A
Training...: 123it [05:24,  2.43s/it][A
Training...: 124it [05:27,  2.42s/it][A
Training...: 125it [05:29,  2.40s/it][A
Training...: 126it [05:31,  2.46s/it][A
Training...: 127it [05:34,  2.57s/it][A
Training...: 128it [05:37,  2.52s/it][A
Training...: 129it [05:39,  2.51s/it][A
Training...: 130it [05:42,  2.47s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 131it [05:51,  4.43s/it][A
Training...: 132it [05:53,  3.82s/it][A
Training...: 133it [05:55,  3.42s/it][A
Training...: 134it [05:58,  3.12s/it][A
Training...: 135it [06:00,  2.90s/it][A
Training...: 136it [06:03,  2.74s/it][A
Training...: 137it [06:05,  2.66s/it][A
Training...: 138it [06:08,  2.58s/it][A
Training...: 139it [06:10,  2.52s/it][A
Training...: 140it [06:12,  2.48s/it][A
Training...: 141it [06:15,  2.48s/it][A
Training...: 142it [06:17,  2.45s/it][A
Training...: 143it [06:20,  2.43s/it][A
Training...: 144it [06:22,  2.41s/it][A
Training...: 145it [06:24,  2.43s/it][A
train_step here is:   247
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4995456, dtype=float32)}
train_step here is:   248
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5413814, dtype=float32)}
train_step here is:   249
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.423832, dtype=float32)}
train_step here is:   250
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.484692, dtype=float32)}
train_step here is:   251
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4481754, dtype=float32)}
train_step here is:   252
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.514046, dtype=float32)}
train_step here is:   253
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4943385, dtype=float32)}
train_step here is:   254
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.507243, dtype=float32)}
train_step here is:   255
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5147753, dtype=float32)}
train_step here is:   256
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4140816, dtype=float32)}
train_step here is:   257
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.495779, dtype=float32)}
train_step here is:   258
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.493942, dtype=float32)}
train_step here is:   259
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5447273, dtype=float32)}
train_step here is:   260
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.530072, dtype=float32)}
train_step here is:   261
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5778446, dtype=float32)}
train_step here is:   262
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.507452, dtype=float32)}
train_step here is:   263
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.526799, dtype=float32)}
train_step here is:   264
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4726124, dtype=float32)}
train_step here is:   265
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.536921, dtype=float32)}
train_step here is:   266
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4911313, dtype=float32)}
train_step here is:   267
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4441156, dtype=float32)}
train_step here is:   268
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4233117, dtype=float32)}
train_step here is:   269
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5544376, dtype=float32)}
train_step here is:   270
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.55
Training...: 146it [06:27,  2.42s/it][A7507, dtype=float32)}
train_step here is:   271
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.3520656, dtype=float32)}
train_step here is:   272
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.511635, dtype=float32)}
train_step here is:   273
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4963665, dtype=float32)}
train_step here is:   274
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4958205, dtype=float32)}
train_step here is:   275
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5045156, dtype=float32)}
train_step here is:   276
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5143795, dtype=float32)}
train_step here is:   277
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5604925, dtype=float32)}
train_step here is:   278
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.552414, dtype=float32)}
train_step here is:   279
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5229726, dtype=float32)}
train_step here is:   280
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.494066, dtype=float32)}
train_step here is:   281
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.479494, dtype=float32)}
train_step here is:   282
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.479351, dtype=float32)}
train_step here is:   283
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.483424, dtype=float32)}
train_step here is:   284
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.513873, dtype=float32)}
train_step here is:   285
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4721746, dtype=float32)}
train_step here is:   286
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4955173, dtype=float32)}
train_step here is:   287
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.478096, dtype=float32)}
train_step here is:   288
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.469138, dtype=float32)}
train_step here is:   289
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.52022, dtype=float32)}
train_step here is:   290
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.556918, dtype=float32)}
train_step here is:   291
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4732, dtype=float32)}
train_step here is:   292
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.49892, dtype=float32)}
train_step here is:   293
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4949875, dtype=float32)}
train_step here is:   294
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5046806, dtype=float32)}
train_step here is:   295
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.466136, dtype=float32)}
Training...: 147it [06:29,  2.40s/it][A
Training...: 148it [06:32,  2.40s/it][A
Training...: 149it [06:34,  2.41s/it][A
                                     [A                                                                                 
train_step here is:   296
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.483591, dtype=float32)}
train_step here is:   297
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5740004, dtype=float32)}
train_step here is:   298
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.501396, dtype=float32)}
Epoch... (2/3 | Loss: 5.501396179199219, Learning Rate: 0.0005000000237487257)
Epoch... (1/3 | Eval Loss: 5.555107116699219):  33% 1/3 [16:38<20:07, 603.67s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  4.41it/s][A[A

Evaluating...: 3it [00:00, 10.17it/s][A[A

Evaluating...: 5it [00:00, 12.99it/s][A[A

Evaluating...: 7it [00:00, 14.18it/s][A[A

Evaluating...: 9it [00:00, 15.16it/s][A[A

Evaluating...: 11it [00:00, 15.58it/s][A[A

Evaluating...: 13it [00:00, 15.85it/s][A[A

Evaluating...: 15it [00:01, 16.12it/s][A[A

Evaluating...: 17it [00:01, 11.24it/s][A[A

Evaluating...: 19it [00:01, 12.70it/s][A[A

Evaluating...: 21it [00:01, 10.78it/s][A[A

Evaluating...: 23it [00:01,  9.52it/s][A[A

Evaluating...: 25it [00:02,  9.19it/s][A[A

Evaluating...: 27it [00:02,  8.54it/s][A[A

Evaluating...: 29it [00:02, 10.07it/s][A[A

Evaluating...: 31it [00:02, 11.42it/s][A[A

Evaluating...: 33it [00:03,  9.52it/s][A[A

Evaluating...: 35it [00:03, 10.81it/s][A[A

Evaluating...: 37it [00:03, 12.26it/s][A[A

Evaluating...: 39it [00:03, 13.27it/s][A[A

Evaluating...: 41it [00:03, 14.07it/s][A[A

Evaluating...: 43it [00:03, 14.72it/s][A[A

Evaluating...: 45it [00:03, 13.64it/s][A[A

Evaluating...: 47it [00:04,  9.26it/s][A[A

Evaluating...: 49it [00:04, 10.73it/s][A[A

Evaluating...: 51it [00:04,  9.88it/s][A[A

Evaluating...: 53it [00:04, 10.01it/s][A[A

Evaluating...: 55it [00:04, 11.26it/s][A[A

Evaluating...: 57it [00:04, 12.30it/s][A[A

Evaluating...: 59it [00:05, 13.50it/s][A[A

Evaluating...: 61it [00:05, 14.19it/s][A[A

Evaluating...: 63it [00:05, 13.03it/s][A[A

Evaluating...: 66it [00:05, 14.75it/s][A[A

                                      [A[A                                                                                 Epoch... (1/3 | Eval Loss: 5.555107116699219):  33% 1/3 [16:43<20:07, 603.67s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.485424995422363):  67% 2/3 [16:52<08:08, 488.92s/it]Epoch... (2/3 | Eval Loss: 5.485424995422363)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.46s/it][A
Training...: 2it [00:05,  2.83s/it][A
Training...: 3it [00:08,  2.63s/it][A
Training...: 4it [00:10,  2.61s/it][A
Training...: 5it [00:13,  2.52s/it][A
Training...: 6it [00:15,  2.48s/it][A
Training...: 7it [00:17,  2.44s/it][A
Training...: 8it [00:20,  2.49s/it][A
Training...: 9it [00:22,  2.45s/it][A
Training...: 10it [00:25,  2.43s/it][A
Training...: 11it [00:27,  2.41s/it][A
Training...: 12it [00:30,  2.46s/it][A
Training...: 13it [00:32,  2.44s/it][A
Training...: 14it [00:35,  2.42s/it][A
Training...: 15it [00:37,  2.41s/it][A
Training...: 16it [00:39,  2.46s/it][A
Training...: 17it [00:42,  2.43s/it][A
Training...: 18it [00:44,  2.42s/it][A
Training...: 19it [00:47,  2.40s/it][A
Training...: 20it [00:49,  2.46s/it][A
Training...: 21it [00:52,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 22it [01:02,  4.87s/it][A
Training...: 23it [01:05,  4.13s/it][A
Training...: 24it [01:07,  3.67s/it][A
Training...: 25it [01:09,  3.28s/it][A
Training...: 26it [01:12,  3.01s/it][A
Training...: 27it [01:14,  2.82s/it][A
Training...: 28it [01:17,  2.75s/it][A
Training...: 29it [01:19,  2.64s/it][A
Training...: 30it [01:22,  2.56s/it][A
Training...: 31it [01:24,  2.50s/it][A
Training...: 32it [01:27,  2.53s/it][A
Training...: 33it [01:29,  2.48s/it][A
Training...: 34it [01:31,  2.45s/it][A
Training...: 35it [01:34,  2.43s/it][A
Training...: 36it [01:36,  2.47s/it][A
Training...: 37it [01:39,  2.44s/it][A
Training...: 38it [01:41,  2.42s/it][A
Training...: 39it [01:43,  2.41s/it][A
Training...: 40it [01:46,  2.46s/it][A
Training...: 41it [01:48,  2.43s/it][A
Training...: 42it [01:51,  2.42s/it][A
Training...: 43it [01:53,  2.46s/it][A
Training...: 44it [01:56,  2.44s/it][A
Training...: 45it [01:58,  2.42s/it][A
Training...: 46it [02:00,  2.41s/it][A
Training...: 47it [02:03,  2.46s/it][A
Training...: 48it [02:05,  2.44s/it][Atrain_step here is:   299
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4797006, dtype=float32)}
train_step here is:   300
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.478735, dtype=float32)}
train_step here is:   301
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.463272, dtype=float32)}
train_step here is:   302
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.440095, dtype=float32)}
train_step here is:   303
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.43629, dtype=float32)}
train_step here is:   304
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5170116, dtype=float32)}
train_step here is:   305
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4782124, dtype=float32)}
train_step here is:   306
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.492998, dtype=float32)}
train_step here is:   307
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5298095, dtype=float32)}
train_step here is:   308
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.518754, dtype=float32)}
train_step here is:   309
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4410753, dtype=float32)}
train_step here is:   310
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5069947, dtype=float32)}
train_step here is:   311
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.481477, dtype=float32)}
train_step here is:   312
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4521775, dtype=float32)}
train_step here is:   313
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4398746, dtype=float32)}
train_step here is:   314
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.454566, dtype=float32)}
train_step here is:   315
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.501761, dtype=float32)}
train_step here is:   316
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4381547, dtype=float32)}
train_step here is:   317
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4978294, dtype=float32)}
train_step here is:   318
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.456184, dtype=float32)}
train_step here is:   319
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4703283, dtype=float32)}
train_step here is:   320
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5338707, dtype=float32)}
train_step here is:   321
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.457426, dtype=float32)}
train_step here is:   322
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4548526, dtype=float32)}
train_step here is:   323
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4257064, dtype=float32)}
train_step here is:   324
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4998493, dtype=float32)}
train_step here is:   325
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.398224, dtype=float32)}
train_step here is:   326
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5096493, dtype=float32)}
train_step here is:   327
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4582014, dtype=float32)}
train_step here is:   328
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.430764, dtype=float32)}
train_step here is:   329
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.43291, dtype=float32)}
train_step here is:   330
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4535117, dtype=float32)}
train_step here is:   331
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4346595, dtype=float32)}
train_step here is:   332
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4564075, dtype=float32)}
train_step here is:   333
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4639044, dtype=float32)}
train_step here is:   334
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4767814, dtype=float32)}
train_step here is:   335
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5255604, dtype=float32)}
train_step here is:   336
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4346604, dtype=float32)}
train_step here is:   337
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4675894, dtype=float32)}
train_step here is:   338
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4436355, dtype=float32)}
train_step here is:   339
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.491291, dtype=float32)}
train_step here is:   340
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.451007, dtype=float32)}
train_step here is:   341
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4405227, dtype=float32)}
train_step here is:   342
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4358354, dtype=float32)}
train_step here is:   343
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4056873, dtype=float32)}
train_step here is:   344
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4874525, dtype=float32)}
train_step here is:   345
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.472085, dtype=float32)}
train_step here is:   346
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.368694, dtype=float32)}
train_step here is:   347
the learning rate here is:  
Training...: 49it [02:08,  2.42s/it][A
Training...: 50it [02:10,  2.41s/it][A
Training...: 51it [02:13,  2.59s/it][A
Training...: 52it [02:15,  2.52s/it][A
Training...: 53it [02:18,  2.48s/it][A
Training...: 54it [02:20,  2.46s/it][A
Training...: 55it [02:23,  2.49s/it][A
Training...: 56it [02:25,  2.46s/it][A
Training...: 57it [02:28,  2.43s/it][A
Training...: 58it [02:30,  2.42s/it][A
Training...: 59it [02:33,  2.46s/it][A
Training...: 60it [02:35,  2.44s/it][A
Training...: 61it [02:37,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 62it [02:48,  4.82s/it][A
Training...: 63it [02:50,  4.14s/it][A
Training...: 64it [02:53,  3.61s/it][A
Training...: 65it [02:55,  3.25s/it][A
Training...: 66it [02:57,  2.99s/it][A
Training...: 67it [03:00,  2.87s/it][A
Training...: 68it [03:02,  2.72s/it][A
Training...: 69it [03:05,  2.61s/it][A
Training...: 70it [03:07,  2.54s/it][A
Training...: 71it [03:10,  2.55s/it][A
Training...: 72it [03:12,  2.50s/it][A
Training...: 73it [03:14,  2.47s/it][A
Training...: 74it [03:17,  2.44s/it][A
Training...: 75it [03:19,  2.48s/it][A
Training...: 76it [03:22,  2.45s/it][A
Training...: 77it [03:24,  2.43s/it][A
Training...: 78it [03:27,  2.42s/it][A
Training...: 79it [03:29,  2.46s/it][A
Training...: 80it [03:32,  2.44s/it][A
Training...: 81it [03:34,  2.42s/it][A
Training...: 82it [03:36,  2.41s/it][A
Training...: 83it [03:39,  2.46s/it][A
Training...: 84it [03:41,  2.43s/it][A
Training...: 85it [03:44,  2.42s/it][A
Training...: 86it [03:46,  2.47s/it][A
Training...: 87it [03:49,  2.44s/it][A
Training...: 88it [03:51,  2.42s/it][A
Training...: 89it [03:53,  2.41s/it][A
Training...: 90it [03:56,  2.46s/it][A
Training...: 91it [03:58,  2.43s/it][A
Training...: 92it [04:01,  2.42s/it][A
Training...: 93it [04:03,  2.41s/it][A
Training...: 94it [04:06,  2.46s/it][A
Training...: 95it [04:08,  2.44s/it][A
Training...: 96it [04:10,  2.42s/it][A
Training...: 97it [04:13,  2.41s/it][A {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4873667, dtype=float32)}
train_step here is:   348
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.444955, dtype=float32)}
train_step here is:   349
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4243684, dtype=float32)}
train_step here is:   350
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4012623, dtype=float32)}
train_step here is:   351
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4800105, dtype=float32)}
train_step here is:   352
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4514275, dtype=float32)}
train_step here is:   353
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4356375, dtype=float32)}
train_step here is:   354
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.429181, dtype=float32)}
train_step here is:   355
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4727287, dtype=float32)}
train_step here is:   356
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4981017, dtype=float32)}
train_step here is:   357
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.450236, dtype=float32)}
train_step here is:   358
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.486354, dtype=float32)}
train_step here is:   359
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.464739, dtype=float32)}
train_step here is:   360
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.517641, dtype=float32)}
train_step here is:   361
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.408991, dtype=float32)}
train_step here is:   362
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4540133, dtype=float32)}
train_step here is:   363
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4170637, dtype=float32)}
train_step here is:   364
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.47311, dtype=float32)}
train_step here is:   365
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.477624, dtype=float32)}
train_step here is:   366
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.446765, dtype=float32)}
train_step here is:   367
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.485792, dtype=float32)}
train_step here is:   368
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5044823, dtype=float32)}
train_step here is:   369
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.414894, dtype=float32)}
train_step here is:   370
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4660454, dtype=float32)}
train_step here is:   371
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.509283, dtype=float32)}
train_step here is:   372
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.446649, dtype=float32)}
train_step here is:   373
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4355116, dtype=float32)}
train_step here is:   374
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4475384, dtype=float32)}
train_step here is:   375
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.404585, dtype=float32)}
train_step here is:   376
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4010363, dtype=float32)}
train_step here is:   377
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.575216, dtype=float32)}
train_step here is:   378
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4690638, dtype=float32)}
train_step here is:   379
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4425178, dtype=float32)}
train_step here is:   380
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.521948, dtype=float32)}
train_step here is:   381
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4928493, dtype=float32)}
train_step here is:   382
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.481696, dtype=float32)}
train_step here is:   383
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.472967, dtype=float32)}
train_step here is:   384
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.474545, dtype=float32)}
train_step here is:   385
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4432116, dtype=float32)}
train_step here is:   386
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.482937, dtype=float32)}
train_step here is:   387
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5326443, dtype=float32)}
train_step here is:   388
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4802413, dtype=float32)}
train_step here is:   389
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4545174, dtype=float32)}
train_step here is:   390
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4776917, dtype=float32)}
train_step here is:   391
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.497909, dtype=float32)}
train_step here is:   392
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.513307, dtype=float32)}
train_step here is:   393
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.51245, dtype=float32)}
train_step here is:   394
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5355268, dtype=float32)}
train_step here is:   395
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4618063, dtype=float32)}
Training...: 98it [04:15,  2.48s/it][A
Training...: 99it [04:18,  2.45s/it][A
Training...: 100it [04:20,  2.43s/it][A
Training...: 101it [04:23,  2.42s/it][A

Evaluating...: 0it [00:00, ?it/s][A[A
train_step here is:   396
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.416767, dtype=float32)}
train_step here is:   397
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4780254, dtype=float32)}
train_step here is:   398
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.3860903, dtype=float32)}
train_step here is:   399
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4226685, dtype=float32)}
train_step here is:   400
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4018326, dtype=float32)}


Evaluating...: 1it [00:00,  4.26it/s][A[A

Evaluating...: 3it [00:00,  7.78it/s][A[A

Evaluating...: 4it [00:00,  8.06it/s][A[A

Evaluating...: 5it [00:00,  8.05it/s][A[A

Evaluating...: 6it [00:00,  7.59it/s][A[A

Evaluating...: 7it [00:00,  7.94it/s][A[A

Evaluating...: 8it [00:01,  8.31it/s][A[A

Evaluating...: 10it [00:01, 10.14it/s][A[A

Evaluating...: 12it [00:01, 11.96it/s][A[A

Evaluating...: 14it [00:01, 13.44it/s][A[A

Evaluating...: 16it [00:01,  9.57it/s][A[A

Evaluating...: 19it [00:01, 12.05it/s][A[A

Evaluating...: 21it [00:02, 12.96it/s][A[A

Evaluating...: 23it [00:02, 13.98it/s][A[A

Evaluating...: 25it [00:02, 12.77it/s][A[A

Evaluating...: 27it [00:02, 11.18it/s][A[A

Evaluating...: 29it [00:02,  9.78it/s][A[A

Evaluating...: 31it [00:03,  9.54it/s][A[A

Evaluating...: 33it [00:03,  8.44it/s][A[A

Evaluating...: 35it [00:03, 10.08it/s][A[A

Evaluating...: 37it [00:03, 11.35it/s][A[A

Evaluating...: 39it [00:03, 12.47it/s][A[A

Evaluating...: 41it [00:03, 13.49it/s][A[A

Evaluating...: 43it [00:03, 14.27it/s][A[A

Evaluating...: 45it [00:04, 14.64it/s][A[A

Evaluating...: 47it [00:04, 10.50it/s][A[A

Evaluating...: 49it [00:04, 10.93it/s][A[A

Evaluating...: 51it [00:04,  9.30it/s][A[A

Evaluating...: 53it [00:05,  8.98it/s][A[A

Evaluating...: 55it [00:05,  8.52it/s][A[A

Evaluating...: 57it [00:05,  9.85it/s][A[A

Evaluating...: 59it [00:05, 11.11it/s][A[A

Evaluating...: 61it [00:05, 12.41it/s][A[A

Evaluating...: 63it [00:05, 12.07it/s][A[A

Evaluating...: 65it [00:06, 13.63it/s][A[A

Evaluating...: 67it [00:06, 14.41it/s][A[A

                                      [A[A                                                                                 
                                     [AEpoch... (2/3 | Eval Loss: 5.485424995422363):  67% 2/3 [21:24<08:08, 488.92s/it]
Training...: 101it [04:32,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 102it [04:40,  6.95s/it][A
Training...: 103it [04:42,  5.58s/it][A
Training...: 104it [04:45,  4.62s/it][A
Training...: 105it [04:47,  3.96s/it][A
Training...: 106it [04:50,  3.55s/it][A
Training...: 107it [04:52,  3.20s/it][A
Training...: 108it [04:55,  2.95s/it][A
Training...: 109it [04:57,  2.78s/it][A
Training...: 110it [05:00,  2.72s/it][A
Training...: 111it [05:02,  2.62s/it][A
Training...: 112it [05:04,  2.54s/it][A
Training...: 113it [05:07,  2.49s/it][A
Training...: 114it [05:09,  2.53s/it][A
Training...: 115it [05:12,  2.48s/it][A
Training...: 116it [05:14,  2.45s/it][A
Training...: 117it [05:16,  2.43s/it][A
Training...: 118it [05:19,  2.47s/it][A
Training...: 119it [05:21,  2.44s/it][A
Training...: 120it [05:24,  2.42s/it][A
Training...: 121it [05:26,  2.41s/it][A
Training...: 122it [05:29,  2.46s/it][A
Training...: 123it [05:31,  2.44s/it][A
Training...: 124it [05:34,  2.42s/it][A
Training...: 125it [05:36,  2.41s/it][A
Training...: 126it [05:39,  2.59s/it][A
Training...: 127it [05:41,  2.53s/it][A
Training...: 128it [05:44,  2.48s/it][A
Training...: 129it [05:46,  2.49s/it][A
Training...: 130it [05:49,  2.45s/it][A
Training...: 131it [05:51,  2.43s/it][A
Training...: 132it [05:53,  2.41s/it][A
Training...: 133it [05:56,  2.44s/it][A
Training...: 134it [05:58,  2.42s/it][A
Training...: 135it [06:01,  2.40s/it][A
Training...: 136it [06:03,  2.40s/it][A
Training...: 137it [06:05,  2.43s/it][A
Training...: 138it [06:08,  2.41s/it][A
Training...: 139it [06:10,  2.40s/it][A
Training...: 140it [06:13,  2.39s/it][A
Training...: 141it [06:15,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json

Training...: 142it [06:24,  4.39s/it][A
Training...: 143it [06:26,  3.80s/it][A
Training...: 144it [06:29,  3.37s/it][A
Training...: 145it [06:31,  3.12s/it][A
Training...: 146it [06:34,  2.90s/it][A
Training...: 147it [06:36,  2.74s/it][A
Training...: 148it [06:38,  2.63s/it][A
Training...: 149it [06:41,  2.57s/it][A
                                     [A                                                                                 Epoch... (3/3 | Eval Loss: 5.505321979522705):  67% 2/3 [23:33<08:08, 488.92s/it]Epoch... (3/3 | Eval Loss: 5.505321979522705)
train_step here is:   401
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4824495, dtype=float32)}
train_step here is:   402
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.516867, dtype=float32)}
train_step here is:   403
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.513158, dtype=float32)}
train_step here is:   404
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4872656, dtype=float32)}
train_step here is:   405
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.441086, dtype=float32)}
train_step here is:   406
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4902086, dtype=float32)}
train_step here is:   407
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4643707, dtype=float32)}
train_step here is:   408
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4284105, dtype=float32)}
train_step here is:   409
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4749317, dtype=float32)}
train_step here is:   410
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.460723, dtype=float32)}
train_step here is:   411
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5124307, dtype=float32)}
train_step here is:   412
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.481941, dtype=float32)}
train_step here is:   413
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4261427, dtype=float32)}
train_step here is:   414
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.506284, dtype=float32)}
train_step here is:   415
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.523356, dtype=float32)}
train_step here is:   416
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.455931, dtype=float32)}
train_step here is:   417
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4351916, dtype=float32)}
train_step here is:   418


Evaluating...: 0it [00:00, ?it/s][A[Athe learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4735036, dtype=float32)}
train_step here is:   419
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5143795, dtype=float32)}
train_step here is:   420
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5091176, dtype=float32)}
train_step here is:   421
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.511073, dtype=float32)}
train_step here is:   422
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.544551, dtype=float32)}
train_step here is:   423
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4476457, dtype=float32)}
train_step here is:   424
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.430973, dtype=float32)}
train_step here is:   425
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.441951, dtype=float32)}
train_step here is:   426
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5030756, dtype=float32)}
train_step here is:   427
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4486213, dtype=float32)}
train_step here is:   428
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4675937, dtype=float32)}
train_step here is:   429
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5107, dtype=float32)}
train_step here is:   430
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.530998, dtype=float32)}
train_step here is:   431
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4556403, dtype=float32)}
train_step here is:   432
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.480373, dtype=float32)}
train_step here is:   433
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4388075, dtype=float32)}
train_step here is:   434
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4369283, dtype=float32)}
train_step here is:   435
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4422293, dtype=float32)}
train_step here is:   436
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.446998, dtype=float32)}
train_step here is:   437
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4925675, dtype=float32)}
train_step here is:   438
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4423833, dtype=float32)}
train_step here is:   439
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4478207, dtype=float32)}
train_step here is:   440
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4517236, dtype=float32)}
train_step here is:   441
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4847984, dtype=float32)}
train_step here is:   442
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.430029, dtype=float32)}
train_step here is:   443
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4368553, dtype=float32)}
train_step here is:   444
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.470345, dtype=float32)}
train_step here is:   445
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4250197, dtype=float32)}
train_step here is:   446
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.4733934, dtype=float32)}
train_step here is:   447
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.449446, dtype=float32)}
Epoch... (3/3 | Loss: 5.449446201324463, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  4.19it/s][A[A

Evaluating...: 3it [00:00,  9.58it/s][A[A

Evaluating...: 6it [00:00, 13.61it/s][A[A

Evaluating...: 8it [00:00, 14.34it/s][A[A

Evaluating...: 10it [00:00, 15.05it/s][A[A

Evaluating...: 12it [00:00, 15.44it/s][A[A

Evaluating...: 14it [00:01, 15.73it/s][A[A

Evaluating...: 16it [00:01, 10.88it/s][A[A

Evaluating...: 18it [00:01, 12.65it/s][A[A

Evaluating...: 20it [00:01, 11.38it/s][A[A

Evaluating...: 22it [00:01,  9.78it/s][A[A

Evaluating...: 24it [00:02,  9.58it/s][A[A

Evaluating...: 26it [00:02,  9.54it/s][A[A

Evaluating...: 28it [00:02, 11.03it/s][A[A

Evaluating...: 30it [00:02, 12.13it/s][A[A

Evaluating...: 32it [00:02,  9.80it/s][A[A

Evaluating...: 35it [00:03, 11.97it/s][A[A

Evaluating...: 37it [00:03, 12.95it/s][A[A

Evaluating...: 39it [00:03, 13.85it/s][A[A

Evaluating...: 41it [00:03, 14.40it/s][A[A

Evaluating...: 43it [00:03, 11.16it/s][A[A

Evaluating...: 45it [00:03, 10.05it/s][A[A

Evaluating...: 47it [00:04,  8.25it/s][A[A

Evaluating...: 49it [00:04,  9.16it/s][A[A

Evaluating...: 51it [00:04, 10.53it/s][A[A

Evaluating...: 53it [00:04, 12.01it/s][A[A

Evaluating...: 55it [00:04, 12.94it/s][A[A

Evaluating...: 57it [00:04, 13.77it/s][A[A

Evaluating...: 59it [00:05, 14.60it/s][A[A

Evaluating...: 61it [00:05, 14.95it/s][A[A

Evaluating...: 63it [00:05, 13.60it/s][A[A

Evaluating...: 65it [00:05, 14.86it/s][A[A

Evaluating...: 67it [00:05, 15.50it/s][A[A

                                      [A[A                                                                                 Epoch... (3/3 | Eval Loss: 5.505321979522705):  67% 2/3 [23:39<08:08, 488.92s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.484010219573975): 100% 3/3 [23:47<00:00, 455.29s/it]Epoch... (3/3 | Eval Loss: 5.484010219573975): 100% 3/3 [23:47<00:00, 475.84s/it]Epoch... (3/3 | Eval Loss: 5.484010219573975)

wandb: Waiting for W&B process to finish, PID 1683163... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▁▃▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss █▇▆▅▄▂▆▃▃▄▅▄▃▄▄▃▃▃▄▃▃▃▃▄▃▄▃▃▄▂▂▁▃▂▃▁▂▃▄▂
wandb:         train/samples ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/time ▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/time_per_step ▆▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.48401
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 5.44945
wandb:         train/samples 114432
wandb:            train/step 447
wandb:            train/time 1411.27466
wandb:   train/time_per_step 3.35659
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced elated-darkness-30: https://wandb.ai/gxucla/dalle-mini/runs/28iajpko
wandb: Find logs at: ./wandb/run-20220306_161834-28iajpko/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr3_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 149/149 [00:00<00:00, 19319.03it/s]
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 215417.43it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-71d0035c557750de
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run vague-moon-31
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/25odvskw
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_183641-25odvskw
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpv2kwesaw/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpamdg2e4x/added_tokens.json. We won't load it.
loading file /tmp/tmpamdg2e4x/vocab.json
loading file /tmp/tmpamdg2e4x/merges.txt
loading file /tmp/tmpamdg2e4x/tokenizer.json
loading file None
loading file /tmp/tmpamdg2e4x/special_tokens_map.json
loading file /tmp/tmpamdg2e4x/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 18:40:08.920308: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 18:40:31.995895: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:10, 190.51s/it][A
Training...: 2it [03:12, 79.85s/it] [A
Training...: 3it [03:15, 44.47s/it][A
Training...: 4it [03:18, 28.10s/it][A
Training...: 5it [03:20, 18.82s/it][A
Training...: 6it [03:23, 13.23s/it][A
Training...: 7it [03:25,  9.68s/it][A
Training...: 8it [03:27,  7.41s/it][A
Training...: 9it [03:30,  5.84s/it][A
Training...: 10it [03:32,  4.77s/it][A
Training...: 11it [03:35,  4.04s/it][A
Training...: 12it [03:37,  3.59s/it][A
Training...: 13it [03:40,  3.22s/it][A
Training...: 14it [03:42,  2.97s/it][A
Training...: 15it [03:44,  2.79s/it][A
Training...: 16it [03:47,  2.73s/it][A
Training...: 17it [03:49,  2.62s/it][A
Training...: 18it [03:52,  2.54s/it][A
Training...: 19it [03:54,  2.49s/it][A
Training...: 20it [03:57,  2.51s/it][A
Training...: 21it [03:59,  2.47s/it][A
Training...: 22it [04:01,  2.44s/it][A
Training...: 23it [04:04,  2.42s/it][A
Training...: 24it [04:06,  2.47s/it][A
Training...: 25it [04:09,  2.44s/it][A
Training...: 26it [04:11,  2.42s/it][A
Training...: 27it [04:13,  2.41s/it][A
Training...: 28it [04:16,  2.46s/it][A
Training...: 29it [04:18,  2.44s/it][A
Training...: 30it [04:21,  2.42s/it][A
Training...: 31it [04:23,  2.41s/it][A
Training...: 32it [04:26,  2.46s/it][A
Training...: 33it [04:28,  2.44s/it][A
Training...: 34it [04:30,  2.43s/it][A
Training...: 35it [04:33,  2.41s/it][A
Training...: 36it [04:35,  2.47s/it][A
Training...: 37it [04:38,  2.44s/it][A
Training...: 38it [04:40,  2.42s/it][A
Training...: 39it [04:43,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 40it [04:52,  4.66s/it][A
Training...: 41it [04:55,  3.99s/it][A
Training...: 42it [04:57,  3.51s/it][A
Training...: 43it [05:00,  3.23s/it][A
Training...: 44it [05:02,  2.98s/it][A
Training...: 45it [05:05,  2.80s/it][A
Training...: 46it [05:07,  2.67s/it][A
Training...: 47it [05:10,  2.65s/it][A
Training...: 48it [05:12,  2.57s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.232057, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.9847884, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.2918005, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(9.874969, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(12.241365, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.663812, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.7921796, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.577874, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.165443, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.920361, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.846353, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9604735, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2755365, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.556632, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.091694, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8978343, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8797956, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.884305, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.940757, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.004279, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9387236, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.001321, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0098467, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.074871, dtype=float32)}
train_step here is:   25
the learning
 rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0244913, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.036854, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.026886, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0439706, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.941501, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9531155, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.94478, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9699736, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.930073, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9281235, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9452105, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9291306, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.934164, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9696283, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.972806, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9583445, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9274616, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.975048, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.981167, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.093521, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0110917, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1304955, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.187302, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.100209, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_Training...: 49it [05:14,  2.51s/it][Arate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1519117, dtype=float32)}
Training...: 50it [05:17,  2.47s/it][A
Training...: 51it [05:19,  2.50s/it][A
Training...: 52it [05:22,  2.46s/it][A
Training...: 53it [05:24,  2.44s/it][A
Training...: 54it [05:26,  2.42s/it][A
Training...: 55it [05:29,  2.47s/it][A
Training...: 56it [05:31,  2.44s/it][A
Training...: 57it [05:34,  2.42s/it][A
Training...: 58it [05:36,  2.41s/it][A
Training...: 59it [05:39,  2.46s/it][A
Training...: 60it [05:41,  2.44s/it][A
Training...: 61it [05:44,  2.42s/it][A
Training...: 62it [05:46,  2.41s/it][A
Training...: 63it [05:48,  2.46s/it][A
Training...: 64it [05:51,  2.43s/it][A
Training...: 65it [05:53,  2.42s/it][A
Training...: 66it [05:56,  2.40s/it][A
Training...: 67it [05:58,  2.46s/it][A
Training...: 68it [06:01,  2.43s/it][A
Training...: 69it [06:03,  2.42s/it][A
Training...: 70it [06:05,  2.41s/it][A
Training...: 71it [06:08,  2.46s/it][A
Training...: 72it [06:10,  2.44s/it][A
Training...: 73it [06:13,  2.42s/it][A
Training...: 74it [06:15,  2.41s/it][A
Training...: 75it [06:18,  2.46s/it][A
Training...: 76it [06:20,  2.43s/it][A
Training...: 77it [06:22,  2.42s/it][A
Training...: 78it [06:25,  2.41s/it][A
Training...: 79it [06:27,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 80it [06:39,  5.12s/it][A
Training...: 81it [06:41,  4.31s/it][A
Training...: 82it [06:43,  3.73s/it][A
Training...: 83it [06:46,  3.39s/it][A
Training...: 84it [06:48,  3.08s/it][A
Training...: 85it [06:51,  2.87s/it][A
Training...: 86it [06:53,  2.78s/it][A
Training...: 87it [06:56,  2.66s/it][A
Training...: 88it [06:58,  2.58s/it][A
Training...: 89it [07:00,  2.51s/it][A
Training...: 90it [07:03,  2.53s/it][A
Training...: 91it [07:05,  2.49s/it][A
Training...: 92it [07:08,  2.45s/it][A
Training...: 93it [07:10,  2.43s/it][A
Training...: 94it [07:13,  2.48s/it][A
Training...: 95it [07:15,  2.45s/it][A
Training...: 96it [07:18,  2.42s/it][A
Training...: 97it [07:20,  2.41s/it][A

train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.263335, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.989722, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.917156, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8470583, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8655543, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.882313, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.890915, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.846897, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8624964, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8988895, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9564023, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9003215, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.890632, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8888917, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.900752, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.896961, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8895216, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8767056, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9112277, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.926595, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.941629, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.904554, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8697243, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8825912, dtype=float32)}
train_step here is:   74Training...: 98it [07:23,  2.60s/it][A
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8418655, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8577566, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8358426, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.857233, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8943634, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.990469, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0397363, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0126286, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.114018, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.405223, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.167608, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0415835, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9683814, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.934861, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.893008, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.906177, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.820973, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8113494, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8919597, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.034946, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0522785, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1002584, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.137034, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.013757, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.230737, dtype=float32)}
Training...: 99it [07:25,  2.53s/it][A
Training...: 100it [07:28,  2.49s/it][A
Training...: 101it [07:30,  2.46s/it][A
Training...: 102it [07:33,  2.49s/it][A
Training...: 103it [07:35,  2.45s/it][A
Training...: 104it [07:37,  2.43s/it][A
Training...: 105it [07:40,  2.41s/it][A
Training...: 106it [07:42,  2.47s/it][A
Training...: 107it [07:45,  2.44s/it][A
Training...: 108it [07:47,  2.42s/it][A
Training...: 109it [07:50,  2.41s/it][A
Training...: 110it [07:52,  2.46s/it][A
Training...: 111it [07:54,  2.44s/it][A
Training...: 112it [07:57,  2.42s/it][A
Training...: 113it [07:59,  2.41s/it][A
Training...: 114it [08:02,  2.46s/it][A
Training...: 115it [08:04,  2.44s/it][A
Training...: 116it [08:07,  2.42s/it][A
Training...: 117it [08:09,  2.41s/it][A
Training...: 118it [08:12,  2.47s/it][A
Training...: 119it [08:14,  2.44s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 120it [08:25,  5.15s/it][A
Training...: 121it [08:28,  4.33s/it][A
Training...: 122it [08:30,  3.81s/it][A
Training...: 123it [08:33,  3.39s/it][A
Training...: 124it [08:35,  3.09s/it][A
Training...: 125it [08:38,  2.87s/it][A
Training...: 126it [08:40,  2.79s/it][A
Training...: 127it [08:43,  2.67s/it][A
Training...: 128it [08:45,  2.58s/it][A
Training...: 129it [08:47,  2.55s/it][A
Training...: 130it [08:50,  2.50s/it][A
Training...: 131it [08:52,  2.47s/it][A
Training...: 132it [08:55,  2.44s/it][A
Training...: 133it [08:57,  2.45s/it][A
Training...: 134it [08:59,  2.43s/it][A
Training...: 135it [09:02,  2.41s/it][A
Training...: 136it [09:04,  2.40s/it][A
Training...: 137it [09:07,  2.44s/it][A
Training...: 138it [09:09,  2.42s/it][A
Training...: 139it [09:12,  2.41s/it][A
Training...: 140it [09:14,  2.40s/it][A
Training...: 141it [09:16,  2.43s/it][A
Training...: 142it [09:19,  2.42s/it][A
Training...: 143it [09:21,  2.40s/it][A
Training...: 144it [09:24,  2.40s/it][A
Training...: 145it [09:26,  2.43s/it][A
Training...: 146it [09:29,  2.55s/it][A
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.722823, dtype=float32)}
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2542315, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.022257, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9015837, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8288455, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.821829, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8061953, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.856641, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8820553, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8955607, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8605585, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.817314, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8185925, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8092504, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.826737, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.838846, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8090587, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.789158, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.79532, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7748413, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7685556, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.808839, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.804604, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8366346, dtype=float32)}
tr
ain_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8801556, dtype=float32)}
train_step here is:   124
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9253283, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.908798, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8897433, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8792295, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9022465, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.181802, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1131015, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2941303, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.276124, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.046649, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8545246, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7881775, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.738362, dtype=float32)}
train_step here is:   137
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6971207, dtype=float32)}
train_step here is:   138
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7633767, dtype=float32)}
train_step here is:   139
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.7676077, dtype=float32)}
train_step here is:   140
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2041564, dtype=float32)}
train_step here is:   141
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.006485, dtype=float32)}
train_step here is:   142
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.925168, dtype=float32)}
train_step here is:   143
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.856233, dtype=float32)}
train_step here is:   144
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.839134, dtype=float32)}
train_step here is:   145
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.778162, dtype=float32)}
train_step here is:   146
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.737172, dtype=float32)}
traiTraining...: 147it [09:31,  2.50s/it][An_step here is:   147
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.721932, dtype=float32)}
Training...: 148it [09:34,  2.46s/it][A
Training...: 149it [09:36,  2.45s/it][A
                                     [A                                          
train_step here is:   148
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.09666, dtype=float32)}
train_step here is:   149
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.576729, dtype=float32)}
Epoch... (1/3 | Loss: 7.576728820800781, Learning Rate: 0.004999999888241291)
Epoch ... (1/3):   0% 0/3 [09:36<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.57s/it][A[A

Evaluating...: 3it [00:12,  3.30s/it][A[A

Evaluating...: 5it [00:12,  1.63s/it][A[A

Evaluating...: 7it [00:12,  1.02it/s][A[A

Evaluating...: 9it [00:13,  1.54it/s][A[A

Evaluating...: 11it [00:13,  2.21it/s][A[A

Evaluating...: 13it [00:13,  2.94it/s][A[A

Evaluating...: 15it [00:13,  3.90it/s][A[A

Evaluating...: 17it [00:14,  4.50it/s][A[A

Evaluating...: 19it [00:14,  5.86it/s][A[A

Evaluating...: 21it [00:14,  7.27it/s][A[A

Evaluating...: 23it [00:14,  8.84it/s][A[A

Evaluating...: 25it [00:14,  6.02it/s][A[A

Evaluating...: 27it [00:15,  7.05it/s][A[A

Evaluating...: 29it [00:15,  7.36it/s][A[A

Evaluating...: 31it [00:15,  8.18it/s][A[A

Evaluating...: 33it [00:15,  7.70it/s][A[A

Evaluating...: 35it [00:16,  8.17it/s][A[A

Evaluating...: 37it [00:16,  9.79it/s][A[A

Evaluating...: 39it [00:16, 11.10it/s][A[A

Evaluating...: 41it [00:16, 12.15it/s][A[A

Evaluating...: 43it [00:16, 13.38it/s][A[A

Evaluating...: 45it [00:16, 14.00it/s][A[A

Evaluating...: 47it [00:17, 10.17it/s][A[A

Evaluating...: 49it [00:17, 11.82it/s][A[A

Evaluating...: 51it [00:17, 13.07it/s][A[A

Evaluating...: 53it [00:17, 10.35it/s][A[A

Evaluating...: 55it [00:17, 10.24it/s][A[A

Evaluating...: 57it [00:17, 10.62it/s][A[A

Evaluating...: 59it [00:18, 10.05it/s][A[A

Evaluating...: 61it [00:18, 10.69it/s][A[A

Evaluating...: 63it [00:18, 10.75it/s][A[A

Evaluating...: 65it [00:18, 12.38it/s][A[A

Evaluating...: 67it [00:18, 13.53it/s][A[A

                                      [A[A                                          Epoch ... (1/3):   0% 0/3 [09:55<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 7.462041854858398):  33% 1/3 [10:03<20:07, 603.71s/it]Epoch... (1/3 | Eval Loss: 7.462041854858398)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.03s/it][A
Training...: 2it [00:05,  2.65s/it][A
Training...: 3it [00:07,  2.54s/it][A
Training...: 4it [00:10,  2.56s/it][A
Training...: 5it [00:12,  2.49s/it][A
Training...: 6it [00:15,  2.45s/it][A
Training...: 7it [00:17,  2.43s/it][A
Training...: 8it [00:20,  2.48s/it][A
Training...: 9it [00:22,  2.45s/it][A
Training...: 10it [00:24,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 11it [00:34,  4.68s/it][A
Training...: 12it [00:37,  4.06s/it][A
Training...: 13it [00:39,  3.55s/it][A
Training...: 14it [00:42,  3.20s/it][A
Training...: 15it [00:44,  2.95s/it][A
Training...: 16it [00:47,  2.85s/it][A
Training...: 17it [00:49,  2.70s/it][A
Training...: 18it [00:51,  2.60s/it][A
Training...: 19it [00:54,  2.53s/it][A
Training...: 20it [00:56,  2.54s/it][A
Training...: 21it [00:59,  2.49s/it][A
Training...: 22it [01:01,  2.46s/it][A
Training...: 23it [01:03,  2.43s/it][A
Training...: 24it [01:06,  2.49s/it][A
Training...: 25it [01:08,  2.45s/it][A
Training...: 26it [01:11,  2.43s/it][A
Training...: 27it [01:13,  2.41s/it][A
Training...: 28it [01:16,  2.47s/it][A
Training...: 29it [01:18,  2.44s/it][A
Training...: 30it [01:20,  2.42s/it][A
Training...: 31it [01:23,  2.41s/it][A
Training...: 32it [01:26,  2.59s/it][A
Training...: 33it [01:28,  2.53s/it][A
Training...: 34it [01:31,  2.49s/it][A
Training...: 35it [01:33,  2.45s/it][A
Training...: 36it [01:36,  2.49s/it][A
Training...: 37it [01:38,  2.46s/it][A
Training...: 38it [01:40,  2.43s/it][A
Training...: 39it [01:43,  2.42s/it][A
Training...: 40it [01:45,  2.46s/it][A
Training...: 41it [01:48,  2.44s/it][A
Training...: 42it [01:50,  2.42s/it][A
Training...: 43it [01:53,  2.47s/it][A
Training...: 44it [01:55,  2.44s/it][A
Training...: 45it [01:57,  2.42s/it][A
Training...: 46it [02:00,  2.41s/it][A
Training...: 47it [02:02,  2.46s/it][A
Training...: 48it [02:05,  2.43s/it][A
train_step here is:   150
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.4853153, dtype=float32)}
train_step here is:   151
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1441865, dtype=float32)}
train_step here is:   152
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.046194, dtype=float32)}
train_step here is:   153
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.883457, dtype=float32)}
train_step here is:   154
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7630377, dtype=float32)}
train_step here is:   155
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.807044, dtype=float32)}
train_step here is:   156
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9135838, dtype=float32)}
train_step here is:   157
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8824306, dtype=float32)}
train_step here is:   158
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.776463, dtype=float32)}
train_step here is:   159
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8250136, dtype=float32)}
train_step here is:   160
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.881673, dtype=float32)}
train_step here is:   161
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.77044, dtype=float32)}
train_step here is:   162
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.692702, dtype=float32)}
train_step here is:   163
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.813612, dtype=float32)}
train_step here is:   164
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0896764, dtype=float32)}
train_step here is:   165
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9934316, dtype=float32)}
train_step here is:   166
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.702147, dtype=float32)}
train_step here is:   167
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.564236, dtype=float32)}
train_step here is:   168
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5701237, dtype=float32)}
train_step here is:   169
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.938739, dtype=float32)}
train_step here is:   170
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.192068, dtype=float32)}
train_step here is:   171
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.256778, dtype=float32)}
train_step here is:   172
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.875942, dtype=float32)}
train_step here is:   173
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6573935, dtype=float32)}
train_Training...: 49it [02:07,  2.42s/it][Astep here is:   174
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.989462, dtype=float32)}
train_step here is:   175
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0824356, dtype=float32)}
train_step here is:   176
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.813714, dtype=float32)}
train_step here is:   177
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.649774, dtype=float32)}
train_step here is:   178
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7716093, dtype=float32)}
train_step here is:   179
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.935475, dtype=float32)}
train_step here is:   180
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6571918, dtype=float32)}
train_step here is:   181
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.872973, dtype=float32)}
train_step here is:   182
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.822178, dtype=float32)}
train_step here is:   183
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.650858, dtype=float32)}
train_step here is:   184
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.283119, dtype=float32)}
train_step here is:   185
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.149761, dtype=float32)}
train_step here is:   186
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7811584, dtype=float32)}
train_step here is:   187
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4332013, dtype=float32)}
train_step here is:   188
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3501887, dtype=float32)}
train_step here is:   189
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.91107, dtype=float32)}
train_step here is:   190
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.184369, dtype=float32)}
train_step here is:   191
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.871082, dtype=float32)}
train_step here is:   192
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6511927, dtype=float32)}
train_step here is:   193
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4493585, dtype=float32)}
train_step here is:   194
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5125046, dtype=float32)}
train_step here is:   195
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8021746, dtype=float32)}
train_step here is:   196
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8233147, dtype=float32)}
train_step here is:   197
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.710713, dtype=float32)}
train_step here is:   198
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6604214, dtype=float32)}
Training...: 50it [02:09,  2.40s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 51it [02:21,  5.09s/it][A
Training...: 52it [02:23,  4.29s/it][A
Training...: 53it [02:26,  3.71s/it][A
Training...: 54it [02:28,  3.32s/it][A
Training...: 55it [02:31,  3.10s/it][A
Training...: 56it [02:33,  2.88s/it][A
Training...: 57it [02:35,  2.73s/it][A
Training...: 58it [02:38,  2.63s/it][A
Training...: 59it [02:40,  2.61s/it][A
Training...: 60it [02:43,  2.54s/it][A
Training...: 61it [02:45,  2.49s/it][A
Training...: 62it [02:47,  2.46s/it][A
Training...: 63it [02:50,  2.49s/it][A
Training...: 64it [02:52,  2.45s/it][A
Training...: 65it [02:55,  2.43s/it][A
Training...: 66it [02:57,  2.41s/it][A
Training...: 67it [03:00,  2.46s/it][A
Training...: 68it [03:02,  2.44s/it][A
Training...: 69it [03:04,  2.42s/it][A
Training...: 70it [03:07,  2.40s/it][A
Training...: 71it [03:09,  2.47s/it][A
Training...: 72it [03:12,  2.44s/it][A
Training...: 73it [03:14,  2.42s/it][A
Training...: 74it [03:17,  2.41s/it][A
Training...: 75it [03:19,  2.46s/it][A
Training...: 76it [03:21,  2.43s/it][A
Training...: 77it [03:24,  2.41s/it][A
Training...: 78it [03:26,  2.40s/it][A
Training...: 79it [03:29,  2.46s/it][A
Training...: 80it [03:31,  2.43s/it][A
Training...: 81it [03:34,  2.42s/it][A
Training...: 82it [03:36,  2.54s/it][A
Training...: 83it [03:39,  2.55s/it][A
Training...: 84it [03:41,  2.50s/it][A
Training...: 85it [03:44,  2.46s/it][A
Training...: 86it [03:46,  2.49s/it][A
Training...: 87it [03:49,  2.46s/it][A
Training...: 88it [03:51,  2.43s/it][A
Training...: 89it [03:53,  2.41s/it][A
Training...: 90it [03:56,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 91it [04:07,  5.15s/it][A
Training...: 92it [04:10,  4.32s/it][A
Training...: 93it [04:12,  3.74s/it][A
Training...: 94it [04:15,  3.40s/it][A
Training...: 95it [04:17,  3.09s/it][A
Training...: 96it [04:20,  2.88s/it][A
Training...: 97it [04:22,  2.73s/it][A
train_step here is:   199
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.458971, dtype=float32)}
train_step here is:   200
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6644793, dtype=float32)}
train_step here is:   201
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7307296, dtype=float32)}
train_step here is:   202
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.61284, dtype=float32)}
train_step here is:   203
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.148329, dtype=float32)}
train_step here is:   204
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0974884, dtype=float32)}
train_step here is:   205
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7163234, dtype=float32)}
train_step here is:   206
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4889836, dtype=float32)}
train_step here is:   207
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7023273, dtype=float32)}
train_step here is:   208
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.41835, dtype=float32)}
train_step here is:   209
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2422967, dtype=float32)}
train_step here is:   210
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.475826, dtype=float32)}
train_step here is:   211
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5327673, dtype=float32)}
train_step here is:   212
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.553452, dtype=float32)}
train_step here is:   213
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5037975, dtype=float32)}
train_step here is:   214
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.522342, dtype=float32)}
train_step here is:   215
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7133827, dtype=float32)}
train_step here is:   216
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6984434, dtype=float32)}
train_step here is:   217
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.418828, dtype=float32)}
train_step here is:   218
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4670577, dtype=float32)}
train_step here is:   219
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5821295, dtype=float32)}
train_step here is:   220
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.342616, dtype=float32)}
train_step here is:   221
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.381998, dtype=float32)}
train_step here is:   222
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7358446, dtype=float32)}
t
Training...: 98it [04:25,  2.69s/it][Arain_step here is:   223
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.679925, dtype=float32)}
train_step here is:   224
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.524916, dtype=float32)}
train_step here is:   225
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.586906, dtype=float32)}
train_step here is:   226
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5109453, dtype=float32)}
train_step here is:   227
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.471672, dtype=float32)}
train_step here is:   228
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3178673, dtype=float32)}
train_step here is:   229
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5010757, dtype=float32)}
train_step here is:   230
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6536245, dtype=float32)}
train_step here is:   231
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3246565, dtype=float32)}
train_step here is:   232
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.324173, dtype=float32)}
train_step here is:   233
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.709551, dtype=float32)}
train_step here is:   234
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6176853, dtype=float32)}
train_step here is:   235
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4092975, dtype=float32)}
train_step here is:   236
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3533335, dtype=float32)}
train_step here is:   237
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2795744, dtype=float32)}
train_step here is:   238
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5232778, dtype=float32)}
train_step here is:   239
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4128466, dtype=float32)}
train_step here is:   240
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.170657, dtype=float32)}
train_step here is:   241
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.229922, dtype=float32)}
train_step here is:   242
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5288754, dtype=float32)}
train_step here is:   243
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.375063, dtype=float32)}
train_step here is:   244
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.286336, dtype=float32)}
train_step here is:   245
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3687167, dtype=float32)}
train_step here is:   246
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3379827, dtype=float32)}
train_step here is:   247
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.356549, dtype=float32)}
Training...: 99it [04:27,  2.59s/it][A
Training...: 100it [04:29,  2.53s/it][A
Training...: 101it [04:32,  2.48s/it][A
Training...: 102it [04:34,  2.51s/it][A
Training...: 103it [04:37,  2.47s/it][A
Training...: 104it [04:39,  2.44s/it][A
Training...: 105it [04:41,  2.42s/it][A
Training...: 106it [04:44,  2.47s/it][A
Training...: 107it [04:46,  2.44s/it][A
Training...: 108it [04:49,  2.42s/it][A
Training...: 109it [04:51,  2.41s/it][A
Training...: 110it [04:54,  2.46s/it][A
Training...: 111it [04:56,  2.43s/it][A
Training...: 112it [04:58,  2.42s/it][A
Training...: 113it [05:01,  2.41s/it][A
Training...: 114it [05:03,  2.46s/it][A
Training...: 115it [05:06,  2.44s/it][A
Training...: 116it [05:08,  2.42s/it][A
Training...: 117it [05:10,  2.41s/it][A
Training...: 118it [05:13,  2.46s/it][A
Training...: 119it [05:15,  2.43s/it][A
Training...: 120it [05:18,  2.42s/it][A
Training...: 121it [05:20,  2.40s/it][A
Training...: 122it [05:23,  2.46s/it][A
Training...: 123it [05:25,  2.43s/it][A
Training...: 124it [05:28,  2.42s/it][A
Training...: 125it [05:30,  2.40s/it][A
Training...: 126it [05:32,  2.46s/it][A
Training...: 127it [05:35,  2.43s/it][A
Training...: 128it [05:37,  2.42s/it][A
Training...: 129it [05:40,  2.44s/it][A
Training...: 130it [05:43,  2.55s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 131it [05:54,  5.18s/it][A
Training...: 132it [05:56,  4.35s/it][A
Training...: 133it [05:59,  3.79s/it][A
Training...: 134it [06:01,  3.38s/it][A
Training...: 135it [06:04,  3.08s/it][A
Training...: 136it [06:06,  2.87s/it][A
Training...: 137it [06:08,  2.75s/it][A
Training...: 138it [06:11,  2.64s/it][A
Training...: 139it [06:13,  2.56s/it][A
Training...: 140it [06:16,  2.51s/it][A
Training...: 141it [06:18,  2.50s/it][A
Training...: 142it [06:20,  2.46s/it][A
Training...: 143it [06:23,  2.44s/it][A
Training...: 144it [06:25,  2.42s/it][A
Training...: 145it [06:28,  2.44s/it][A
Training...: 146it [06:30,  2.42s/it][A
train_step here is:   248
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.428135, dtype=float32)}
train_step here is:   249
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.343874, dtype=float32)}
train_step here is:   250
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4877195, dtype=float32)}
train_step here is:   251
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.42719, dtype=float32)}
train_step here is:   252
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.185522, dtype=float32)}
train_step here is:   253
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1924267, dtype=float32)}
train_step here is:   254
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4405456, dtype=float32)}
train_step here is:   255
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.390028, dtype=float32)}
train_step here is:   256
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.153051, dtype=float32)}
train_step here is:   257
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3607917, dtype=float32)}
train_step here is:   258
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.209091, dtype=float32)}
train_step here is:   259
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.229076, dtype=float32)}
train_step here is:   260
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.427042, dtype=float32)}
train_step here is:   261
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1455016, dtype=float32)}
train_step here is:   262
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1511636, dtype=float32)}
train_step here is:   263
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5202475, dtype=float32)}
train_step here is:   264
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3043547, dtype=float32)}
train_step here is:   265
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.139203, dtype=float32)}
train_step here is:   266
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2119684, dtype=float32)}
train_step here is:   267
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2871027, dtype=float32)}
train_step here is:   268
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3682127, dtype=float32)}
train_step here is:   269
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.43452, dtype=float32)}
train_step here is:   270
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2526054, dtype=float32)}
train_step here is:   271
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1358914, dtype=float32)}
tr
Training...: 147it [06:32,  2.41s/it][Aain_step here is:   272
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.24347, dtype=float32)}
train_step here is:   273
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.040263, dtype=float32)}
train_step here is:   274
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1483603, dtype=float32)}
train_step here is:   275
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5116343, dtype=float32)}
train_step here is:   276
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3119135, dtype=float32)}
train_step here is:   277
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.259826, dtype=float32)}
train_step here is:   278
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.340939, dtype=float32)}
train_step here is:   279
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.137929, dtype=float32)}
train_step here is:   280
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2244596, dtype=float32)}
train_step here is:   281
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.418755, dtype=float32)}
train_step here is:   282
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2872825, dtype=float32)}
train_step here is:   283
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4072294, dtype=float32)}
train_step here is:   284
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5301003, dtype=float32)}
train_step here is:   285
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.198394, dtype=float32)}
train_step here is:   286
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1544256, dtype=float32)}
train_step here is:   287
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.103978, dtype=float32)}
train_step here is:   288
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1266623, dtype=float32)}
train_step here is:   289
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2040434, dtype=float32)}
train_step here is:   290
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1095996, dtype=float32)}
train_step here is:   291
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1092114, dtype=float32)}
train_step here is:   292
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.014074, dtype=float32)}
train_step here is:   293
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.050565, dtype=float32)}
train_step here is:   294
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0184426, dtype=float32)}
train_step here is:   295
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1039643, dtype=float32)}
train_step here is:   296
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.208359, dtype=float32)}
Training...: 148it [06:35,  2.40s/it][A
Training...: 149it [06:37,  2.41s/it][A
                                     [A                                                                                 Epoch... (1/3 | Eval Loss: 7.462041854858398):  33% 1/3 [16:41<20:07, 603.71s/it]

Evaluating...: 0it [00:00, ?it/s][A[A
train_step here is:   297
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3133726, dtype=float32)}
train_step here is:   298
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1858454, dtype=float32)}
Epoch... (2/3 | Loss: 6.185845375061035, Learning Rate: 0.004999999888241291)


Evaluating...: 1it [00:00,  4.19it/s][A[A

Evaluating...: 4it [00:00, 11.21it/s][A[A

Evaluating...: 6it [00:00, 13.26it/s][A[A

Evaluating...: 8it [00:00, 14.18it/s][A[A

Evaluating...: 10it [00:00, 14.67it/s][A[A

Evaluating...: 12it [00:00, 15.34it/s][A[A

Evaluating...: 14it [00:01, 13.23it/s][A[A

Evaluating...: 16it [00:01,  8.75it/s][A[A

Evaluating...: 18it [00:01, 10.31it/s][A[A

Evaluating...: 20it [00:01,  9.37it/s][A[A

Evaluating...: 22it [00:02,  9.95it/s][A[A

Evaluating...: 24it [00:02, 11.36it/s][A[A

Evaluating...: 26it [00:02, 12.52it/s][A[A

Evaluating...: 28it [00:02, 13.41it/s][A[A

Evaluating...: 30it [00:02, 14.18it/s][A[A

Evaluating...: 32it [00:02, 10.43it/s][A[A

Evaluating...: 35it [00:02, 12.62it/s][A[A

Evaluating...: 37it [00:03, 13.36it/s][A[A

Evaluating...: 39it [00:03, 10.97it/s][A[A

Evaluating...: 41it [00:03,  9.77it/s][A[A

Evaluating...: 43it [00:03,  9.48it/s][A[A

Evaluating...: 45it [00:04,  8.74it/s][A[A

Evaluating...: 47it [00:04,  7.84it/s][A[A

Evaluating...: 49it [00:04,  9.43it/s][A[A

Evaluating...: 51it [00:04, 10.85it/s][A[A

Evaluating...: 53it [00:04, 12.12it/s][A[A

Evaluating...: 55it [00:04, 13.15it/s][A[A

Evaluating...: 57it [00:05, 14.01it/s][A[A

Evaluating...: 59it [00:05, 14.68it/s][A[A

Evaluating...: 61it [00:05, 15.20it/s][A[A

Evaluating...: 63it [00:05, 11.15it/s][A[A

Evaluating...: 65it [00:05, 10.54it/s][A[A

Evaluating...: 67it [00:06,  9.23it/s][A[A

                                      [A[A                                                                                 Epoch... (1/3 | Eval Loss: 7.462041854858398):  33% 1/3 [16:47<20:07, 603.71s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 6.2548041343688965):  67% 2/3 [16:55<08:11, 491.04s/it]Epoch... (2/3 | Eval Loss: 6.2548041343688965)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.02s/it][A
Training...: 2it [00:05,  2.64s/it][A
Training...: 3it [00:07,  2.54s/it][A
Training...: 4it [00:10,  2.56s/it][A
Training...: 5it [00:13,  2.65s/it][A
Training...: 6it [00:15,  2.56s/it][A
Training...: 7it [00:17,  2.50s/it][A
Training...: 8it [00:20,  2.52s/it][A
Training...: 9it [00:22,  2.48s/it][A
Training...: 10it [00:25,  2.45s/it][A
Training...: 11it [00:27,  2.42s/it][A
Training...: 12it [00:30,  2.47s/it][A
Training...: 13it [00:32,  2.44s/it][A
Training...: 14it [00:35,  2.42s/it][A
Training...: 15it [00:37,  2.41s/it][A
Training...: 16it [00:39,  2.46s/it][A
Training...: 17it [00:42,  2.44s/it][A
Training...: 18it [00:44,  2.42s/it][A
Training...: 19it [00:47,  2.41s/it][A
Training...: 20it [00:49,  2.45s/it][A
Training...: 21it [00:52,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 22it [01:03,  5.10s/it][A
Training...: 23it [01:05,  4.29s/it][A
Training...: 24it [01:08,  3.78s/it][A
Training...: 25it [01:10,  3.37s/it][A
Training...: 26it [01:13,  3.07s/it][A
Training...: 27it [01:15,  2.86s/it][A
Training...: 28it [01:18,  2.78s/it][A
Training...: 29it [01:20,  2.66s/it][A
Training...: 30it [01:22,  2.57s/it][A
Training...: 31it [01:25,  2.51s/it][A
Training...: 32it [01:27,  2.54s/it][A
Training...: 33it [01:30,  2.49s/it][A
Training...: 34it [01:32,  2.46s/it][A
Training...: 35it [01:34,  2.43s/it][A
Training...: 36it [01:37,  2.49s/it][A
Training...: 37it [01:39,  2.46s/it][A
Training...: 38it [01:42,  2.43s/it][A
Training...: 39it [01:44,  2.42s/it][A
Training...: 40it [01:47,  2.47s/it][A
Training...: 41it [01:49,  2.44s/it][A
Training...: 42it [01:52,  2.42s/it][A
Training...: 43it [01:54,  2.47s/it][A
Training...: 44it [01:57,  2.44s/it][A
Training...: 45it [01:59,  2.42s/it][A
Training...: 46it [02:01,  2.41s/it][A
Training...: 47it [02:04,  2.46s/it][A
Training...: 48it [02:06,  2.43s/it][Atrain_step here is:   299
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.274192, dtype=float32)}
train_step here is:   300
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.373377, dtype=float32)}
train_step here is:   301
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.145767, dtype=float32)}
train_step here is:   302
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3302903, dtype=float32)}
train_step here is:   303
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4827743, dtype=float32)}
train_step here is:   304
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0074253, dtype=float32)}
train_step here is:   305
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9228897, dtype=float32)}
train_step here is:   306
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.107039, dtype=float32)}
train_step here is:   307
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.039442, dtype=float32)}
train_step here is:   308
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9354444, dtype=float32)}
train_step here is:   309
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.925171, dtype=float32)}
train_step here is:   310
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0608387, dtype=float32)}
train_step here is:   311
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9893475, dtype=float32)}
train_step here is:   312
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.96052, dtype=float32)}
train_step here is:   313
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.031535, dtype=float32)}
train_step here is:   314
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0218763, dtype=float32)}
train_step here is:   315
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.985569, dtype=float32)}
train_step here is:   316
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.109373, dtype=float32)}
train_step here is:   317
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9527, dtype=float32)}
train_step here is:   318
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.941573, dtype=float32)}
train_step here is:   319
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0433483, dtype=float32)}
train_step here is:   320
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9289107, dtype=float32)}
train_step here is:   321
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.064308, dtype=float32)}
train_step here is:   322
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2978096, dtype=float32)}
train_s
Training...: 49it [02:09,  2.42s/it][Atep here is:   323
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0165825, dtype=float32)}
train_step here is:   324
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.059824, dtype=float32)}
train_step here is:   325
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.1625795, dtype=float32)}
train_step here is:   326
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.960362, dtype=float32)}
train_step here is:   327
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0460205, dtype=float32)}
train_step here is:   328
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.334464, dtype=float32)}
train_step here is:   329
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.226199, dtype=float32)}
train_step here is:   330
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.136949, dtype=float32)}
train_step here is:   331
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2060704, dtype=float32)}
train_step here is:   332
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.2175865, dtype=float32)}
train_step here is:   333
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.096367, dtype=float32)}
train_step here is:   334
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.979913, dtype=float32)}
train_step here is:   335
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0275035, dtype=float32)}
train_step here is:   336
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0610957, dtype=float32)}
train_step here is:   337
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8246217, dtype=float32)}
train_step here is:   338
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.89914, dtype=float32)}
train_step here is:   339
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.030739, dtype=float32)}
train_step here is:   340
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.066698, dtype=float32)}
train_step here is:   341
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9731536, dtype=float32)}
train_step here is:   342
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.870585, dtype=float32)}
train_step here is:   343
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8922834, dtype=float32)}
train_step here is:   344
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.847818, dtype=float32)}
train_step here is:   345
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8589344, dtype=float32)}
train_step here is:   346
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8295364, dtype=float32)}
train_step here is:   347
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.847464, dtype=float32)}
Training...: 50it [02:11,  2.41s/it][A
Training...: 51it [02:14,  2.47s/it][A
Training...: 52it [02:16,  2.44s/it][A
Training...: 53it [02:18,  2.42s/it][A
Training...: 54it [02:21,  2.54s/it][A
Training...: 55it [02:24,  2.55s/it][A
Training...: 56it [02:26,  2.50s/it][A
Training...: 57it [02:28,  2.46s/it][A
Training...: 58it [02:31,  2.44s/it][A
Training...: 59it [02:33,  2.48s/it][A
Training...: 60it [02:36,  2.45s/it][A
Training...: 61it [02:38,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 62it [02:50,  5.09s/it][A
Training...: 63it [02:52,  4.34s/it][A
Training...: 64it [02:54,  3.75s/it][A
Training...: 65it [02:57,  3.35s/it][A
Training...: 66it [02:59,  3.06s/it][A
Training...: 67it [03:02,  2.91s/it][A
Training...: 68it [03:04,  2.75s/it][A
Training...: 69it [03:07,  2.64s/it][A
Training...: 70it [03:09,  2.56s/it][A
Training...: 71it [03:12,  2.56s/it][A
Training...: 72it [03:14,  2.51s/it][A
Training...: 73it [03:16,  2.47s/it][A
Training...: 74it [03:19,  2.44s/it][A
Training...: 75it [03:21,  2.49s/it][A
Training...: 76it [03:24,  2.45s/it][A
Training...: 77it [03:26,  2.43s/it][A
Training...: 78it [03:28,  2.41s/it][A
Training...: 79it [03:31,  2.46s/it][A
Training...: 80it [03:33,  2.44s/it][A
Training...: 81it [03:36,  2.42s/it][A
Training...: 82it [03:38,  2.41s/it][A
Training...: 83it [03:41,  2.46s/it][A
Training...: 84it [03:43,  2.43s/it][A
Training...: 85it [03:45,  2.42s/it][A
Training...: 86it [03:48,  2.47s/it][A
Training...: 87it [03:50,  2.44s/it][A
Training...: 88it [03:53,  2.42s/it][A
Training...: 89it [03:55,  2.41s/it][A
Training...: 90it [03:58,  2.46s/it][A
Training...: 91it [04:00,  2.43s/it][A
Training...: 92it [04:02,  2.42s/it][A
Training...: 93it [04:05,  2.41s/it][A
Training...: 94it [04:07,  2.46s/it][A
Training...: 95it [04:10,  2.44s/it][A
Training...: 96it [04:12,  2.42s/it][A
Training...: 97it [04:15,  2.41s/it][A

train_step here is:   348
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.816026, dtype=float32)}
train_step here is:   349
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9638605, dtype=float32)}
train_step here is:   350
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9364777, dtype=float32)}
train_step here is:   351
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8991456, dtype=float32)}
train_step here is:   352
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.93552, dtype=float32)}
train_step here is:   353
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.922436, dtype=float32)}
train_step here is:   354
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.047993, dtype=float32)}
train_step here is:   355
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.126424, dtype=float32)}
train_step here is:   356
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.873987, dtype=float32)}
train_step here is:   357
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8789873, dtype=float32)}
train_step here is:   358
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8098726, dtype=float32)}
train_step here is:   359
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.895439, dtype=float32)}
train_step here is:   360
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.831502, dtype=float32)}
train_step here is:   361
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.803813, dtype=float32)}
train_step here is:   362
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.026555, dtype=float32)}
train_step here is:   363
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9288063, dtype=float32)}
train_step here is:   364
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0925674, dtype=float32)}
train_step here is:   365
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.125265, dtype=float32)}
train_step here is:   366
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.877119, dtype=float32)}
train_step here is:   367
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.844017, dtype=float32)}
train_step here is:   368
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0805025, dtype=float32)}
train_step here is:   369
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.192533, dtype=float32)}
train_step here is:   370
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0272703, dtype=float32)}
train_step here is:   371
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9755826, dtype=float32)}
train_step here is:   372
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8977194, dtype=float32)}
train_step here is:   373
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9124684, dtype=float32)}
train_step here is:   374
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9676933, dtype=float32)}
train_step here is:   375
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9697022, dtype=float32)}
train_step here is:   376
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.818516, dtype=float32)}
train_step here is:   377
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8781304, dtype=float32)}
train_step here is:   378
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8934045, dtype=float32)}
train_step here is:   379
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.800139, dtype=float32)}
train_step here is:   380
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7606077, dtype=float32)}
train_step here is:   381
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8944798, dtype=float32)}
train_step here is:   382
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.979625, dtype=float32)}
train_step here is:   383
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9900537, dtype=float32)}
train_step here is:   384
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.846979, dtype=float32)}
train_step here is:   385
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7566214, dtype=float32)}
train_step here is:   386
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.827539, dtype=float32)}
train_step here is:   387
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8684726, dtype=float32)}
train_step here is:   388
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8275576, dtype=float32)}
train_step here is:   389
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7978535, dtype=float32)}
train_step here is:   390
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8550477, dtype=float32)}
train_step here is:   391
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9850526, dtype=float32)}
train_step here is:   392
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.779937, dtype=float32)}
train_step here is:   393
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9666567, dtype=float32)}
train_step here is:   394
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.108716, dtype=float32)}
train_step here is:   395
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.895766, dtype=float32)}
traTraining...: 98it [04:17,  2.46s/it][Ain_step here is:   396
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8898726, dtype=float32)}
Training...: 99it [04:20,  2.43s/it][A
Training...: 100it [04:22,  2.42s/it][A
Training...: 101it [04:24,  2.41s/it][A
train_step here is:   397
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9951496, dtype=float32)}
train_step here is:   398
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.855644, dtype=float32)}
train_step here is:   399
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.929452, dtype=float32)}
train_step here is:   400
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.782731, dtype=float32)}


Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  1.40it/s][A[A

Evaluating...: 3it [00:00,  3.99it/s][A[A

Evaluating...: 4it [00:01,  4.94it/s][A[A

Evaluating...: 5it [00:01,  5.93it/s][A[A

Evaluating...: 6it [00:01,  6.79it/s][A[A

Evaluating...: 8it [00:01,  8.23it/s][A[A

Evaluating...: 9it [00:01,  8.58it/s][A[A

Evaluating...: 10it [00:01,  8.63it/s][A[A

Evaluating...: 12it [00:01, 10.46it/s][A[A

Evaluating...: 14it [00:01, 12.13it/s][A[A

Evaluating...: 16it [00:02,  9.05it/s][A[A

Evaluating...: 19it [00:02, 11.63it/s][A[A

Evaluating...: 21it [00:02, 12.68it/s][A[A

Evaluating...: 23it [00:02, 13.64it/s][A[A

Evaluating...: 25it [00:02, 14.23it/s][A[A

Evaluating...: 27it [00:02, 12.81it/s][A[A

Evaluating...: 29it [00:03, 11.80it/s][A[A

Evaluating...: 31it [00:03, 11.26it/s][A[A

Evaluating...: 33it [00:03,  9.30it/s][A[A

Evaluating...: 35it [00:03,  9.62it/s][A[A

Evaluating...: 37it [00:03, 10.59it/s][A[A

Evaluating...: 39it [00:04, 11.79it/s][A[A

Evaluating...: 41it [00:04, 13.00it/s][A[A

Evaluating...: 43it [00:04, 13.98it/s][A[A

Evaluating...: 45it [00:04, 14.64it/s][A[A

Evaluating...: 47it [00:04, 10.55it/s][A[A

Evaluating...: 50it [00:04, 12.70it/s][A[A

Evaluating...: 52it [00:05, 11.79it/s][A[A

Evaluating...: 54it [00:05, 10.91it/s][A[A

Evaluating...: 56it [00:05, 10.62it/s][A[A

Evaluating...: 58it [00:05, 10.62it/s][A[A

Evaluating...: 60it [00:05, 10.21it/s][A[A

Evaluating...: 62it [00:06, 11.19it/s][A[A

Evaluating...: 64it [00:06, 11.38it/s][A[A

Evaluating...: 66it [00:06, 12.81it/s][A[A

                                      [A[A                                                                                  
                                     [AEpoch... (2/3 | Eval Loss: 6.2548041343688965):  67% 2/3 [21:29<08:11, 491.04s/it]
Training...: 101it [04:33,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 102it [04:42,  7.06s/it][A
Training...: 103it [04:45,  5.66s/it][A
Training...: 104it [04:47,  4.68s/it][A
Training...: 105it [04:49,  4.00s/it][A
Training...: 106it [04:52,  3.57s/it][A
Training...: 107it [04:54,  3.22s/it][A
Training...: 108it [04:57,  2.96s/it][A
Training...: 109it [04:59,  2.79s/it][A
Training...: 110it [05:02,  2.73s/it][A
Training...: 111it [05:04,  2.62s/it][A
Training...: 112it [05:06,  2.55s/it][A
Training...: 113it [05:09,  2.50s/it][A
Training...: 114it [05:11,  2.52s/it][A
Training...: 115it [05:14,  2.48s/it][A
Training...: 116it [05:16,  2.45s/it][A
Training...: 117it [05:19,  2.43s/it][A
Training...: 118it [05:21,  2.47s/it][A
Training...: 119it [05:24,  2.45s/it][A
Training...: 120it [05:26,  2.43s/it][A
Training...: 121it [05:28,  2.41s/it][A
Training...: 122it [05:31,  2.47s/it][A
Training...: 123it [05:33,  2.44s/it][A
Training...: 124it [05:36,  2.42s/it][A
Training...: 125it [05:38,  2.41s/it][A
Training...: 126it [05:41,  2.46s/it][A
Training...: 127it [05:43,  2.57s/it][A
Training...: 128it [05:46,  2.51s/it][A
Training...: 129it [05:48,  2.51s/it][A
Training...: 130it [05:51,  2.47s/it][A
Training...: 131it [05:53,  2.45s/it][A
Training...: 132it [05:55,  2.43s/it][A
Training...: 133it [05:58,  2.45s/it][A
Training...: 134it [06:00,  2.43s/it][A
Training...: 135it [06:03,  2.41s/it][A
Training...: 136it [06:05,  2.41s/it][A
Training...: 137it [06:08,  2.43s/it][A
Training...: 138it [06:10,  2.42s/it][A
Training...: 139it [06:12,  2.40s/it][A
Training...: 140it [06:15,  2.40s/it][A
Training...: 141it [06:17,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json

Training...: 142it [06:29,  5.14s/it][A
Training...: 143it [06:31,  4.32s/it][A
Training...: 144it [06:33,  3.74s/it][A
Training...: 145it [06:36,  3.37s/it][A
Training...: 146it [06:38,  3.08s/it][A
Training...: 147it [06:41,  2.86s/it][A
Training...: 148it [06:43,  2.72s/it][A
Training...: 149it [06:46,  2.63s/it][A
                                     [AEpoch... (3/3 | Eval Loss: 5.816659927368164)
train_step here is:   401
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8154173, dtype=float32)}
train_step here is:   402
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.887094, dtype=float32)}
train_step here is:   403
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.866294, dtype=float32)}
train_step here is:   404
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8615675, dtype=float32)}
train_step here is:   405
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8230467, dtype=float32)}
train_step here is:   406
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.841445, dtype=float32)}
train_step here is:   407
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9076753, dtype=float32)}
train_step here is:   408
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.851104, dtype=float32)}
train_step here is:   409
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.865898, dtype=float32)}
train_step here is:   410
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7829657, dtype=float32)}
train_step here is:   411
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.857109, dtype=float32)}
train_step here is:   412
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8578835, dtype=float32)}
train_step here is:   413
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8699384, dtype=float32)}
train_step here is:   414
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.790042, dtype=float32)}
train_step here is:   415
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7994876, dtype=float32)}
train_step here is:   416
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7136245, dtype=float32)}
train_step here is:   417
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.78155, dtype=float32)}
train_step here is:   418
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8678575, dtype=float32)}
train_step here is:   419
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8592377, dtype=float32)}
train_step here is:   420
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8559933, dtype=float32)}
train_step here is:   421
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.810597, dtype=float32)}
train_step here is:   422
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.765444, dtype=float32)}
train_step here is:   423
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9484706, dtype=float32)}
train_step here is:   424
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': Sha                                                                                  Epoch... (3/3 | Eval Loss: 5.816659927368164):  67% 2/3 [23:41<08:11, 491.04s/it]

Evaluating...: 0it [00:00, ?it/s][A[ArdedDeviceArray(5.9583354, dtype=float32)}
train_step here is:   425
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0680304, dtype=float32)}
train_step here is:   426
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.082829, dtype=float32)}
train_step here is:   427
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.794894, dtype=float32)}
train_step here is:   428
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9842367, dtype=float32)}
train_step here is:   429
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.0403895, dtype=float32)}
train_step here is:   430
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.851656, dtype=float32)}
train_step here is:   431
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.943495, dtype=float32)}
train_step here is:   432
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.835713, dtype=float32)}
train_step here is:   433
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8869944, dtype=float32)}
train_step here is:   434
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8605046, dtype=float32)}
train_step here is:   435
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.894585, dtype=float32)}
train_step here is:   436
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9240046, dtype=float32)}
train_step here is:   437
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.9366355, dtype=float32)}
train_step here is:   438
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8261466, dtype=float32)}
train_step here is:   439
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8980136, dtype=float32)}
train_step here is:   440
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.972756, dtype=float32)}
train_step here is:   441
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.798423, dtype=float32)}
train_step here is:   442
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8221965, dtype=float32)}
train_step here is:   443
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8425846, dtype=float32)}
train_step here is:   444
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7982607, dtype=float32)}
train_step here is:   445
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7733045, dtype=float32)}
train_step here is:   446
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.7591333, dtype=float32)}
train_step here is:   447
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(5.8520794, dtype=float32)}
Epoch... (3/3 | Loss: 5.852079391479492, Learning Rate: 0.004999999888241291)


Evaluating...: 1it [00:00,  4.12it/s][A[A

Evaluating...: 3it [00:00,  9.50it/s][A[A

Evaluating...: 5it [00:00, 10.55it/s][A[A

Evaluating...: 7it [00:00,  9.25it/s][A[A

Evaluating...: 9it [00:00,  9.39it/s][A[A

Evaluating...: 11it [00:01,  8.45it/s][A[A

Evaluating...: 12it [00:01,  8.53it/s][A[A

Evaluating...: 14it [00:01, 10.17it/s][A[A

Evaluating...: 16it [00:01,  8.56it/s][A[A

Evaluating...: 18it [00:01, 10.50it/s][A[A

Evaluating...: 20it [00:02, 11.98it/s][A[A

Evaluating...: 22it [00:02, 13.20it/s][A[A

Evaluating...: 24it [00:02, 13.91it/s][A[A

Evaluating...: 26it [00:02, 14.68it/s][A[A

Evaluating...: 28it [00:02, 15.22it/s][A[A

Evaluating...: 30it [00:02, 11.38it/s][A[A

Evaluating...: 32it [00:03,  8.22it/s][A[A

Evaluating...: 34it [00:03,  9.45it/s][A[A

Evaluating...: 36it [00:03,  9.04it/s][A[A

Evaluating...: 38it [00:03, 10.56it/s][A[A

Evaluating...: 40it [00:03, 11.74it/s][A[A

Evaluating...: 42it [00:03, 12.83it/s][A[A

Evaluating...: 44it [00:04, 13.73it/s][A[A

Evaluating...: 46it [00:04, 14.45it/s][A[A

Evaluating...: 48it [00:04, 10.73it/s][A[A

Evaluating...: 50it [00:04, 12.14it/s][A[A

Evaluating...: 52it [00:04, 13.23it/s][A[A

Evaluating...: 54it [00:04, 12.20it/s][A[A

Evaluating...: 56it [00:05, 11.16it/s][A[A

Evaluating...: 58it [00:05,  9.68it/s][A[A

Evaluating...: 60it [00:05,  9.52it/s][A[A

Evaluating...: 62it [00:05,  9.39it/s][A[A

Evaluating...: 63it [00:05,  9.25it/s][A[A

Evaluating...: 66it [00:06, 11.79it/s][A[A

                                      [A[A                                                                                 Epoch... (3/3 | Eval Loss: 5.816659927368164):  67% 2/3 [23:48<08:11, 491.04s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.869821071624756): 100% 3/3 [23:54<00:00, 458.07s/it]Epoch... (3/3 | Eval Loss: 5.869821071624756): 100% 3/3 [23:54<00:00, 478.24s/it]Epoch... (3/3 | Eval Loss: 5.869821071624756)

wandb: Waiting for W&B process to finish, PID 1710931... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▃▁▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss ▅▆▆▆▇▆▆▅▇▅▅▆██▅▆▄▄▄▃▄▃▄▃▃▂▃▃▂▂▂▂▁▂▁▁▁▁▁▁
wandb:         train/samples ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/time ▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/time_per_step ▆▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.86982
wandb:           train/epoch 0
wandb:   train/learning_rate 0.005
wandb:            train/loss 5.85208
wandb:         train/samples 114432
wandb:            train/step 447
wandb:            train/time 1419.55225
wandb:   train/time_per_step 3.71047
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced vague-moon-31: https://wandb.ai/gxucla/dalle-mini/runs/25odvskw
wandb: Find logs at: ./wandb/run-20220306_183641-25odvskw/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr5_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=5e-05, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 149/149 [00:00<00:00, 13847.80it/s]
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 220071.51it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-71d0035c557750de
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run valiant-energy-32
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/1g5swf6w
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_190823-1g5swf6w
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmppa_kfveo/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpjvebv5o9/added_tokens.json. We won't load it.
loading file /tmp/tmpjvebv5o9/vocab.json
loading file /tmp/tmpjvebv5o9/merges.txt
loading file /tmp/tmpjvebv5o9/tokenizer.json
loading file None
loading file /tmp/tmpjvebv5o9/special_tokens_map.json
loading file /tmp/tmpjvebv5o9/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 19:11:49.967730: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 19:12:12.036092: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:10, 190.19s/it][A
Training...: 2it [03:12, 79.71s/it] [A
Training...: 3it [03:14, 44.40s/it][A
Training...: 4it [03:17, 27.89s/it][A
Training...: 5it [03:19, 18.70s/it][A
Training...: 6it [03:22, 13.14s/it][A
Training...: 7it [03:24,  9.63s/it][A
Training...: 8it [03:27,  7.38s/it][A
Training...: 9it [03:29,  5.82s/it][A
Training...: 10it [03:32,  4.76s/it][A
Training...: 11it [03:34,  4.03s/it][A
Training...: 12it [03:36,  3.59s/it][A
Training...: 13it [03:39,  3.22s/it][A
Training...: 14it [03:41,  2.97s/it][A
Training...: 15it [03:44,  2.78s/it][A
Training...: 16it [03:46,  2.73s/it][A
Training...: 17it [03:49,  2.62s/it][A
Training...: 18it [03:51,  2.55s/it][A
Training...: 19it [03:53,  2.49s/it][A
Training...: 20it [03:56,  2.51s/it][A
Training...: 21it [03:58,  2.47s/it][A
Training...: 22it [04:01,  2.44s/it][A
Training...: 23it [04:03,  2.42s/it][A
Training...: 24it [04:06,  2.47s/it][A
Training...: 25it [04:08,  2.44s/it][A
Training...: 26it [04:10,  2.42s/it][A
Training...: 27it [04:13,  2.41s/it][A
Training...: 28it [04:16,  2.59s/it][A
Training...: 29it [04:18,  2.53s/it][A
Training...: 30it [04:20,  2.48s/it][A
Training...: 31it [04:23,  2.45s/it][A
Training...: 32it [04:25,  2.49s/it][A
Training...: 33it [04:28,  2.46s/it][A
Training...: 34it [04:30,  2.44s/it][A
Training...: 35it [04:33,  2.42s/it][A
Training...: 36it [04:35,  2.47s/it][A
Training...: 37it [04:38,  2.44s/it][A
Training...: 38it [04:40,  2.42s/it][A
Training...: 39it [04:42,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 40it [04:51,  4.43s/it][A
Training...: 41it [04:54,  3.83s/it][A
Training...: 42it [04:56,  3.39s/it][A
Training...: 43it [04:59,  3.15s/it][A
Training...: 44it [05:01,  2.92s/it][A
Training...: 45it [05:04,  2.76s/it][A
Training...: 46it [05:06,  2.64s/it][A
Training...: 47it [05:09,  2.63s/it][A
Training...: 48it [05:11,  2.56s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.92138, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.081294, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.7517533, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.590703, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.137411, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.9145603, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.6837316, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.426363, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.3770075, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.221508, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.186543, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0569086, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.998785, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.980507, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.867673, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8838253, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.9283733, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8690867, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.806548, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7772593, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.86228, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.81461, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8274627, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.78824, dtype=float32)}
train_step here is
:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.708391, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7639575, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.800159, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.773504, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6809864, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.742398, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6583257, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7155647, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.708172, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.704882, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6814194, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6484203, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6813445, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6431694, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.653774, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.655345, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.631835, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.602586, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6390038, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.696759, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.669729, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6771965, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.566988, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6068306, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5868745, dtype=float32)}Training...: 49it [05:13,  2.50s/it][A
Training...: 50it [05:16,  2.47s/it][A
Training...: 51it [05:18,  2.50s/it][A
Training...: 52it [05:21,  2.46s/it][A
Training...: 53it [05:23,  2.44s/it][A
Training...: 54it [05:25,  2.42s/it][A
Training...: 55it [05:28,  2.47s/it][A
Training...: 56it [05:30,  2.44s/it][A
Training...: 57it [05:33,  2.42s/it][A
Training...: 58it [05:35,  2.41s/it][A
Training...: 59it [05:38,  2.46s/it][A
Training...: 60it [05:40,  2.44s/it][A
Training...: 61it [05:42,  2.42s/it][A
Training...: 62it [05:45,  2.41s/it][A
Training...: 63it [05:47,  2.46s/it][A
Training...: 64it [05:50,  2.44s/it][A
Training...: 65it [05:52,  2.42s/it][A
Training...: 66it [05:55,  2.41s/it][A
Training...: 67it [05:57,  2.46s/it][A
Training...: 68it [06:00,  2.43s/it][A
Training...: 69it [06:02,  2.42s/it][A
Training...: 70it [06:04,  2.40s/it][A
Training...: 71it [06:07,  2.46s/it][A
Training...: 72it [06:09,  2.44s/it][A
Training...: 73it [06:12,  2.42s/it][A
Training...: 74it [06:14,  2.41s/it][A
Training...: 75it [06:17,  2.58s/it][A
Training...: 76it [06:19,  2.52s/it][A
Training...: 77it [06:22,  2.48s/it][A
Training...: 78it [06:24,  2.45s/it][A
Training...: 79it [06:27,  2.49s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 80it [06:37,  4.78s/it][A
Training...: 81it [06:39,  4.08s/it][A
Training...: 82it [06:42,  3.57s/it][A
Training...: 83it [06:44,  3.28s/it][A
Training...: 84it [06:47,  3.01s/it][A
Training...: 85it [06:49,  2.82s/it][A
Training...: 86it [06:52,  2.75s/it][A
Training...: 87it [06:54,  2.64s/it][A
Training...: 88it [06:56,  2.56s/it][A
Training...: 89it [06:59,  2.51s/it][A
Training...: 90it [07:01,  2.54s/it][A
Training...: 91it [07:04,  2.49s/it][A
Training...: 92it [07:06,  2.45s/it][A
Training...: 93it [07:08,  2.43s/it][A
Training...: 94it [07:11,  2.48s/it][A
Training...: 95it [07:13,  2.45s/it][A
Training...: 96it [07:16,  2.43s/it][A
Training...: 97it [07:18,  2.42s/it][A
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5760083, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5489616, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.585555, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6063867, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5925875, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.648616, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5060086, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5716076, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.567256, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5943985, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.578433, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.611869, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5839643, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.591673, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5590553, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.620398, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5038433, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5351677, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5067472, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.562499, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5403013, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.591977, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5707603, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5907407, dtype=float32)}
Training...: 98it [07:21,  2.47s/it][A
train_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5719175, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5091176, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4691978, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.532968, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5627437, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5175133, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5415354, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4973288, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.470331, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5198298, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.557249, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5158415, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.532744, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.534171, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.55254, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4684496, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4516563, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.521833, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.523821, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.471273, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4947367, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5557265, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5093327, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5203466, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5402393, dtype=float32)}
Training...: 99it [07:23,  2.44s/it][A
Training...: 100it [07:26,  2.42s/it][A
Training...: 101it [07:28,  2.41s/it][A
Training...: 102it [07:31,  2.46s/it][A
Training...: 103it [07:33,  2.43s/it][A
Training...: 104it [07:35,  2.42s/it][A
Training...: 105it [07:38,  2.41s/it][A
Training...: 106it [07:40,  2.46s/it][A
Training...: 107it [07:43,  2.44s/it][A
Training...: 108it [07:45,  2.42s/it][A
Training...: 109it [07:47,  2.41s/it][A
Training...: 110it [07:50,  2.46s/it][A
Training...: 111it [07:52,  2.43s/it][A
Training...: 112it [07:55,  2.42s/it][A
Training...: 113it [07:57,  2.41s/it][A
Training...: 114it [08:00,  2.46s/it][A
Training...: 115it [08:02,  2.43s/it][A
Training...: 116it [08:04,  2.42s/it][A
Training...: 117it [08:07,  2.41s/it][A
Training...: 118it [08:09,  2.46s/it][A
Training...: 119it [08:12,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 120it [08:22,  4.87s/it][A
Training...: 121it [08:25,  4.13s/it][A
Training...: 122it [08:27,  3.67s/it][A
Training...: 123it [08:30,  3.29s/it][A
Training...: 124it [08:32,  3.02s/it][A
Training...: 125it [08:35,  2.83s/it][A
Training...: 126it [08:37,  2.75s/it][A
Training...: 127it [08:39,  2.64s/it][A
Training...: 128it [08:42,  2.56s/it][A
Training...: 129it [08:44,  2.55s/it][A
Training...: 130it [08:47,  2.50s/it][A
Training...: 131it [08:49,  2.46s/it][A
Training...: 132it [08:51,  2.44s/it][A
Training...: 133it [08:54,  2.46s/it][A
Training...: 134it [08:56,  2.43s/it][A
Training...: 135it [08:59,  2.41s/it][A
Training...: 136it [09:01,  2.40s/it][A
Training...: 137it [09:04,  2.43s/it][A
Training...: 138it [09:06,  2.42s/it][A
Training...: 139it [09:08,  2.40s/it][A
Training...: 140it [09:11,  2.40s/it][A
Training...: 141it [09:13,  2.43s/it][A
Training...: 142it [09:16,  2.41s/it][A
Training...: 143it [09:18,  2.40s/it][A
Training...: 144it [09:20,  2.39s/it][A
Training...: 145it [09:23,  2.43s/it][A
Training...: 146it [09:25,  2.41s/it][A
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.512164, dtype=float32)}
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.558091, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.486674, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.49608, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.461432, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4907165, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4743166, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4722476, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5485234, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5494266, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4635634, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4942417, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5178156, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.467669, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.453555, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.550711, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4142447, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5064836, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5134487, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4943285, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4504642, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5753827, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.500931, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5294867, dtype=float32)}
train_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.49255, dtype=float32)}
train_step here is:   124
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5030007, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4723163, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.43409, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.476641, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.488108, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4731417, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4673643, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.545492, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5026197, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4846206, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5104322, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4970074, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5150785, dtype=float32)}
train_step here is:   137
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.492726, dtype=float32)}
train_step here is:   138
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4168243, dtype=float32)}
train_step here is:   139
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.476692, dtype=float32)}
train_step here is:   140
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.474161, dtype=float32)}
train_step here is:   141
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.492948, dtype=float32)}
train_step here is:   142
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.478399, dtype=float32)}
train_step here is:   143
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4638453, dtype=float32)}
train_step here is:   144
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.540805, dtype=float32)}
train_step here is:   145
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.482358, dtype=float32)}
train_step here is:   146
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4729853, dtype=float32)}
train_step here is:   147
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4565134, dtype=float32)}
Training...: 147it [09:28,  2.40s/it][A
Training...: 148it [09:30,  2.39s/it][A
Training...: 149it [09:32,  2.41s/it][A
                                     [A                                          
train_step here is:   148
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.47874, dtype=float32)}
train_step here is:   149
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.535769, dtype=float32)}
Epoch... (1/3 | Loss: 5.535768985748291, Learning Rate: 4.999999873689376e-05)
Epoch ... (1/3):   0% 0/3 [09:32<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.86s/it][A[A

Evaluating...: 3it [00:12,  3.37s/it][A[A

Evaluating...: 5it [00:13,  1.67s/it][A[A

Evaluating...: 7it [00:13,  1.01it/s][A[A

Evaluating...: 9it [00:13,  1.57it/s][A[A

Evaluating...: 11it [00:13,  2.29it/s][A[A

Evaluating...: 13it [00:13,  3.12it/s][A[A

Evaluating...: 15it [00:13,  3.84it/s][A[A

Evaluating...: 17it [00:14,  4.44it/s][A[A

Evaluating...: 19it [00:14,  5.49it/s][A[A

Evaluating...: 21it [00:14,  6.32it/s][A[A

Evaluating...: 23it [00:14,  7.75it/s][A[A

Evaluating...: 25it [00:14,  9.33it/s][A[A

Evaluating...: 27it [00:14, 10.68it/s][A[A

Evaluating...: 29it [00:15, 12.02it/s][A[A

Evaluating...: 31it [00:15, 13.07it/s][A[A

Evaluating...: 33it [00:15, 10.16it/s][A[A

Evaluating...: 35it [00:15, 11.59it/s][A[A

Evaluating...: 37it [00:15, 11.24it/s][A[A

Evaluating...: 39it [00:16,  9.81it/s][A[A

Evaluating...: 41it [00:16,  9.06it/s][A[A

Evaluating...: 43it [00:16,  8.82it/s][A[A

Evaluating...: 44it [00:16,  8.74it/s][A[A

Evaluating...: 46it [00:16, 10.08it/s][A[A

Evaluating...: 48it [00:17,  8.72it/s][A[A

Evaluating...: 50it [00:17, 10.37it/s][A[A

Evaluating...: 52it [00:17, 11.72it/s][A[A

Evaluating...: 54it [00:17, 12.89it/s][A[A

Evaluating...: 56it [00:17, 13.86it/s][A[A

Evaluating...: 58it [00:17, 14.46it/s][A[A

Evaluating...: 60it [00:17, 15.06it/s][A[A

Evaluating...: 62it [00:18, 11.57it/s][A[A

Evaluating...: 64it [00:18,  9.92it/s][A[A

Evaluating...: 66it [00:18,  9.35it/s][A[A

                                      [A[A                                          Epoch ... (1/3):   0% 0/3 [09:52<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 5.477606773376465):  33% 1/3 [09:59<19:59, 599.72s/it]Epoch... (1/3 | Eval Loss: 5.477606773376465)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.05s/it][A
Training...: 2it [00:05,  2.66s/it][A
Training...: 3it [00:07,  2.55s/it][A
Training...: 4it [00:10,  2.56s/it][A
Training...: 5it [00:12,  2.49s/it][A
Training...: 6it [00:15,  2.46s/it][A
Training...: 7it [00:17,  2.43s/it][A
Training...: 8it [00:20,  2.62s/it][A
Training...: 9it [00:22,  2.54s/it][A
Training...: 10it [00:25,  2.49s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 11it [00:33,  4.32s/it][A
Training...: 12it [00:36,  3.80s/it][A
Training...: 13it [00:38,  3.37s/it][A
Training...: 14it [00:41,  3.10s/it][A
Training...: 15it [00:43,  2.88s/it][A
Training...: 16it [00:46,  2.80s/it][A
Training...: 17it [00:48,  2.67s/it][A
Training...: 18it [00:51,  2.59s/it][A
Training...: 19it [00:53,  2.52s/it][A
Training...: 20it [00:55,  2.53s/it][A
Training...: 21it [00:58,  2.49s/it][A
Training...: 22it [01:00,  2.45s/it][A
Training...: 23it [01:03,  2.43s/it][A
Training...: 24it [01:05,  2.48s/it][A
Training...: 25it [01:08,  2.45s/it][A
Training...: 26it [01:10,  2.43s/it][A
Training...: 27it [01:12,  2.41s/it][A
Training...: 28it [01:15,  2.47s/it][A
Training...: 29it [01:17,  2.44s/it][A
Training...: 30it [01:20,  2.42s/it][A
Training...: 31it [01:22,  2.41s/it][A
Training...: 32it [01:25,  2.47s/it][A
Training...: 33it [01:27,  2.44s/it][A
Training...: 34it [01:29,  2.42s/it][A
Training...: 35it [01:32,  2.41s/it][A
Training...: 36it [01:34,  2.46s/it][A
Training...: 37it [01:37,  2.44s/it][A
Training...: 38it [01:39,  2.42s/it][A
Training...: 39it [01:42,  2.41s/it][A
Training...: 40it [01:44,  2.46s/it][A
Training...: 41it [01:46,  2.43s/it][A
Training...: 42it [01:49,  2.42s/it][A
Training...: 43it [01:51,  2.47s/it][A
Training...: 44it [01:54,  2.44s/it][A
Training...: 45it [01:56,  2.42s/it][A
Training...: 46it [01:59,  2.41s/it][A
Training...: 47it [02:01,  2.46s/it][A
Training...: 48it [02:04,  2.44s/it][A
train_step here is:   150
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4790525, dtype=float32)}
train_step here is:   151
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5197687, dtype=float32)}
train_step here is:   152
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4966187, dtype=float32)}
train_step here is:   153
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.441065, dtype=float32)}
train_step here is:   154
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.414521, dtype=float32)}
train_step here is:   155
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4491186, dtype=float32)}
train_step here is:   156
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.490322, dtype=float32)}
train_step here is:   157
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5203543, dtype=float32)}
train_step here is:   158
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.513794, dtype=float32)}
train_step here is:   159
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4782743, dtype=float32)}
train_step here is:   160
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.551955, dtype=float32)}
train_step here is:   161
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4968996, dtype=float32)}
train_step here is:   162
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.439537, dtype=float32)}
train_step here is:   163
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.450473, dtype=float32)}
train_step here is:   164
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4820848, dtype=float32)}
train_step here is:   165
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4800024, dtype=float32)}
train_step here is:   166
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4472165, dtype=float32)}
train_step here is:   167
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4753647, dtype=float32)}
train_step here is:   168
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5137978, dtype=float32)}
train_step here is:   169
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4331026, dtype=float32)}
train_step here is:   170
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.383832, dtype=float32)}
train_step here is:   171
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.421237, dtype=float32)}
train_step here is:   172
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4344425, dtype=float32)}
train_step here is:   173
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.45Training...: 49it [02:06,  2.42s/it][A84656, dtype=float32)}
train_step here is:   174
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.46754, dtype=float32)}
train_step here is:   175
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5002446, dtype=float32)}
train_step here is:   176
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.463977, dtype=float32)}
train_step here is:   177
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4901857, dtype=float32)}
train_step here is:   178
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.392013, dtype=float32)}
train_step here is:   179
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.451259, dtype=float32)}
train_step here is:   180
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4860373, dtype=float32)}
train_step here is:   181
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4821777, dtype=float32)}
train_step here is:   182
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4126606, dtype=float32)}
train_step here is:   183
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.449999, dtype=float32)}
train_step here is:   184
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.467499, dtype=float32)}
train_step here is:   185
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.443948, dtype=float32)}
train_step here is:   186
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.44349, dtype=float32)}
train_step here is:   187
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4045577, dtype=float32)}
train_step here is:   188
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.435897, dtype=float32)}
train_step here is:   189
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4856915, dtype=float32)}
train_step here is:   190
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4208636, dtype=float32)}
train_step here is:   191
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.449828, dtype=float32)}
train_step here is:   192
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4011087, dtype=float32)}
train_step here is:   193
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.516302, dtype=float32)}
train_step here is:   194
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4697146, dtype=float32)}
train_step here is:   195
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.457899, dtype=float32)}
train_step here is:   196
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.490894, dtype=float32)}
train_step here is:   197
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4469666, dtype=float32)}
train_step here is:   198
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4430833, dtype=float32)}
Training...: 50it [02:08,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 51it [02:17,  4.30s/it][A
Training...: 52it [02:19,  3.74s/it][A
Training...: 53it [02:22,  3.33s/it][A
Training...: 54it [02:24,  3.06s/it][A
Training...: 55it [02:27,  2.92s/it][A
Training...: 56it [02:30,  2.89s/it][A
Training...: 57it [02:32,  2.74s/it][A
Training...: 58it [02:34,  2.63s/it][A
Training...: 59it [02:37,  2.61s/it][A
Training...: 60it [02:39,  2.54s/it][A
Training...: 61it [02:42,  2.49s/it][A
Training...: 62it [02:44,  2.46s/it][A
Training...: 63it [02:47,  2.49s/it][A
Training...: 64it [02:49,  2.45s/it][A
Training...: 65it [02:51,  2.43s/it][A
Training...: 66it [02:54,  2.41s/it][A
Training...: 67it [02:56,  2.46s/it][A
Training...: 68it [02:59,  2.44s/it][A
Training...: 69it [03:01,  2.42s/it][A
Training...: 70it [03:04,  2.41s/it][A
Training...: 71it [03:06,  2.46s/it][A
Training...: 72it [03:09,  2.44s/it][A
Training...: 73it [03:11,  2.42s/it][A
Training...: 74it [03:13,  2.41s/it][A
Training...: 75it [03:16,  2.46s/it][A
Training...: 76it [03:18,  2.43s/it][A
Training...: 77it [03:21,  2.41s/it][A
Training...: 78it [03:23,  2.40s/it][A
Training...: 79it [03:26,  2.45s/it][A
Training...: 80it [03:28,  2.43s/it][A
Training...: 81it [03:30,  2.41s/it][A
Training...: 82it [03:33,  2.40s/it][A
Training...: 83it [03:35,  2.46s/it][A
Training...: 84it [03:38,  2.44s/it][A
Training...: 85it [03:40,  2.42s/it][A
Training...: 86it [03:43,  2.47s/it][A
Training...: 87it [03:45,  2.44s/it][A
Training...: 88it [03:47,  2.42s/it][A
Training...: 89it [03:50,  2.41s/it][A
Training...: 90it [03:52,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 91it [04:02,  4.76s/it][A
Training...: 92it [04:05,  4.06s/it][A
Training...: 93it [04:07,  3.55s/it][A
Training...: 94it [04:10,  3.27s/it][A
Training...: 95it [04:12,  3.00s/it][A
Training...: 96it [04:15,  2.82s/it][A
Training...: 97it [04:17,  2.69s/it][A

train_step here is:   199
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.45959, dtype=float32)}
train_step here is:   200
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5023084, dtype=float32)}
train_step here is:   201
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4492283, dtype=float32)}
train_step here is:   202
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.438078, dtype=float32)}
train_step here is:   203
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4380965, dtype=float32)}
train_step here is:   204
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.495202, dtype=float32)}
train_step here is:   205
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.452507, dtype=float32)}
train_step here is:   206
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5039616, dtype=float32)}
train_step here is:   207
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.420595, dtype=float32)}
train_step here is:   208
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.48389, dtype=float32)}
train_step here is:   209
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.407873, dtype=float32)}
train_step here is:   210
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4532175, dtype=float32)}
train_step here is:   211
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4703493, dtype=float32)}
train_step here is:   212
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.468726, dtype=float32)}
train_step here is:   213
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.457248, dtype=float32)}
train_step here is:   214
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4594736, dtype=float32)}
train_step here is:   215
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.439704, dtype=float32)}
train_step here is:   216
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4299483, dtype=float32)}
train_step here is:   217
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.431447, dtype=float32)}
train_step here is:   218
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.421871, dtype=float32)}
train_step here is:   219
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.423641, dtype=float32)}
train_step here is:   220
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.509117, dtype=float32)}
train_step here is:   221
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4447937, dtype=float32)}
train_step here is:   222
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4325805Training...: 98it [04:20,  2.66s/it][A, dtype=float32)}
train_step here is:   223
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4111986, dtype=float32)}
train_step here is:   224
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4861774, dtype=float32)}
train_step here is:   225
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4482346, dtype=float32)}
train_step here is:   226
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4339542, dtype=float32)}
train_step here is:   227
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.51686, dtype=float32)}
train_step here is:   228
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4541264, dtype=float32)}
train_step here is:   229
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.427046, dtype=float32)}
train_step here is:   230
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.432107, dtype=float32)}
train_step here is:   231
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.435849, dtype=float32)}
train_step here is:   232
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3973193, dtype=float32)}
train_step here is:   233
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.445913, dtype=float32)}
train_step here is:   234
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4257116, dtype=float32)}
train_step here is:   235
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.441914, dtype=float32)}
train_step here is:   236
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.477605, dtype=float32)}
train_step here is:   237
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.434787, dtype=float32)}
train_step here is:   238
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.442941, dtype=float32)}
train_step here is:   239
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4360104, dtype=float32)}
train_step here is:   240
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4593887, dtype=float32)}
train_step here is:   241
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5034237, dtype=float32)}
train_step here is:   242
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4875264, dtype=float32)}
train_step here is:   243
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.480486, dtype=float32)}
train_step here is:   244
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4798303, dtype=float32)}
train_step here is:   245
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4571176, dtype=float32)}
train_step here is:   246
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.525356, dtype=float32)}
train_step here is:   247
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3917084, dtype=float32)}
Training...: 99it [04:22,  2.57s/it][A
Training...: 100it [04:24,  2.51s/it][A
Training...: 101it [04:27,  2.47s/it][A
Training...: 102it [04:29,  2.50s/it][A
Training...: 103it [04:32,  2.46s/it][A
Training...: 104it [04:34,  2.44s/it][A
Training...: 105it [04:37,  2.55s/it][A
Training...: 106it [04:39,  2.56s/it][A
Training...: 107it [04:42,  2.50s/it][A
Training...: 108it [04:44,  2.47s/it][A
Training...: 109it [04:47,  2.44s/it][A
Training...: 110it [04:49,  2.48s/it][A
Training...: 111it [04:52,  2.45s/it][A
Training...: 112it [04:54,  2.43s/it][A
Training...: 113it [04:56,  2.41s/it][A
Training...: 114it [04:59,  2.47s/it][A
Training...: 115it [05:01,  2.44s/it][A
Training...: 116it [05:04,  2.42s/it][A
Training...: 117it [05:06,  2.41s/it][A
Training...: 118it [05:09,  2.46s/it][A
Training...: 119it [05:11,  2.43s/it][A
Training...: 120it [05:13,  2.42s/it][A
Training...: 121it [05:16,  2.40s/it][A
Training...: 122it [05:18,  2.46s/it][A
Training...: 123it [05:21,  2.43s/it][A
Training...: 124it [05:23,  2.42s/it][A
Training...: 125it [05:25,  2.40s/it][A
Training...: 126it [05:28,  2.46s/it][A
Training...: 127it [05:30,  2.44s/it][A
Training...: 128it [05:33,  2.42s/it][A
Training...: 129it [05:35,  2.44s/it][A
Training...: 130it [05:38,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 131it [05:48,  4.73s/it][A
Training...: 132it [05:50,  4.03s/it][A
Training...: 133it [05:53,  3.57s/it][A
Training...: 134it [05:55,  3.23s/it][A
Training...: 135it [05:57,  2.97s/it][A
Training...: 136it [06:00,  2.79s/it][A
Training...: 137it [06:02,  2.70s/it][A
Training...: 138it [06:05,  2.61s/it][A
Training...: 139it [06:07,  2.54s/it][A
Training...: 140it [06:09,  2.49s/it][A
Training...: 141it [06:12,  2.49s/it][A
Training...: 142it [06:14,  2.46s/it][A
Training...: 143it [06:17,  2.43s/it][A
Training...: 144it [06:19,  2.42s/it][A
Training...: 145it [06:22,  2.44s/it][A
Training...: 146it [06:24,  2.42s/it][A
train_step here is:   248
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.483617, dtype=float32)}
train_step here is:   249
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4876657, dtype=float32)}
train_step here is:   250
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.458241, dtype=float32)}
train_step here is:   251
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5045433, dtype=float32)}
train_step here is:   252
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4304113, dtype=float32)}
train_step here is:   253
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.450529, dtype=float32)}
train_step here is:   254
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4795914, dtype=float32)}
train_step here is:   255
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4579897, dtype=float32)}
train_step here is:   256
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.431418, dtype=float32)}
train_step here is:   257
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5103574, dtype=float32)}
train_step here is:   258
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4332275, dtype=float32)}
train_step here is:   259
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4084334, dtype=float32)}
train_step here is:   260
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4512367, dtype=float32)}
train_step here is:   261
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.499228, dtype=float32)}
train_step here is:   262
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.457591, dtype=float32)}
train_step here is:   263
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4393473, dtype=float32)}
train_step here is:   264
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.496484, dtype=float32)}
train_step here is:   265
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4713984, dtype=float32)}
train_step here is:   266
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4774084, dtype=float32)}
train_step here is:   267
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4668717, dtype=float32)}
train_step here is:   268
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.411584, dtype=float32)}
train_step here is:   269
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.462102, dtype=float32)}
train_step here is:   270
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.479174, dtype=float32)}
train_step here is:   271
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.459155, dtype=float32)}
train_step here is:   272
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4591208, dtype=float32)}
train_step here is:   273
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.469179, dtype=float32)}
train_step here is:   274
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.520754, dtype=float32)}
train_step here is:   275
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.433685, dtype=float32)}
train_step here is:   276
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4713798, dtype=float32)}
train_step here is:   277
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.464778, dtype=float32)}
train_step here is:   278
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.45798, dtype=float32)}
train_step here is:   279
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4606385, dtype=float32)}
train_step here is:   280
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4537506, dtype=float32)}
train_step here is:   281
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4778028, dtype=float32)}
train_step here is:   282
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.520578, dtype=float32)}
train_step here is:   283
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4469604, dtype=float32)}
train_step here is:   284
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4501786, dtype=float32)}
train_step here is:   285
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.428586, dtype=float32)}
train_step here is:   286
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4495583, dtype=float32)}
train_step here is:   287
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.439454, dtype=float32)}
train_step here is:   288
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.426594, dtype=float32)}
train_step here is:   289
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3953867, dtype=float32)}
train_step here is:   290
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.438941, dtype=float32)}
train_step here is:   291
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4697456, dtype=float32)}
train_step here is:   292
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4769716, dtype=float32)}
train_step here is:   293
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4552917, dtype=float32)}
train_step here is:   294
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.449184, dtype=float32)}
train_step here is:   295
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4552155, dtype=float32)}
train_step here is:   296
the learning rate here is:   
Training...: 147it [06:26,  2.41s/it][A
Training...: 148it [06:29,  2.40s/it][A
Training...: 149it [06:31,  2.41s/it][A
                                     [A{'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4833, dtype=float32)}
train_step here is:   297
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.462863, dtype=float32)}
train_step here is:   298
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.429764, dtype=float32)}
Epoch... (2/3 | Loss: 5.4297637939453125, Learning Rate: 4.999999873689376e-05)
                                                                                 Epoch... (1/3 | Eval Loss: 5.477606773376465):  33% 1/3 [16:31<19:59, 599.72s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  4.14it/s][A[A

Evaluating...: 3it [00:00,  9.00it/s][A[A

Evaluating...: 5it [00:00, 12.25it/s][A[A

Evaluating...: 7it [00:00, 13.78it/s][A[A

Evaluating...: 9it [00:00, 14.58it/s][A[A

Evaluating...: 11it [00:00, 15.26it/s][A[A

Evaluating...: 13it [00:00, 15.69it/s][A[A

Evaluating...: 15it [00:01, 15.73it/s][A[A

Evaluating...: 17it [00:01,  6.37it/s][A[A

Evaluating...: 19it [00:02,  7.10it/s][A[A

Evaluating...: 21it [00:02,  7.05it/s][A[A

Evaluating...: 22it [00:02,  7.37it/s][A[A

Evaluating...: 24it [00:02,  8.93it/s][A[A

Evaluating...: 26it [00:02, 10.52it/s][A[A

Evaluating...: 28it [00:02, 11.76it/s][A[A

Evaluating...: 30it [00:02, 13.02it/s][A[A

Evaluating...: 32it [00:03,  9.76it/s][A[A

Evaluating...: 34it [00:03, 11.41it/s][A[A

Evaluating...: 36it [00:03, 12.72it/s][A[A

Evaluating...: 38it [00:03, 13.65it/s][A[A

Evaluating...: 40it [00:03, 10.88it/s][A[A

Evaluating...: 42it [00:04, 10.58it/s][A[A

Evaluating...: 44it [00:04,  9.25it/s][A[A

Evaluating...: 46it [00:04,  9.17it/s][A[A

Evaluating...: 48it [00:04,  8.22it/s][A[A

Evaluating...: 50it [00:04,  9.83it/s][A[A

Evaluating...: 52it [00:05, 11.17it/s][A[A

Evaluating...: 54it [00:05, 12.36it/s][A[A

Evaluating...: 56it [00:05, 13.25it/s][A[A

Evaluating...: 58it [00:05, 13.91it/s][A[A

Evaluating...: 60it [00:05, 15.04it/s][A[A

Evaluating...: 62it [00:05, 15.30it/s][A[A

Evaluating...: 64it [00:05, 14.16it/s][A[A

Evaluating...: 66it [00:06, 10.91it/s][A[A

                                      [A[A                                                                                 Epoch... (1/3 | Eval Loss: 5.477606773376465):  33% 1/3 [16:37<19:59, 599.72s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.449147701263428):  67% 2/3 [16:45<08:05, 485.63s/it]Epoch... (2/3 | Eval Loss: 5.449147701263428)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.03s/it][A
Training...: 2it [00:05,  2.65s/it][A
Training...: 3it [00:07,  2.55s/it][A
Training...: 4it [00:10,  2.56s/it][A
Training...: 5it [00:12,  2.50s/it][A
Training...: 6it [00:15,  2.46s/it][A
Training...: 7it [00:17,  2.43s/it][A
Training...: 8it [00:20,  2.48s/it][A
Training...: 9it [00:22,  2.44s/it][A
Training...: 10it [00:24,  2.42s/it][A
Training...: 11it [00:27,  2.41s/it][A
Training...: 12it [00:29,  2.46s/it][A
Training...: 13it [00:32,  2.44s/it][A
Training...: 14it [00:34,  2.42s/it][A
Training...: 15it [00:36,  2.41s/it][A
Training...: 16it [00:39,  2.46s/it][A
Training...: 17it [00:41,  2.43s/it][A
Training...: 18it [00:44,  2.42s/it][A
Training...: 19it [00:46,  2.41s/it][A
Training...: 20it [00:49,  2.46s/it][A
Training...: 21it [00:51,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 22it [01:01,  4.72s/it][A
Training...: 23it [01:04,  4.02s/it][A
Training...: 24it [01:06,  3.59s/it][A
Training...: 25it [01:09,  3.24s/it][A
Training...: 26it [01:11,  2.98s/it][A
Training...: 27it [01:13,  2.80s/it][A
Training...: 28it [01:16,  2.73s/it][A
Training...: 29it [01:19,  2.76s/it][A
Training...: 30it [01:21,  2.65s/it][A
Training...: 31it [01:24,  2.57s/it][A
Training...: 32it [01:26,  2.58s/it][A
Training...: 33it [01:28,  2.52s/it][A
Training...: 34it [01:31,  2.47s/it][A
Training...: 35it [01:33,  2.44s/it][A
Training...: 36it [01:36,  2.49s/it][A
Training...: 37it [01:38,  2.45s/it][A
Training...: 38it [01:41,  2.43s/it][A
Training...: 39it [01:43,  2.41s/it][A
Training...: 40it [01:46,  2.46s/it][A
Training...: 41it [01:48,  2.44s/it][A
Training...: 42it [01:50,  2.42s/it][A
Training...: 43it [01:53,  2.48s/it][A
Training...: 44it [01:55,  2.45s/it][A
Training...: 45it [01:58,  2.42s/it][A
Training...: 46it [02:00,  2.41s/it][A
Training...: 47it [02:03,  2.47s/it][A
Training...: 48it [02:05,  2.44s/it][Atrain_step here is:   299
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.423527, dtype=float32)}
train_step here is:   300
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.422681, dtype=float32)}
train_step here is:   301
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4515905, dtype=float32)}
train_step here is:   302
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.450967, dtype=float32)}
train_step here is:   303
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.450875, dtype=float32)}
train_step here is:   304
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4482627, dtype=float32)}
train_step here is:   305
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.417613, dtype=float32)}
train_step here is:   306
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4175663, dtype=float32)}
train_step here is:   307
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.460847, dtype=float32)}
train_step here is:   308
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.421753, dtype=float32)}
train_step here is:   309
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.460157, dtype=float32)}
train_step here is:   310
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4562244, dtype=float32)}
train_step here is:   311
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3945827, dtype=float32)}
train_step here is:   312
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.470436, dtype=float32)}
train_step here is:   313
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4479275, dtype=float32)}
train_step here is:   314
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3897486, dtype=float32)}
train_step here is:   315
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4675527, dtype=float32)}
train_step here is:   316
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4908, dtype=float32)}
train_step here is:   317
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4667645, dtype=float32)}
train_step here is:   318
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4639034, dtype=float32)}
train_step here is:   319
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4721584, dtype=float32)}
train_step here is:   320
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4173317, dtype=float32)}
train_step here is:   321
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4568834, dtype=float32)}
train_step here is:   322
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.466357
7, dtype=float32)}
train_step here is:   323
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.421729, dtype=float32)}
train_step here is:   324
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4257526, dtype=float32)}
train_step here is:   325
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.409463, dtype=float32)}
train_step here is:   326
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4190016, dtype=float32)}
train_step here is:   327
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.444561, dtype=float32)}
train_step here is:   328
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.402054, dtype=float32)}
train_step here is:   329
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.440377, dtype=float32)}
train_step here is:   330
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4232235, dtype=float32)}
train_step here is:   331
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4613123, dtype=float32)}
train_step here is:   332
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4075723, dtype=float32)}
train_step here is:   333
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3956447, dtype=float32)}
train_step here is:   334
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4444275, dtype=float32)}
train_step here is:   335
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.441879, dtype=float32)}
train_step here is:   336
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.450694, dtype=float32)}
train_step here is:   337
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3915186, dtype=float32)}
train_step here is:   338
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.417706, dtype=float32)}
train_step here is:   339
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4621344, dtype=float32)}
train_step here is:   340
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4140196, dtype=float32)}
train_step here is:   341
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4622145, dtype=float32)}
train_step here is:   342
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4274006, dtype=float32)}
train_step here is:   343
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.41058, dtype=float32)}
train_step here is:   344
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.420927, dtype=float32)}
train_step here is:   345
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4413037, dtype=float32)}
train_step here is:   346
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4443603, dtype=float32)}
train_step here is:   347
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.429144, dtype=float32)}Training...: 49it [02:07,  2.42s/it][A
Training...: 50it [02:10,  2.41s/it][A
Training...: 51it [02:12,  2.46s/it][A
Training...: 52it [02:15,  2.43s/it][A
Training...: 53it [02:17,  2.42s/it][A
Training...: 54it [02:19,  2.41s/it][A
Training...: 55it [02:22,  2.47s/it][A
Training...: 56it [02:24,  2.44s/it][A
Training...: 57it [02:27,  2.42s/it][A
Training...: 58it [02:29,  2.41s/it][A
Training...: 59it [02:32,  2.46s/it][A
Training...: 60it [02:34,  2.44s/it][A
Training...: 61it [02:37,  2.42s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 62it [02:47,  4.70s/it][A
Training...: 63it [02:49,  4.08s/it][A
Training...: 64it [02:52,  3.57s/it][A
Training...: 65it [02:54,  3.22s/it][A
Training...: 66it [02:56,  2.97s/it][A
Training...: 67it [02:59,  2.85s/it][A
Training...: 68it [03:01,  2.71s/it][A
Training...: 69it [03:04,  2.61s/it][A
Training...: 70it [03:06,  2.54s/it][A
Training...: 71it [03:09,  2.55s/it][A
Training...: 72it [03:11,  2.50s/it][A
Training...: 73it [03:13,  2.46s/it][A
Training...: 74it [03:16,  2.44s/it][A
Training...: 75it [03:18,  2.48s/it][A
Training...: 76it [03:21,  2.58s/it][A
Training...: 77it [03:24,  2.52s/it][A
Training...: 78it [03:26,  2.48s/it][A
Training...: 79it [03:29,  2.51s/it][A
Training...: 80it [03:31,  2.47s/it][A
Training...: 81it [03:33,  2.44s/it][A
Training...: 82it [03:36,  2.42s/it][A
Training...: 83it [03:38,  2.47s/it][A
Training...: 84it [03:41,  2.44s/it][A
Training...: 85it [03:43,  2.42s/it][A
Training...: 86it [03:46,  2.47s/it][A
Training...: 87it [03:48,  2.44s/it][A
Training...: 88it [03:50,  2.43s/it][A
Training...: 89it [03:53,  2.41s/it][A
Training...: 90it [03:55,  2.47s/it][A
Training...: 91it [03:58,  2.44s/it][A
Training...: 92it [04:00,  2.42s/it][A
Training...: 93it [04:02,  2.41s/it][A
Training...: 94it [04:05,  2.46s/it][A
Training...: 95it [04:07,  2.43s/it][A
Training...: 96it [04:10,  2.42s/it][A
Training...: 97it [04:12,  2.41s/it][A
train_step here is:   348
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4426856, dtype=float32)}
train_step here is:   349
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4793835, dtype=float32)}
train_step here is:   350
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4858813, dtype=float32)}
train_step here is:   351
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4558983, dtype=float32)}
train_step here is:   352
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.413372, dtype=float32)}
train_step here is:   353
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4103827, dtype=float32)}
train_step here is:   354
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.483652, dtype=float32)}
train_step here is:   355
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4429016, dtype=float32)}
train_step here is:   356
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.378354, dtype=float32)}
train_step here is:   357
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.466173, dtype=float32)}
train_step here is:   358
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.375453, dtype=float32)}
train_step here is:   359
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4471025, dtype=float32)}
train_step here is:   360
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4282956, dtype=float32)}
train_step here is:   361
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.416588, dtype=float32)}
train_step here is:   362
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.42013, dtype=float32)}
train_step here is:   363
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4466186, dtype=float32)}
train_step here is:   364
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4087796, dtype=float32)}
train_step here is:   365
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3776655, dtype=float32)}
train_step here is:   366
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4582267, dtype=float32)}
train_step here is:   367
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4472938, dtype=float32)}
train_step here is:   368
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.461991, dtype=float32)}
train_step here is:   369
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4346237, dtype=float32)}
train_step here is:   370
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4719076, dtype=float32)}
train_step here is:   371
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4662695, dtype=float32)}
train_step here is:   372
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4016385, dtype=float32)}
train_step here is:   373
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4024343, dtype=float32)}
train_step here is:   374
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.497751, dtype=float32)}
train_step here is:   375
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.459425, dtype=float32)}
train_step here is:   376
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.461583, dtype=float32)}
train_step here is:   377
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4653563, dtype=float32)}
train_step here is:   378
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4319296, dtype=float32)}
train_step here is:   379
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4565477, dtype=float32)}
train_step here is:   380
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4458714, dtype=float32)}
train_step here is:   381
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4534507, dtype=float32)}
train_step here is:   382
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4787927, dtype=float32)}
train_step here is:   383
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.414757, dtype=float32)}
train_step here is:   384
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4590774, dtype=float32)}
train_step here is:   385
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.420038, dtype=float32)}
train_step here is:   386
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4947686, dtype=float32)}
train_step here is:   387
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.460274, dtype=float32)}
train_step here is:   388
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.429429, dtype=float32)}
train_step here is:   389
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4636793, dtype=float32)}
train_step here is:   390
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3800077, dtype=float32)}
train_step here is:   391
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5007863, dtype=float32)}
train_step here is:   392
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4541764, dtype=float32)}
train_step here is:   393
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.449439, dtype=float32)}
train_step here is:   394
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.401448, dtype=float32)}
train_step here is:   395
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.43521, dtype=float32)}
train_step here is:   396
the learning rate here is:  
Training...: 98it [04:15,  2.46s/it][A
Training...: 99it [04:17,  2.44s/it][A
Training...: 100it [04:20,  2.43s/it][A
Training...: 101it [04:22,  2.41s/it][A

 {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.440748, dtype=float32)}
train_step here is:   397
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4569654, dtype=float32)}
train_step here is:   398
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4534388, dtype=float32)}
train_step here is:   399
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.485361, dtype=float32)}
train_step here is:   400
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4838295, dtype=float32)}
Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  4.27it/s][A[A

Evaluating...: 3it [00:00,  9.30it/s][A[A

Evaluating...: 5it [00:00,  7.97it/s][A[A

Evaluating...: 6it [00:00,  8.17it/s][A[A

Evaluating...: 7it [00:00,  8.06it/s][A[A

Evaluating...: 8it [00:01,  7.76it/s][A[A

Evaluating...: 9it [00:01,  7.91it/s][A[A

Evaluating...: 11it [00:01,  9.99it/s][A[A

Evaluating...: 13it [00:01, 11.89it/s][A[A

Evaluating...: 15it [00:01, 13.25it/s][A[A

Evaluating...: 17it [00:01,  9.90it/s][A[A

Evaluating...: 19it [00:01, 11.67it/s][A[A

Evaluating...: 21it [00:02, 12.83it/s][A[A

Evaluating...: 23it [00:02, 13.77it/s][A[A

Evaluating...: 25it [00:02, 14.50it/s][A[A

Evaluating...: 27it [00:02, 11.13it/s][A[A

Evaluating...: 29it [00:02,  9.75it/s][A[A

Evaluating...: 31it [00:03,  9.39it/s][A[A

Evaluating...: 33it [00:03,  8.29it/s][A[A

Evaluating...: 35it [00:03,  9.85it/s][A[A

Evaluating...: 37it [00:03, 11.22it/s][A[A

Evaluating...: 39it [00:03, 12.49it/s][A[A

Evaluating...: 41it [00:03, 13.35it/s][A[A

Evaluating...: 43it [00:03, 14.32it/s][A[A

Evaluating...: 45it [00:04, 14.86it/s][A[A

Evaluating...: 47it [00:04, 10.74it/s][A[A

Evaluating...: 49it [00:04, 12.46it/s][A[A

Evaluating...: 51it [00:04, 10.27it/s][A[A

Evaluating...: 53it [00:04, 10.04it/s][A[A

Evaluating...: 55it [00:05,  8.91it/s][A[A

Evaluating...: 57it [00:05,  8.96it/s][A[A

Evaluating...: 59it [00:06,  5.79it/s][A[A

Evaluating...: 61it [00:06,  7.30it/s][A[A

Evaluating...: 63it [00:06,  8.22it/s][A[A

Evaluating...: 65it [00:06,  9.86it/s][A[A

Evaluating...: 67it [00:06, 11.32it/s][A[A

                                      [A[A                                                                                 
                                     [AEpoch... (2/3 | Eval Loss: 5.449147701263428):  67% 2/3 [21:17<08:05, 485.63s/it]
Training...: 101it [04:31,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 102it [04:39,  6.86s/it][A
Training...: 103it [04:42,  5.52s/it][A
Training...: 104it [04:44,  4.58s/it][A
Training...: 105it [04:46,  3.93s/it][A
Training...: 106it [04:49,  3.54s/it][A
Training...: 107it [04:51,  3.19s/it][A
Training...: 108it [04:54,  2.94s/it][A
Training...: 109it [04:56,  2.77s/it][A
Training...: 110it [04:59,  2.72s/it][A
Training...: 111it [05:01,  2.61s/it][A
Training...: 112it [05:03,  2.54s/it][A
Training...: 113it [05:06,  2.49s/it][A
Training...: 114it [05:08,  2.52s/it][A
Training...: 115it [05:11,  2.47s/it][A
Training...: 116it [05:13,  2.45s/it][A
Training...: 117it [05:16,  2.42s/it][A
Training...: 118it [05:18,  2.47s/it][A
Training...: 119it [05:20,  2.44s/it][A
Training...: 120it [05:23,  2.42s/it][A
Training...: 121it [05:25,  2.41s/it][A
Training...: 122it [05:28,  2.46s/it][A
Training...: 123it [05:30,  2.44s/it][A
Training...: 124it [05:33,  2.42s/it][A
Training...: 125it [05:35,  2.41s/it][A
Training...: 126it [05:38,  2.46s/it][A
Training...: 127it [05:40,  2.44s/it][A
Training...: 128it [05:42,  2.42s/it][A
Training...: 129it [05:45,  2.44s/it][A
Training...: 130it [05:47,  2.43s/it][A
Training...: 131it [05:50,  2.41s/it][A
Training...: 132it [05:52,  2.40s/it][A
Training...: 133it [05:54,  2.43s/it][A
Training...: 134it [05:57,  2.41s/it][A
Training...: 135it [05:59,  2.40s/it][A
Training...: 136it [06:02,  2.40s/it][A
Training...: 137it [06:04,  2.44s/it][A
Training...: 138it [06:06,  2.42s/it][A
Training...: 139it [06:09,  2.41s/it][A
Training...: 140it [06:11,  2.40s/it][A
Training...: 141it [06:14,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json

Training...: 142it [06:24,  4.71s/it][A
Training...: 143it [06:26,  4.02s/it][A
Training...: 144it [06:29,  3.53s/it][A
Training...: 145it [06:31,  3.24s/it][A
Training...: 146it [06:34,  2.98s/it][A
Training...: 147it [06:36,  2.80s/it][A
Training...: 148it [06:38,  2.67s/it][A
Training...: 149it [06:41,  2.60s/it][A
                                     [A                                                                                 Epoch... (3/3 | Eval Loss: 5.440903663635254)
train_step here is:   401
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4490438, dtype=float32)}
train_step here is:   402
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.386758, dtype=float32)}
train_step here is:   403
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.424617, dtype=float32)}
train_step here is:   404
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.453849, dtype=float32)}
train_step here is:   405
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4112387, dtype=float32)}
train_step here is:   406
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4124727, dtype=float32)}
train_step here is:   407
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.49956, dtype=float32)}
train_step here is:   408
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.395359, dtype=float32)}
train_step here is:   409
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.397938, dtype=float32)}
train_step here is:   410
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4670086, dtype=float32)}
train_step here is:   411
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4544888, dtype=float32)}
train_step here is:   412
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.426813, dtype=float32)}
train_step here is:   413
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4764385, dtype=float32)}
train_step here is:   414
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.437349, dtype=float32)}
train_step here is:   415
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.426176, dtype=float32)}
train_step here is:   416
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.425656, dtype=float32)}
train_step here is:   417
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.408885, dtype=float32)}
train_step here is:   418
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4564815, dtype=float32)}
train_step here is:   419
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4705315, dtype=float32)}
train_step here is:   420
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4423375, dtype=float32)}
train_step here is:   421
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.427864, dtype=float32)}
train_step here is:   422
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.45921, dtype=float32)}
train_step here is:   423
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.486784, dtype=float32)}
train_step here is:   424
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=fEpoch... (3/3 | Eval Loss: 5.440903663635254):  67% 2/3 [23:26<08:05, 485.63s/it]

Evaluating...: 0it [00:00, ?it/s][A[Aloat32), 'loss': ShardedDeviceArray(5.4887633, dtype=float32)}
train_step here is:   425
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4852667, dtype=float32)}
train_step here is:   426
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.44952, dtype=float32)}
train_step here is:   427
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4383364, dtype=float32)}
train_step here is:   428
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4598684, dtype=float32)}
train_step here is:   429
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.447008, dtype=float32)}
train_step here is:   430
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4604807, dtype=float32)}
train_step here is:   431
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.33545, dtype=float32)}
train_step here is:   432
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.404651, dtype=float32)}
train_step here is:   433
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.346364, dtype=float32)}
train_step here is:   434
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4335537, dtype=float32)}
train_step here is:   435
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3923235, dtype=float32)}
train_step here is:   436
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.445398, dtype=float32)}
train_step here is:   437
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4593115, dtype=float32)}
train_step here is:   438
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4289923, dtype=float32)}
train_step here is:   439
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4273167, dtype=float32)}
train_step here is:   440
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.364706, dtype=float32)}
train_step here is:   441
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4894924, dtype=float32)}
train_step here is:   442
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.425026, dtype=float32)}
train_step here is:   443
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4090147, dtype=float32)}
train_step here is:   444
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.390258, dtype=float32)}
train_step here is:   445
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.439519, dtype=float32)}
train_step here is:   446
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.4273977, dtype=float32)}
train_step here is:   447
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.3685193, dtype=float32)}
Epoch... (3/3 | Loss: 5.368519306182861, Learning Rate: 4.999999873689376e-05)


Evaluating...: 1it [00:00,  4.23it/s][A[A

Evaluating...: 3it [00:00,  7.61it/s][A[A

Evaluating...: 4it [00:00,  3.88it/s][A[A

Evaluating...: 6it [00:01,  5.77it/s][A[A

Evaluating...: 7it [00:01,  6.17it/s][A[A

Evaluating...: 9it [00:01,  8.14it/s][A[A

Evaluating...: 11it [00:01, 10.02it/s][A[A

Evaluating...: 13it [00:01, 11.72it/s][A[A

Evaluating...: 15it [00:01, 12.98it/s][A[A

Evaluating...: 17it [00:02, 10.07it/s][A[A

Evaluating...: 19it [00:02, 11.71it/s][A[A

Evaluating...: 21it [00:02, 12.88it/s][A[A

Evaluating...: 23it [00:02, 13.70it/s][A[A

Evaluating...: 25it [00:02, 13.46it/s][A[A

Evaluating...: 27it [00:02, 10.46it/s][A[A

Evaluating...: 29it [00:03, 10.10it/s][A[A

Evaluating...: 31it [00:03,  9.02it/s][A[A

Evaluating...: 33it [00:03,  8.11it/s][A[A

Evaluating...: 35it [00:03,  9.80it/s][A[A

Evaluating...: 37it [00:03, 11.03it/s][A[A

Evaluating...: 39it [00:03, 12.26it/s][A[A

Evaluating...: 41it [00:04, 13.32it/s][A[A

Evaluating...: 43it [00:04, 14.10it/s][A[A

Evaluating...: 45it [00:04, 14.85it/s][A[A

Evaluating...: 47it [00:04, 10.63it/s][A[A

Evaluating...: 50it [00:04, 11.98it/s][A[A

Evaluating...: 52it [00:05, 10.26it/s][A[A

Evaluating...: 54it [00:05,  9.66it/s][A[A

Evaluating...: 56it [00:05,  9.33it/s][A[A

Evaluating...: 58it [00:05, 10.41it/s][A[A

Evaluating...: 60it [00:05, 11.76it/s][A[A

Evaluating...: 62it [00:05, 12.90it/s][A[A

Evaluating...: 64it [00:06, 12.58it/s][A[A

Evaluating...: 66it [00:06, 13.89it/s][A[A

                                      [A[A                                                                                 Epoch... (3/3 | Eval Loss: 5.440903663635254):  67% 2/3 [23:33<08:05, 485.63s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.438005447387695): 100% 3/3 [23:40<00:00, 453.47s/it]Epoch... (3/3 | Eval Loss: 5.438005447387695): 100% 3/3 [23:40<00:00, 473.56s/it]
Epoch... (3/3 | Eval Loss: 5.438005447387695)
wandb: Waiting for W&B process to finish, PID 1717927... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▃▂▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/loss █▄▄▃▃▃▂▂▃▂▃▂▂▂▃▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▂▂▂▂▁
wandb:         train/samples ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/time ▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/time_per_step ▆▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.43801
wandb:           train/epoch 0
wandb:   train/learning_rate 5e-05
wandb:            train/loss 5.36852
wandb:         train/samples 114432
wandb:            train/step 447
wandb:            train/time 1404.32556
wandb:   train/time_per_step 3.5139
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced valiant-energy-32: https://wandb.ai/gxucla/dalle-mini/runs/1g5swf6w
wandb: Find logs at: ./wandb/run-20220306_190823-1g5swf6w/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr4_5k', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 241705.65it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data-e6ab78ca19a9a68f
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run rose-butterfly-33
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/1m3h2gye
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_193455-1m3h2gye
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpmhv68v_w/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmp_tggk8q8/added_tokens.json. We won't load it.
loading file /tmp/tmp_tggk8q8/vocab.json
loading file /tmp/tmp_tggk8q8/merges.txt
loading file /tmp/tmp_tggk8q8/tokenizer.json
loading file None
loading file /tmp/tmp_tggk8q8/special_tokens_map.json
loading file /tmp/tmp_tggk8q8/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 19:38:29.278489: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 19:38:52.736325: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:16, 196.25s/it][A
Training...: 2it [03:18, 82.21s/it] [A
Training...: 3it [03:21, 45.75s/it][A
Training...: 4it [03:24, 29.12s/it][A
Training...: 5it [03:26, 19.47s/it][A
Training...: 6it [03:29, 13.80s/it][A
Training...: 7it [03:32, 10.07s/it][A
Training...: 8it [03:35,  8.00s/it][A
Training...: 9it [03:38,  6.24s/it][A
Training...: 10it [03:40,  5.05s/it][A
Training...: 11it [03:42,  4.23s/it][A
Training...: 12it [03:46,  4.03s/it][A
Training...: 13it [03:48,  3.53s/it][A
Training...: 14it [03:51,  3.18s/it][A
Training...: 15it [03:53,  2.93s/it][A
Training...: 16it [03:56,  2.90s/it][A
Training...: 17it [03:58,  2.74s/it][A
                                    [A                                          Epoch ... (1/3):   0% 0/3 [03:58<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.682215, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(8.721166, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.4249487, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.0191197, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.122402, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.927242, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8901668, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.80973, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.832259, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6913056, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.798186, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6640673, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6461086, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6507425, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6939087, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6760817, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.759306, dtype=float32)}
Epoch... (1/3 | Loss: 5.759305953979492, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:12, 12.62s/it][A[A

Evaluating...: 3it [00:12,  3.31s/it][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:11<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_5k/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 5.795512676239014):  33% 1/3 [04:16<08:33, 256.60s/it]Epoch... (1/3 | Eval Loss: 5.795512676239014)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:04,  4.33s/it][A
Training...: 2it [00:06,  3.19s/it][A
Training...: 3it [00:09,  2.83s/it][A
Training...: 4it [00:12,  3.10s/it][A
Training...: 5it [00:15,  2.84s/it][A
Training...: 6it [00:17,  2.68s/it][A
Training...: 7it [00:19,  2.58s/it][A
Training...: 8it [00:23,  2.91s/it][A
Training...: 9it [00:25,  2.74s/it][A
Training...: 10it [00:28,  2.63s/it][A
Training...: 11it [00:30,  2.55s/it][A
Training...: 12it [00:34,  2.86s/it][A
Training...: 13it [00:36,  2.72s/it][A
Training...: 14it [00:38,  2.62s/it][A
Training...: 15it [00:41,  2.54s/it][A
Training...: 16it [00:44,  2.62s/it][A
Training...: 17it [00:46,  2.55s/it][A
                                    [A                                                                                 Epoch... (1/3 | Eval Loss: 5.795512676239014):  33% 1/3 [05:03<08:33, 256.60s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7593627, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7083406, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.668893, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.673072, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.621082, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6050043, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5997796, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.652327, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6075096, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.552775, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5482864, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.58098, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.70654, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7789845, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6909523, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6783333, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.733476, dtype=float32)}
Epoch... (2/3 | Loss: 5.733476161956787, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  2.19it/s][A[A

Evaluating...: 3it [00:00,  5.53it/s][A[A

Evaluating...: 4it [00:00,  5.71it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 5.795512676239014):  33% 1/3 [05:03<08:33, 256.60s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_5k/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.686615943908691):  67% 2/3 [05:10<02:17, 137.27s/it]
Epoch... (2/3 | Eval Loss: 5.686615943908691)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.91s/it][A
Training...: 2it [00:06,  3.01s/it][A
Training...: 3it [00:08,  2.75s/it][A
Training...: 4it [00:12,  3.06s/it][A
Training...: 5it [00:14,  2.81s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_5k/special_tokens_map.json

Training...: 6it [00:23,  4.78s/it][A
Training...: 7it [00:25,  4.00s/it][A
Training...: 8it [00:29,  3.87s/it][A
Training...: 9it [00:31,  3.42s/it][A
Training...: 10it [00:34,  3.10s/it][A
Training...: 11it [00:36,  2.88s/it][A
Training...: 12it [00:39,  3.08s/it][A
Training...: 13it [00:42,  2.87s/it][A
Training...: 14it [00:44,  2.72s/it][A
Training...: 15it [00:47,  2.62s/it][A
Training...: 16it [00:49,  2.68s/it][A
Training...: 17it [00:52,  2.59s/it][A
                                    [A                                                                                 Epoch... (2/3 | Eval Loss: 5.686615943908691):  67% 2/3 [06:02<02:17, 137.27s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5875893, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6015005, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.637772, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.58103, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.633183, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.582102, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5982075, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.614206, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.619136, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5915256, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.568668, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.498777, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.576049, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.616085, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6036005, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6149178, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5484505, dtype=float32)}
Epoch... (3/3 | Loss: 5.548450469970703, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:00,  2.10it/s][A[A

Evaluating...: 3it [00:00,  6.09it/s][A[A

                                     [A[A                                                                                 Epoch... (2/3 | Eval Loss: 5.686615943908691):  67% 2/3 [06:03<02:17, 137.27s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr4_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr4_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr4_5k/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.612642765045166): 100% 3/3 [06:08<00:00, 101.07s/it]Epoch... (3/3 | Eval Loss: 5.612642765045166): 100% 3/3 [06:08<00:00, 122.78s/it]Epoch... (3/3 | Eval Loss: 5.612642765045166)

wandb: Waiting for W&B process to finish, PID 1724057... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▄▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss ▆█▅▆▇▂▃▁
wandb:         train/samples ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/step ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/time ▁▅▆▆▆▆▇▇▇▇▇███
wandb:   train/time_per_step █▁▁▃▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.61264
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 5.54845
wandb:         train/samples 13056
wandb:            train/step 51
wandb:            train/time 360.26437
wandb:   train/time_per_step 2.38608
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced rose-butterfly-33: https://wandb.ai/gxucla/dalle-mini/runs/1m3h2gye
wandb: Find logs at: ./wandb/run-20220306_193455-1m3h2gye/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr3_5k', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 254654.17it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data-e6ab78ca19a9a68f
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run winter-shadow-34
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/g00ynr1y
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_195236-g00ynr1y
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpnwmln5cn/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmplab_bkvc/added_tokens.json. We won't load it.
loading file /tmp/tmplab_bkvc/vocab.json
loading file /tmp/tmplab_bkvc/merges.txt
loading file /tmp/tmplab_bkvc/tokenizer.json
loading file None
loading file /tmp/tmplab_bkvc/special_tokens_map.json
loading file /tmp/tmplab_bkvc/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 19:56:05.789234: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 19:56:29.123817: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:14, 194.21s/it][A
Training...: 2it [03:16, 81.36s/it] [A
Training...: 3it [03:18, 45.29s/it][A
Training...: 4it [03:22, 29.00s/it][A
Training...: 5it [03:25, 19.40s/it][A
Training...: 6it [03:27, 13.61s/it][A
Training...: 7it [03:30,  9.93s/it][A
Training...: 8it [03:33,  7.91s/it][A
Training...: 9it [03:36,  6.18s/it][A
Training...: 10it [03:38,  5.00s/it][A
Training...: 11it [03:40,  4.20s/it][A
Training...: 12it [03:44,  4.01s/it][A
Training...: 13it [03:46,  3.51s/it][A
Training...: 14it [03:49,  3.17s/it][A
Training...: 15it [03:51,  2.93s/it][A
Training...: 16it [03:54,  2.90s/it][A
Training...: 17it [03:56,  2.74s/it][A
                                    [A                                          Epoch ... (1/3):   0% 0/3 [03:56<?, ?it/s]train_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.619157, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.613512, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.9230146, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(10.605408, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.703512, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(9.356161, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.116858, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.413396, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2349467, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.203838, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.483254, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.6745048, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.251896, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1722903, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2178597, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2284718, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.986933, dtype=float32)}
Epoch... (1/3 | Loss: 6.98693323135376, Learning Rate: 0.004999999888241291)


Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.74s/it][A[A

Evaluating...: 3it [00:12,  3.34s/it][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:09<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_5k/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 6.962425231933594):  33% 1/3 [04:14<08:29, 254.60s/it]Epoch... (1/3 | Eval Loss: 6.962425231933594)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:04,  4.33s/it][A
Training...: 2it [00:06,  3.18s/it][A
Training...: 3it [00:09,  2.83s/it][A
Training...: 4it [00:12,  3.12s/it][A
Training...: 5it [00:15,  2.85s/it][A
Training...: 6it [00:17,  2.69s/it][A
Training...: 7it [00:19,  2.58s/it][A
Training...: 8it [00:23,  2.90s/it][A
Training...: 9it [00:25,  2.74s/it][A
Training...: 10it [00:28,  2.62s/it][A
Training...: 11it [00:30,  2.55s/it][A
Training...: 12it [00:34,  2.85s/it][A
Training...: 13it [00:36,  2.71s/it][A
Training...: 14it [00:38,  2.61s/it][A
Training...: 15it [00:41,  2.54s/it][A
Training...: 16it [00:43,  2.62s/it][A
Training...: 17it [00:46,  2.55s/it][A
                                    [A                                                                                 Epoch... (1/3 | Eval Loss: 6.962425231933594):  33% 1/3 [05:00<08:29, 254.60s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.976607, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0154, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.109567, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9223447, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9111032, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.38032, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.406096, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.228495, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1495275, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0152283, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.4444838, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.602615, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2146316, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2307963, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.4194636, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9985943, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0671577, dtype=float32)}
Epoch... (2/3 | Loss: 7.067157745361328, Learning Rate: 0.004999999888241291)


Evaluating...: 1it [00:00,  2.09it/s][A[A

Evaluating...: 4it [00:00,  7.57it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 6.962425231933594):  33% 1/3 [05:01<08:29, 254.60s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_5k/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 7.369095325469971):  67% 2/3 [05:06<02:15, 135.40s/it]
Epoch... (2/3 | Eval Loss: 7.369095325469971)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.96s/it][A
Training...: 2it [00:06,  3.05s/it][A
Training...: 3it [00:08,  2.74s/it][A
Training...: 4it [00:12,  3.05s/it][A
Training...: 5it [00:14,  2.81s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_5k/special_tokens_map.json

Training...: 6it [00:23,  4.83s/it][A
Training...: 7it [00:25,  4.04s/it][A
Training...: 8it [00:29,  3.88s/it][A
Training...: 9it [00:31,  3.41s/it][A
Training...: 10it [00:34,  3.09s/it][A
Training...: 11it [00:36,  2.87s/it][A
Training...: 12it [00:40,  3.09s/it][A
Training...: 13it [00:42,  2.87s/it][A
Training...: 14it [00:44,  2.72s/it][A
Training...: 15it [00:47,  2.62s/it][A
Training...: 16it [00:50,  2.69s/it][A
Training...: 17it [00:52,  2.59s/it][A
                                    [A                                                                                 Epoch... (2/3 | Eval Loss: 7.369095325469971):  67% 2/3 [05:59<02:15, 135.40s/it]train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.3560457, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.346776, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.882842, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.5741215, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2225776, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.983024, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.852147, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.110872, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2422023, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0217013, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.88624, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.78381, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.706428, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7164516, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.635112, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9474564, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8418245, dtype=float32)}
Epoch... (3/3 | Loss: 6.841824531555176, Learning Rate: 0.004999999888241291)


Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  2.13it/s][A[A

Evaluating...: 3it [00:00,  5.92it/s][A[A

                                     [A[A                                                                                 Epoch... (2/3 | Eval Loss: 7.369095325469971):  67% 2/3 [05:59<02:15, 135.40s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr3_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr3_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr3_5k/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 6.559536933898926): 100% 3/3 [06:04<00:00, 100.03s/it]Epoch... (3/3 | Eval Loss: 6.559536933898926): 100% 3/3 [06:04<00:00, 121.50s/it]
Epoch... (3/3 | Eval Loss: 6.559536933898926)
wandb: Waiting for W&B process to finish, PID 1728204... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss ▄█▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss █▄▆█▅▄▃▁
wandb:         train/samples ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/step ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/time ▁▅▆▆▆▆▇▇▇▇▇███
wandb:   train/time_per_step █▁▁▃▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 6.55954
wandb:           train/epoch 0
wandb:   train/learning_rate 0.005
wandb:            train/loss 6.84182
wandb:         train/samples 13056
wandb:            train/step 51
wandb:            train/time 356.6384
wandb:   train/time_per_step 2.37974
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced winter-shadow-34: https://wandb.ai/gxucla/dalle-mini/runs/g00ynr1y
wandb: Find logs at: ./wandb/run-20220306_195236-g00ynr1y/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr5_5k', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=5e-05, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 302132.07it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data-e6ab78ca19a9a68f
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run misunderstood-thunder-35
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/2ic9yzmi
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_201333-2ic9yzmi
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmpc69mlmc_/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpcz8y8pho/added_tokens.json. We won't load it.
loading file /tmp/tmpcz8y8pho/vocab.json
loading file /tmp/tmpcz8y8pho/merges.txt
loading file /tmp/tmpcz8y8pho/tokenizer.json
loading file None
loading file /tmp/tmpcz8y8pho/special_tokens_map.json
loading file /tmp/tmpcz8y8pho/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 20:17:02.138949: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 20:17:25.336389: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:13, 193.58s/it][A
Training...: 2it [03:15, 81.10s/it] [A
Training...: 3it [03:18, 45.34s/it][A
Training...: 4it [03:22, 28.87s/it][A
Training...: 5it [03:24, 19.31s/it][A
Training...: 6it [03:27, 13.56s/it][A
Training...: 7it [03:29,  9.91s/it][A
Training...: 8it [03:33,  7.90s/it][A
Training...: 9it [03:35,  6.17s/it][A
Training...: 10it [03:37,  5.00s/it][A
Training...: 11it [03:40,  4.20s/it][A
Training...: 12it [03:43,  4.02s/it][A
Training...: 13it [03:46,  3.52s/it][A
Training...: 14it [03:48,  3.18s/it][A
Training...: 15it [03:50,  2.93s/it][A
Training...: 16it [03:53,  2.91s/it][A
Training...: 17it [03:56,  2.75s/it][A
                                    [A                                          train_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.667762, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.764348, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.304349, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.754104, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.7222743, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.3503895, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.1919622, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.9269924, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.683779, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.5775933, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.426038, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.3058205, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.137497, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.1670876, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0854564, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.1461306, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.1130075, dtype=float32)}
Epoch... (1/3 | Loss: 6.113007545471191, Learning Rate: 4.999999873689376e-05)
Epoch ... (1/3):   0% 0/3 [03:56<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:13, 13.05s/it][A[A

Evaluating...: 3it [00:13,  3.43s/it][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:09<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_5k/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 6.081572532653809):  33% 1/3 [04:16<08:32, 256.26s/it]Epoch... (1/3 | Eval Loss: 6.081572532653809)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.95s/it][A
Training...: 2it [00:06,  3.03s/it][A
Training...: 3it [00:08,  2.75s/it][A
Training...: 4it [00:12,  3.07s/it][A
Training...: 5it [00:14,  2.82s/it][A
Training...: 6it [00:17,  2.66s/it][A
Training...: 7it [00:19,  2.57s/it][A
Training...: 8it [00:22,  2.88s/it][A
Training...: 9it [00:25,  2.72s/it][A
Training...: 10it [00:27,  2.61s/it][A
Training...: 11it [00:30,  2.54s/it][A
Training...: 12it [00:33,  2.87s/it][A
Training...: 13it [00:36,  2.72s/it][A
Training...: 14it [00:38,  2.62s/it][A
Training...: 15it [00:40,  2.54s/it][A
Training...: 16it [00:43,  2.63s/it][A
Training...: 17it [00:46,  2.55s/it][A
                                    [A                                                                                 Epoch... (1/3 | Eval Loss: 6.081572532653809):  33% 1/3 [05:02<08:32, 256.26s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0346584, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0448627, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.993907, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0205555, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0264425, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.886841, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.943337, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.9226656, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.9117537, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.875188, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.88439, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.815361, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.923202, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8149447, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.809417, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.759984, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7592287, dtype=float32)}
Epoch... (2/3 | Loss: 5.759228706359863, Learning Rate: 4.999999873689376e-05)


Evaluating...: 1it [00:00,  2.03it/s][A[A

Evaluating...: 4it [00:00,  7.34it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 6.081572532653809):  33% 1/3 [05:03<08:32, 256.26s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_5k/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.831295013427734):  67% 2/3 [05:10<02:17, 137.38s/it]Epoch... (2/3 | Eval Loss: 5.831295013427734)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:03,  3.95s/it][A
Training...: 2it [00:06,  3.03s/it][A
Training...: 3it [00:08,  2.76s/it][A
Training...: 4it [00:12,  3.07s/it][A
Training...: 5it [00:14,  2.82s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_5k/special_tokens_map.json

Training...: 6it [00:24,  5.22s/it][A
Training...: 7it [00:26,  4.30s/it][A
Training...: 8it [00:30,  4.08s/it][A
Training...: 9it [00:33,  3.56s/it][A
Training...: 10it [00:35,  3.19s/it][A
Training...: 11it [00:37,  2.94s/it][A
Training...: 12it [00:41,  3.14s/it][A
Training...: 13it [00:43,  2.91s/it][A
Training...: 14it [00:46,  2.75s/it][A
Training...: 15it [00:48,  2.63s/it][A
Training...: 16it [00:51,  2.71s/it][A
Training...: 17it [00:53,  2.61s/it][A
                                    [A                                                                                 train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8436093, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.735668, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.813825, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.805439, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.740943, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7838717, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7030787, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.769001, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.728716, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8044653, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6920853, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7139606, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.713535, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.710626, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6560955, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6322894, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.631398, dtype=float32)}
Epoch... (3/3 | Loss: 5.6313982009887695, Learning Rate: 4.999999873689376e-05)
Epoch... (2/3 | Eval Loss: 5.831295013427734):  67% 2/3 [06:04<02:17, 137.38s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  2.16it/s][A[A

Evaluating...: 4it [00:00,  7.63it/s][A[A

                                     [A[A                                                                                 Epoch... (2/3 | Eval Loss: 5.831295013427734):  67% 2/3 [06:04<02:17, 137.38s/it]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr5_5k/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr5_5k/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr5_5k/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.717952251434326): 100% 3/3 [06:12<00:00, 102.81s/it]Epoch... (3/3 | Eval Loss: 5.717952251434326): 100% 3/3 [06:12<00:00, 124.03s/it]
Epoch... (3/3 | Eval Loss: 5.717952251434326)
wandb: Waiting for W&B process to finish, PID 1732829... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▃▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss █▅▄▃▂▂▁▁
wandb:         train/samples ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/step ▁▂▃▃▃▄▅▆▆▆▆███
wandb:            train/time ▁▅▆▆▆▆▇▇▇▇▇███
wandb:   train/time_per_step █▁▁▄▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.71795
wandb:           train/epoch 0
wandb:   train/learning_rate 5e-05
wandb:            train/loss 5.6314
wandb:         train/samples 13056
wandb:            train/step 51
wandb:            train/time 361.78506
wandb:   train/time_per_step 2.38543
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced misunderstood-thunder-35: https://wandb.ai/gxucla/dalle-mini/runs/2ic9yzmi
wandb: Find logs at: ./wandb/run-20220306_201333-2ic9yzmi/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../aug_finetuned_model1_lr2_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.05, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=8)
Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 149/149 [00:00<00:00, 18974.72it/s]
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 134028.51it/s]
WARNING:datasets.builder:Using custom data configuration aug_encoded_data-71d0035c557750de
INFO:__main__:Local TPUs: 8
INFO:__main__:Global TPUs: 8
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run playful-tree-36
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/12x1218r
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220306_210958-12x1218r
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmp_gdyyh74/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpq5kd_1i_/added_tokens.json. We won't load it.
loading file /tmp/tmpq5kd_1i_/vocab.json
loading file /tmp/tmpq5kd_1i_/merges.txt
loading file /tmp/tmpq5kd_1i_/tokenizer.json
loading file None
loading file /tmp/tmpq5kd_1i_/special_tokens_map.json
loading file /tmp/tmpq5kd_1i_/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 8
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 256
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-06 21:13:28.340478: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-06 21:13:53.304786: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:16, 196.45s/it][A
Training...: 2it [03:18, 82.29s/it] [A
Training...: 3it [03:21, 45.80s/it][A
Training...: 4it [03:23, 28.75s/it][A
Training...: 5it [03:26, 19.23s/it][A
Training...: 6it [03:28, 13.50s/it][A
Training...: 7it [03:31, 10.00s/it][A
Training...: 8it [03:33,  7.64s/it][A
Training...: 9it [03:36,  5.99s/it][A
Training...: 10it [03:38,  4.87s/it][A
Training...: 11it [03:41,  4.11s/it][A
Training...: 12it [03:43,  3.64s/it][A
Training...: 13it [03:45,  3.26s/it][A
Training...: 14it [03:48,  2.99s/it][A
Training...: 15it [03:50,  2.80s/it][A
Training...: 16it [03:53,  2.74s/it][A
Training...: 17it [03:55,  2.63s/it][A
Training...: 18it [03:58,  2.55s/it][A
Training...: 19it [04:00,  2.50s/it][A
Training...: 20it [04:02,  2.51s/it][A
Training...: 21it [04:05,  2.47s/it][A
Training...: 22it [04:07,  2.44s/it][A
Training...: 23it [04:10,  2.42s/it][A
Training...: 24it [04:12,  2.46s/it][A
Training...: 25it [04:15,  2.43s/it][A
Training...: 26it [04:17,  2.42s/it][A
Training...: 27it [04:19,  2.40s/it][A
Training...: 28it [04:22,  2.46s/it][A
Training...: 29it [04:24,  2.43s/it][A
Training...: 30it [04:27,  2.41s/it][A
Training...: 31it [04:29,  2.40s/it][A
Training...: 32it [04:32,  2.45s/it][A
Training...: 33it [04:34,  2.43s/it][A
Training...: 34it [04:36,  2.41s/it][A
Training...: 35it [04:39,  2.40s/it][A
Training...: 36it [04:41,  2.45s/it][A
Training...: 37it [04:44,  2.43s/it][A
Training...: 38it [04:46,  2.41s/it][A
Training...: 39it [04:48,  2.40s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 40it [04:56,  3.89s/it][A
Training...: 41it [04:58,  3.45s/it][A
Training...: 42it [05:01,  3.13s/it][A
Training...: 43it [05:03,  2.96s/it][A
Training...: 44it [05:05,  2.79s/it][A
Training...: 45it [05:08,  2.66s/it][A
Training...: 46it [05:10,  2.58s/it][A
Training...: 47it [05:13,  2.58s/it][A
Training...: 48it [05:15,  2.52s/it][A
Training...: 49it [05:18,  2.48s/it][Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(8.126036, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(8.016306, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(25.553919, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(24.745863, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(20.99098, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(50.064354, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(25.05943, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(14.488237, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(16.983103, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(15.07181, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(17.047274, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(18.481262, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(16.831253, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(15.894255, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(14.420603, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(11.933864, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(14.431267, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(13.372512, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(12.172375, dtype=float32)}
train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(11.107154, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(9.985796, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(8.8409195, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(8.019968, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.611723, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.4014525, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.459072, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.157617, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.1414, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.414981, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0050178, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.019193, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.952303, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0558205, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.041851, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0820613, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.957759, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9436536, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9062347, dtype=float32)}
train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0018253, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8646364, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.864038, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8850284, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0209265, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.899834, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9199457, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8882, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9490566, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.933176, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.1763287, dtype=float32)}
train_step here is:   50
the learning rate here is:  
Training...: 50it [05:20,  2.45s/it][A
Training...: 51it [05:23,  2.49s/it][A
Training...: 52it [05:25,  2.58s/it][A
Training...: 53it [05:28,  2.52s/it][A
Training...: 54it [05:30,  2.48s/it][A
Training...: 55it [05:33,  2.51s/it][A
Training...: 56it [05:35,  2.47s/it][A
Training...: 57it [05:37,  2.44s/it][A
Training...: 58it [05:40,  2.42s/it][A
Training...: 59it [05:42,  2.47s/it][A
Training...: 60it [05:45,  2.44s/it][A
Training...: 61it [05:47,  2.42s/it][A
Training...: 62it [05:50,  2.41s/it][A
Training...: 63it [05:52,  2.45s/it][A
Training...: 64it [05:54,  2.43s/it][A
Training...: 65it [05:57,  2.41s/it][A
Training...: 66it [05:59,  2.40s/it][A
Training...: 67it [06:02,  2.45s/it][A
Training...: 68it [06:04,  2.43s/it][A
Training...: 69it [06:07,  2.42s/it][A
Training...: 70it [06:09,  2.41s/it][A
Training...: 71it [06:11,  2.46s/it][A
Training...: 72it [06:14,  2.43s/it][A
Training...: 73it [06:16,  2.42s/it][A
Training...: 74it [06:19,  2.40s/it][A
Training...: 75it [06:21,  2.46s/it][A
Training...: 76it [06:24,  2.43s/it][A
Training...: 77it [06:26,  2.42s/it][A
Training...: 78it [06:28,  2.41s/it][A
Training...: 79it [06:31,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 80it [06:40,  4.35s/it][A
Training...: 81it [06:42,  3.78s/it][A
Training...: 82it [06:45,  3.36s/it][A
Training...: 83it [06:47,  3.12s/it][A
Training...: 84it [06:49,  2.90s/it][A
Training...: 85it [06:52,  2.74s/it][A
Training...: 86it [06:54,  2.70s/it][A
Training...: 87it [06:57,  2.60s/it][A
Training...: 88it [06:59,  2.53s/it][A
Training...: 89it [07:02,  2.49s/it][A
Training...: 90it [07:04,  2.51s/it][A
Training...: 91it [07:07,  2.47s/it][A
Training...: 92it [07:09,  2.44s/it][A
Training...: 93it [07:11,  2.42s/it][A
Training...: 94it [07:14,  2.47s/it][A
Training...: 95it [07:16,  2.44s/it][A
Training...: 96it [07:19,  2.42s/it][A
Training...: 97it [07:21,  2.41s/it][A
Training...: 98it [07:24,  2.46s/it][A
Training...: 99it [07:26,  2.44s/it][A {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8559537, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8367276, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8198915, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.843734, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.886216, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0178905, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.3160067, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0564413, dtype=float32)}
train_step here is:   58
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9146094, dtype=float32)}
train_step here is:   59
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.93538, dtype=float32)}
train_step here is:   60
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.905204, dtype=float32)}
train_step here is:   61
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.026032, dtype=float32)}
train_step here is:   62
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.808869, dtype=float32)}
train_step here is:   63
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7786136, dtype=float32)}
train_step here is:   64
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8196983, dtype=float32)}
train_step here is:   65
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.830684, dtype=float32)}
train_step here is:   66
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8910136, dtype=float32)}
train_step here is:   67
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8721404, dtype=float32)}
train_step here is:   68
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8691335, dtype=float32)}
train_step here is:   69
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.784482, dtype=float32)}
train_step here is:   70
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.798529, dtype=float32)}
train_step here is:   71
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.820728, dtype=float32)}
train_step here is:   72
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(7.0701203, dtype=float32)}
train_step here is:   73
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.872476, dtype=float32)}
train_step here is:   74
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.874503, dtype=float32)}
train_step here is:   75
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9246626, dtype=float32)}
train_step here is:   76
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8728094, dtype=float32)}
train_step here is:   77
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.880806, dtype=float32)}
train_step here is:   78
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8530917, dtype=float32)}
train_step here is:   79
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.9300528, dtype=float32)}
train_step here is:   80
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.770826, dtype=float32)}
train_step here is:   81
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.768942, dtype=float32)}
train_step here is:   82
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7700324, dtype=float32)}
train_step here is:   83
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7974195, dtype=float32)}
train_step here is:   84
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.821513, dtype=float32)}
train_step here is:   85
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.83484, dtype=float32)}
train_step here is:   86
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.961111, dtype=float32)}
train_step here is:   87
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.751622, dtype=float32)}
train_step here is:   88
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.768647, dtype=float32)}
train_step here is:   89
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.778344, dtype=float32)}
train_step here is:   90
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8956175, dtype=float32)}
train_step here is:   91
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7500753, dtype=float32)}
train_step here is:   92
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7452307, dtype=float32)}
train_step here is:   93
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.763109, dtype=float32)}
train_step here is:   94
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7893486, dtype=float32)}
train_step here is:   95
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8214326, dtype=float32)}
train_step here is:   96
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.820675, dtype=float32)}
train_step here is:   97
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7657456, dtype=float32)}
train_step here is:   98
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.80294, dtype=float32)}
train_step here is:   99
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.740177, dtype=float32)}
Training...: 100it [07:29,  2.55s/it][A
Training...: 101it [07:31,  2.50s/it][A
Training...: 102it [07:34,  2.52s/it][A
Training...: 103it [07:36,  2.47s/it][A
Training...: 104it [07:38,  2.45s/it][A
Training...: 105it [07:41,  2.43s/it][A
Training...: 106it [07:43,  2.47s/it][A
Training...: 107it [07:46,  2.45s/it][A
Training...: 108it [07:48,  2.43s/it][A
Training...: 109it [07:51,  2.41s/it][A
Training...: 110it [07:53,  2.47s/it][A
Training...: 111it [07:56,  2.44s/it][A
Training...: 112it [07:58,  2.42s/it][A
Training...: 113it [08:00,  2.41s/it][A
Training...: 114it [08:03,  2.47s/it][A
Training...: 115it [08:05,  2.44s/it][A
Training...: 116it [08:08,  2.42s/it][A
Training...: 117it [08:10,  2.41s/it][A
Training...: 118it [08:13,  2.46s/it][A
Training...: 119it [08:15,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 120it [08:24,  4.33s/it][A
Training...: 121it [08:26,  3.76s/it][A
Training...: 122it [08:29,  3.41s/it][A
Training...: 123it [08:31,  3.10s/it][A
Training...: 124it [08:34,  2.88s/it][A
Training...: 125it [08:36,  2.73s/it][A
Training...: 126it [08:38,  2.68s/it][A
Training...: 127it [08:41,  2.59s/it][A
Training...: 128it [08:43,  2.53s/it][A
Training...: 129it [08:46,  2.52s/it][A
Training...: 130it [08:48,  2.48s/it][A
Training...: 131it [08:50,  2.45s/it][A
Training...: 132it [08:53,  2.43s/it][A
Training...: 133it [08:55,  2.45s/it][A
Training...: 134it [08:58,  2.43s/it][A
Training...: 135it [09:00,  2.42s/it][A
Training...: 136it [09:03,  2.40s/it][A
Training...: 137it [09:05,  2.43s/it][A
Training...: 138it [09:07,  2.41s/it][A
Training...: 139it [09:10,  2.40s/it][A
Training...: 140it [09:12,  2.40s/it][A
Training...: 141it [09:15,  2.43s/it][A
Training...: 142it [09:17,  2.41s/it][A
Training...: 143it [09:19,  2.40s/it][A
Training...: 144it [09:22,  2.40s/it][A
Training...: 145it [09:24,  2.43s/it][A
Training...: 146it [09:27,  2.41s/it][A
Training...: 147it [09:29,  2.53s/it][A
train_step here is:   100
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.74617, dtype=float32)}
train_step here is:   101
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.748702, dtype=float32)}
train_step here is:   102
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8853683, dtype=float32)}
train_step here is:   103
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7358785, dtype=float32)}
train_step here is:   104
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.725007, dtype=float32)}
train_step here is:   105
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.727272, dtype=float32)}
train_step here is:   106
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7417755, dtype=float32)}
train_step here is:   107
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7473683, dtype=float32)}
train_step here is:   108
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8353415, dtype=float32)}
train_step here is:   109
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.778803, dtype=float32)}
train_step here is:   110
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8233414, dtype=float32)}
train_step here is:   111
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.772818, dtype=float32)}
train_step here is:   112
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8624506, dtype=float32)}
train_step here is:   113
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7679324, dtype=float32)}
train_step here is:   114
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7848682, dtype=float32)}
train_step here is:   115
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.753785, dtype=float32)}
train_step here is:   116
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.803599, dtype=float32)}
train_step here is:   117
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7376404, dtype=float32)}
train_step here is:   118
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7649274, dtype=float32)}
train_step here is:   119
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7565084, dtype=float32)}
train_step here is:   120
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8069324, dtype=float32)}
train_step here is:   121
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7390513, dtype=float32)}
train_step here is:   122
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7432485, dtype=float32)}
train_step here is:   123
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7506037, dtype=float32)}
train_step here is:   1
24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.773274, dtype=float32)}
train_step here is:   125
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.785456, dtype=float32)}
train_step here is:   126
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7707176, dtype=float32)}
train_step here is:   127
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8733034, dtype=float32)}
train_step here is:   128
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.766328, dtype=float32)}
train_step here is:   129
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.750189, dtype=float32)}
train_step here is:   130
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7498765, dtype=float32)}
train_step here is:   131
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7561107, dtype=float32)}
train_step here is:   132
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.778727, dtype=float32)}
train_step here is:   133
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.804602, dtype=float32)}
train_step here is:   134
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.849792, dtype=float32)}
train_step here is:   135
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8096724, dtype=float32)}
train_step here is:   136
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7509685, dtype=float32)}
train_step here is:   137
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.755932, dtype=float32)}
train_step here is:   138
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7432733, dtype=float32)}
train_step here is:   139
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.748672, dtype=float32)}
train_step here is:   140
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7238793, dtype=float32)}
train_step here is:   141
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7350106, dtype=float32)}
train_step here is:   142
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.735528, dtype=float32)}
train_step here is:   143
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.8058367, dtype=float32)}
train_step here is:   144
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7470217, dtype=float32)}
train_step here is:   145
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7512665, dtype=float32)}
train_step here is:   146
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.74788, dtype=float32)}
train_step here is:   147
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.832311, dtype=float32)}
train_step here is:   148
the learning rate here iTraining...: 148it [09:32,  2.49s/it][As:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7399397, dtype=float32)}
Training...: 149it [09:34,  2.47s/it][A
                                     [A                                          
train_step here is:   149
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.720829, dtype=float32)}
Epoch... (1/3 | Loss: 6.720829010009766, Learning Rate: 0.05000000074505806)
Epoch ... (1/3):   0% 0/3 [09:34<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:13, 13.45s/it][A[A

Evaluating...: 3it [00:13,  3.53s/it][A[A

Evaluating...: 5it [00:13,  1.74s/it][A[A

Evaluating...: 7it [00:13,  1.03s/it][A[A

Evaluating...: 9it [00:13,  1.50it/s][A[A

Evaluating...: 11it [00:14,  2.21it/s][A[A

Evaluating...: 13it [00:14,  3.09it/s][A[A

Evaluating...: 15it [00:14,  4.21it/s][A[A

Evaluating...: 17it [00:14,  4.81it/s][A[A

Evaluating...: 19it [00:14,  5.71it/s][A[A

Evaluating...: 21it [00:15,  6.33it/s][A[A

Evaluating...: 23it [00:15,  6.70it/s][A[A

Evaluating...: 25it [00:15,  7.26it/s][A[A

Evaluating...: 26it [00:15,  7.42it/s][A[A

Evaluating...: 28it [00:15,  8.91it/s][A[A

Evaluating...: 30it [00:15, 10.54it/s][A[A

Evaluating...: 32it [00:16,  5.40it/s][A[A

Evaluating...: 34it [00:16,  6.92it/s][A[A

Evaluating...: 36it [00:16,  7.78it/s][A[A

Evaluating...: 38it [00:17,  7.67it/s][A[A

Evaluating...: 40it [00:17,  7.94it/s][A[A

Evaluating...: 41it [00:17,  7.65it/s][A[A

Evaluating...: 42it [00:17,  7.87it/s][A[A

Evaluating...: 43it [00:17,  8.24it/s][A[A

Evaluating...: 45it [00:17,  9.82it/s][A[A

Evaluating...: 47it [00:18,  8.25it/s][A[A

Evaluating...: 50it [00:18, 10.77it/s][A[A

Evaluating...: 52it [00:18, 12.09it/s][A[A

Evaluating...: 54it [00:18, 13.23it/s][A[A

Evaluating...: 56it [00:18, 13.92it/s][A[A

Evaluating...: 58it [00:18, 14.52it/s][A[A

Evaluating...: 60it [00:19, 12.68it/s][A[A

Evaluating...: 62it [00:19, 10.67it/s][A[A

Evaluating...: 64it [00:19,  9.78it/s][A[A

Evaluating...: 66it [00:19,  9.52it/s][A[A

                                      [A[A                                          Epoch ... (1/3):   0% 0/3 [09:55<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 6.72186279296875):  33% 1/3 [10:01<20:03, 601.62s/it]
Epoch... (1/3 | Eval Loss: 6.72186279296875)
Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:02,  2.98s/it][A
Training...: 2it [00:05,  2.63s/it][A
Training...: 3it [00:07,  2.52s/it][A
Training...: 4it [00:10,  2.54s/it][A
Training...: 5it [00:12,  2.48s/it][A
Training...: 6it [00:15,  2.44s/it][A
Training...: 7it [00:17,  2.42s/it][A
Training...: 8it [00:20,  2.48s/it][A
Training...: 9it [00:22,  2.45s/it][A
Training...: 10it [00:24,  2.43s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 11it [00:33,  4.37s/it][A
Training...: 12it [00:36,  3.83s/it][A
Training...: 13it [00:38,  3.39s/it][A
Training...: 14it [00:40,  3.10s/it][A
Training...: 15it [00:43,  2.88s/it][A
Training...: 16it [00:45,  2.79s/it][A
Training...: 17it [00:48,  2.66s/it][A
Training...: 18it [00:50,  2.58s/it][A
Training...: 19it [00:53,  2.52s/it][A
Training...: 20it [00:55,  2.53s/it][A
Training...: 21it [00:57,  2.48s/it][A
Training...: 22it [01:00,  2.45s/it][A
Training...: 23it [01:02,  2.43s/it][A
Training...: 24it [01:05,  2.47s/it][A
Training...: 25it [01:07,  2.44s/it][A
Training...: 26it [01:10,  2.42s/it][A
Training...: 27it [01:12,  2.41s/it][A
Training...: 28it [01:15,  2.46s/it][A
Training...: 29it [01:17,  2.44s/it][A
Training...: 30it [01:19,  2.42s/it][A
Training...: 31it [01:22,  2.41s/it][A
Training...: 32it [01:24,  2.46s/it][A
Training...: 33it [01:27,  2.56s/it][A
Training...: 34it [01:29,  2.51s/it][A
Training...: 35it [01:32,  2.47s/it][A
Training...: 36it [01:34,  2.50s/it][A
Training...: 37it [01:37,  2.47s/it][A
Training...: 38it [01:39,  2.44s/it][A
Training...: 39it [01:42,  2.42s/it][A
Training...: 40it [01:44,  2.47s/it][A
Training...: 41it [01:46,  2.44s/it][A
Training...: 42it [01:49,  2.42s/it][A
Training...: 43it [01:51,  2.47s/it][A
Training...: 44it [01:54,  2.44s/it][A
Training...: 45it [01:56,  2.42s/it][A
Training...: 46it [01:59,  2.41s/it][A
Training...: 47it [02:01,  2.46s/it][A
Training...: 48it [02:03,  2.43s/it][Atrain_step here is:   150
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7171607, dtype=float32)}
train_step here is:   151
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7146077, dtype=float32)}
train_step here is:   152
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.710011, dtype=float32)}
train_step here is:   153
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7224064, dtype=float32)}
train_step here is:   154
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.725932, dtype=float32)}
train_step here is:   155
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7575283, dtype=float32)}
train_step here is:   156
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.736775, dtype=float32)}
train_step here is:   157
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7529106, dtype=float32)}
train_step here is:   158
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7391896, dtype=float32)}
train_step here is:   159
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7274194, dtype=float32)}
train_step here is:   160
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.71542, dtype=float32)}
train_step here is:   161
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.713069, dtype=float32)}
train_step here is:   162
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.738146, dtype=float32)}
train_step here is:   163
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.748813, dtype=float32)}
train_step here is:   164
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7281895, dtype=float32)}
train_step here is:   165
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.778118, dtype=float32)}
train_step here is:   166
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7196064, dtype=float32)}
train_step here is:   167
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7145715, dtype=float32)}
train_step here is:   168
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.720441, dtype=float32)}
train_step here is:   169
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7239904, dtype=float32)}
train_step here is:   170
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7254114, dtype=float32)}
train_step here is:   171
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7107725, dtype=float32)}
train_step here is:   172
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7466326, dtype=float32)}
train_step here is:   173
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7448483, dtype=float32)}
train_step here is:   174
Training...: 49it [02:06,  2.42s/it][A
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.798659, dtype=float32)}
train_step here is:   175
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.724223, dtype=float32)}
train_step here is:   176
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7248163, dtype=float32)}
train_step here is:   177
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7122316, dtype=float32)}
train_step here is:   178
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7146225, dtype=float32)}
train_step here is:   179
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7243404, dtype=float32)}
train_step here is:   180
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7237163, dtype=float32)}
train_step here is:   181
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7213593, dtype=float32)}
train_step here is:   182
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.739995, dtype=float32)}
train_step here is:   183
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.733718, dtype=float32)}
train_step here is:   184
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.788155, dtype=float32)}
train_step here is:   185
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.739617, dtype=float32)}
train_step here is:   186
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.76037, dtype=float32)}
train_step here is:   187
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7305274, dtype=float32)}
train_step here is:   188
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.702126, dtype=float32)}
train_step here is:   189
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.72463, dtype=float32)}
train_step here is:   190
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7357855, dtype=float32)}
train_step here is:   191
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.768012, dtype=float32)}
train_step here is:   192
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.72379, dtype=float32)}
train_step here is:   193
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7464952, dtype=float32)}
train_step here is:   194
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.745833, dtype=float32)}
train_step here is:   195
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7565126, dtype=float32)}
train_step here is:   196
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.714251, dtype=float32)}
train_step here is:   197
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7189865, dtype=float32)}
train_step here is:   198
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.733656, dtype=float32)}
Training...: 50it [02:08,  2.41s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 51it [02:17,  4.42s/it][A
Training...: 52it [02:20,  3.83s/it][A
Training...: 53it [02:22,  3.39s/it][A
Training...: 54it [02:25,  3.10s/it][A
Training...: 55it [02:27,  2.94s/it][A
Training...: 56it [02:30,  2.77s/it][A
Training...: 57it [02:32,  2.65s/it][A
Training...: 58it [02:34,  2.57s/it][A
Training...: 59it [02:37,  2.57s/it][A
Training...: 60it [02:39,  2.52s/it][A
Training...: 61it [02:42,  2.48s/it][A
Training...: 62it [02:44,  2.45s/it][A
Training...: 63it [02:47,  2.48s/it][A
Training...: 64it [02:49,  2.45s/it][A
Training...: 65it [02:51,  2.43s/it][A
Training...: 66it [02:54,  2.41s/it][A
Training...: 67it [02:56,  2.46s/it][A
Training...: 68it [02:59,  2.44s/it][A
Training...: 69it [03:01,  2.42s/it][A
Training...: 70it [03:03,  2.41s/it][A
Training...: 71it [03:06,  2.46s/it][A
Training...: 72it [03:08,  2.43s/it][A
Training...: 73it [03:11,  2.42s/it][A
Training...: 74it [03:13,  2.41s/it][A
Training...: 75it [03:16,  2.46s/it][A
Training...: 76it [03:18,  2.43s/it][A
Training...: 77it [03:20,  2.41s/it][A
Training...: 78it [03:23,  2.40s/it][A
Training...: 79it [03:25,  2.46s/it][A
Training...: 80it [03:28,  2.43s/it][A
Training...: 81it [03:31,  2.55s/it][A
Training...: 82it [03:33,  2.50s/it][A
Training...: 83it [03:36,  2.52s/it][A
Training...: 84it [03:38,  2.48s/it][A
Training...: 85it [03:40,  2.45s/it][A
Training...: 86it [03:43,  2.49s/it][A
Training...: 87it [03:45,  2.45s/it][A
Training...: 88it [03:48,  2.43s/it][A
Training...: 89it [03:50,  2.41s/it][A
Training...: 90it [03:53,  2.46s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 91it [04:02,  4.46s/it][A
Training...: 92it [04:04,  3.84s/it][A
Training...: 93it [04:07,  3.40s/it][A
Training...: 94it [04:09,  3.17s/it][A
Training...: 95it [04:12,  2.93s/it][A
Training...: 96it [04:14,  2.77s/it][A
Training...: 97it [04:16,  2.65s/it][A
Training...: 98it [04:19,  2.63s/it]
train_step here is:   199
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.749448, dtype=float32)}
train_step here is:   200
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7428184, dtype=float32)}
train_step here is:   201
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.821864, dtype=float32)}
train_step here is:   202
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.748702, dtype=float32)}
train_step here is:   203
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.732265, dtype=float32)}
train_step here is:   204
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7575717, dtype=float32)}
train_step here is:   205
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.739192, dtype=float32)}
train_step here is:   206
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7212706, dtype=float32)}
train_step here is:   207
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.725193, dtype=float32)}
train_step here is:   208
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.741308, dtype=float32)}
train_step here is:   209
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.736398, dtype=float32)}
train_step here is:   210
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7644258, dtype=float32)}
train_step here is:   211
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7352858, dtype=float32)}
train_step here is:   212
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7298756, dtype=float32)}
train_step here is:   213
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7206526, dtype=float32)}
train_step here is:   214
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.730977, dtype=float32)}
train_step here is:   215
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7260356, dtype=float32)}
train_step here is:   216
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7426953, dtype=float32)}
train_step here is:   217
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7563334, dtype=float32)}
train_step here is:   218
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7582994, dtype=float32)}
train_step here is:   219
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7355475, dtype=float32)}
train_step here is:   220
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7240534, dtype=float32)}
train_step here is:   221
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.726786, dtype=float32)}
train_step here is:   222
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7275715, dtype=float32)}
train_step here is:   2[A23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7303534, dtype=float32)}
train_step here is:   224
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7234364, dtype=float32)}
train_step here is:   225
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7297506, dtype=float32)}
train_step here is:   226
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.714763, dtype=float32)}
train_step here is:   227
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.736447, dtype=float32)}
train_step here is:   228
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7052794, dtype=float32)}
train_step here is:   229
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7642245, dtype=float32)}
train_step here is:   230
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.725548, dtype=float32)}
train_step here is:   231
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.750002, dtype=float32)}
train_step here is:   232
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7432137, dtype=float32)}
train_step here is:   233
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.762525, dtype=float32)}
train_step here is:   234
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.710249, dtype=float32)}
train_step here is:   235
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7257953, dtype=float32)}
train_step here is:   236
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.709428, dtype=float32)}
train_step here is:   237
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.724718, dtype=float32)}
train_step here is:   238
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7441893, dtype=float32)}
train_step here is:   239
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7283196, dtype=float32)}
train_step here is:   240
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7267056, dtype=float32)}
train_step here is:   241
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7369366, dtype=float32)}
train_step here is:   242
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7182245, dtype=float32)}
train_step here is:   243
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.736079, dtype=float32)}
train_step here is:   244
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.725974, dtype=float32)}
train_step here is:   245
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7337003, dtype=float32)}
train_step here is:   246
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7334642, dtype=float32)}
train_step here is:   247
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.05, dtype=float32), 'loss': ShardedDeviceArray(6.7714453, dtype=float32)}
Training...: 99it [04:21,  2.56s/it][A
Training...: 100it [04:24,  2.50s/it][A
Training...: 101it [04:26,  2.47s/it][A
Training...: 102it [04:29,  2.50s/it][A
Training...: 103it [04:31,  2.46s/it][A
Training...: 104it [04:33,  2.43s/it][A
Training...: 105it [04:36,  2.42s/it][A
Training...: 106it [04:38,  2.47s/it][A
Training...: 107it [04:41,  2.45s/it][A
Training...: 108it [04:43,  2.43s/it][A
Training...: 109it [04:45,  2.41s/it][A
Training...: 110it [04:48,  2.46s/it][A
Training...: 111it [04:50,  2.44s/it][A
Training...: 112it [04:53,  2.42s/it][A
Training...: 113it [04:55,  2.41s/it][A
Training...: 114it [04:58,  2.46s/it][A
Training...: 115it [05:00,  2.44s/it][A
Training...: 116it [05:03,  2.42s/it][A
Training...: 117it [05:05,  2.41s/it][A
Training...: 118it [05:08,  2.46s/it][A
Training...: 119it [05:10,  2.44s/it][A
Training...: 120it [05:12,  2.42s/it][A
Training...: 121it [05:15,  2.40s/it][A
Training...: 122it [05:17,  2.46s/it][A
Training...: 123it [05:20,  2.43s/it][A
Training...: 124it [05:22,  2.41s/it][A
Training...: 125it [05:24,  2.40s/it][A
Training...: 126it [05:27,  2.45s/it][A
Training...: 127it [05:29,  2.43s/it][A
Training...: 128it [05:32,  2.41s/it][A
Training...: 129it [05:35,  2.57s/it][A
Training...: 130it [05:37,  2.51s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/aug_finetuned_model1_lr2_adafactor/flax_model.msgpack
tokenizer config file saved in ../aug_finetuned_model1_lr2_adafactor/tokenizer_config.json
Special tokens file saved in ../aug_finetuned_model1_lr2_adafactor/special_tokens_map.json

Training...: 131it [05:46,  4.49s/it][A
Training...: 132it [05:48,  3.86s/it][A
Training...: 133it [05:51,  3.45s/it][A
Training...: 134it [05:53,  3.15s/it][A
Training...: 135it [05:56,  2.91s/it][A
Training...: 136it [05:58,  2.75s/it][A
Training...: 137it [06:01,  2.67s/it][A
Training...: 138it [06:03,  2.58s/it][A
Training...: 139it [06:05,  2.52s/it][A
Training...: 140it [06:08,  2.48s/it][A
Training...: 141it [06:10,  2.48s/it][A/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
python: can't open file 'inference.py': [Errno 2] No such file or directory
WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../subset_finetuned_model_lr5_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=5e-05, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=7)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 275301.81it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data_5k-6141c8f43c53f37f
INFO:__main__:Local TPUs: 7
INFO:__main__:Global TPUs: 7
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run avid-puddle-37
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/zee3ny6l
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220307_220626-zee3ny6l
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmptiv9lekq/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpvtzzrlwx/added_tokens.json. We won't load it.
loading file /tmp/tmpvtzzrlwx/vocab.json
loading file /tmp/tmpvtzzrlwx/merges.txt
loading file /tmp/tmpvtzzrlwx/tokenizer.json
loading file None
loading file /tmp/tmpvtzzrlwx/special_tokens_map.json
loading file /tmp/tmpvtzzrlwx/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 7
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 224
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-07 22:09:53.862625: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-07 22:10:12.482728: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:06, 186.57s/it][A
Training...: 2it [03:08, 78.22s/it] [A
Training...: 3it [03:11, 43.58s/it][A
Training...: 4it [03:14, 27.47s/it][A
Training...: 5it [03:16, 18.47s/it][A
Training...: 6it [03:18, 13.00s/it][A
Training...: 7it [03:21,  9.52s/it][A
Training...: 8it [03:23,  7.25s/it][A
Training...: 9it [03:26,  5.76s/it][A
Training...: 10it [03:28,  4.71s/it][A
Training...: 11it [03:30,  4.00s/it][A
Training...: 12it [03:33,  3.50s/it][A
Training...: 13it [03:35,  3.16s/it][A
Training...: 14it [03:38,  2.96s/it][A
Training...: 15it [03:40,  2.78s/it][A
Training...: 16it [03:42,  2.66s/it][A
Training...: 17it [03:45,  2.57s/it][A
Training...: 18it [03:47,  2.54s/it][A
Training...: 19it [03:50,  2.48s/it][A
                                    [A                                          Epoch ... (1/3):   0% 0/3 [03:50<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.052633, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(8.031892, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.755599, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.355649, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(7.102938, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.8895173, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.6855536, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.437358, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.4402323, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.172451, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.136791, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0982337, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0272207, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(6.0224066, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.975512, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.838698, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.890909, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.826024, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.893208, dtype=float32)}
Epoch... (1/3 | Loss: 5.893208026885986, Learning Rate: 4.999999873689376e-05)


Evaluating...: 1it [00:12, 12.29s/it][A[A

Evaluating...: 4it [00:12,  2.37s/it][A[A

Evaluating...: 6it [00:12,  1.45s/it][A[A

Evaluating...: 7it [00:13,  1.14s/it][A[A

Evaluating...: 8it [00:13,  1.13it/s][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:03<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr5_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 5.799401760101318):  33% 1/3 [04:08<08:16, 248.44s/it]Epoch... (1/3 | Eval Loss: 5.799401760101318)

Training...: 0it [00:00, ?it/s][A
Training...: 1it [00:02,  2.82s/it][A
Training...: 2it [00:05,  2.56s/it][A
Training...: 3it [00:07,  2.49s/it][A
Training...: 4it [00:09,  2.45s/it][A
Training...: 5it [00:12,  2.46s/it][A
Training...: 6it [00:14,  2.43s/it][A
Training...: 7it [00:17,  2.41s/it][A
Training...: 8it [00:19,  2.40s/it][A
Training...: 9it [00:22,  2.43s/it][A
Training...: 10it [00:24,  2.41s/it][A
Training...: 11it [00:26,  2.40s/it][A
Training...: 12it [00:29,  2.39s/it][A
Training...: 13it [00:31,  2.39s/it][A
Training...: 14it [00:34,  2.42s/it][A
Training...: 15it [00:36,  2.40s/it][A
Training...: 16it [00:38,  2.40s/it][A
Training...: 17it [00:41,  2.39s/it][A
Training...: 18it [00:43,  2.40s/it][A
Training...: 19it [00:45,  2.39s/it][A
                                    [A                                                                                 Epoch... (1/3 | Eval Loss: 5.799401760101318):  33% 1/3 [04:54<08:16, 248.44s/it]

train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8744307, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.843293, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7710285, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.8025885, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.797399, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7794647, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7643437, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.765869, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7263494, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6935062, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6608467, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.771608, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7483587, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.713789, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6854687, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.658493, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6381245, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.692046, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6434975, dtype=float32)}
Epoch... (2/3 | Loss: 5.643497467041016, Learning Rate: 4.999999873689376e-05)
Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  6.99it/s][A[A

Evaluating...: 3it [00:00, 12.94it/s][A[A

Evaluating...: 5it [00:00, 11.11it/s][A[A

Evaluating...: 7it [00:00,  9.13it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 5.799401760101318):  33% 1/3 [04:55<08:16, 248.44s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr5_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.625343322753906):  67% 2/3 [05:01<02:13, 133.72s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (2/3 | Eval Loss: 5.625343322753906)

Training...: 1it [00:02,  2.90s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr5_adafactor/special_tokens_map.json

Training...: 2it [00:11,  6.23s/it][A
Training...: 3it [00:13,  4.48s/it][A
Training...: 4it [00:16,  3.65s/it][A
Training...: 5it [00:18,  3.24s/it][A
Training...: 6it [00:21,  2.95s/it][A
Training...: 7it [00:23,  2.76s/it][A
Training...: 8it [00:25,  2.64s/it][A
Training...: 9it [00:28,  2.59s/it][A
Training...: 10it [00:30,  2.52s/it][A
Training...: 11it [00:33,  2.48s/it][A
Training...: 12it [00:35,  2.45s/it][A
Training...: 13it [00:37,  2.42s/it][A
Training...: 14it [00:40,  2.44s/it][A
Training...: 15it [00:42,  2.42s/it][A
Training...: 16it [00:45,  2.41s/it][A
Training...: 17it [00:47,  2.40s/it][A
Training...: 18it [00:49,  2.41s/it][A
Training...: 19it [00:52,  2.40s/it][A
                                    [A                                                                                 train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5873156, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6320066, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.7018547, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6565967, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6143093, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6296206, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5823936, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.64601, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6142387, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6225176, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6451244, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.579621, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.672762, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.58168, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.6382704, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.594742, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5825205, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.5560493, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(5.e-05, dtype=float32), 'loss': ShardedDeviceArray(5.544503, dtype=float32)}
Epoch... (3/3 | Loss: 5.544503211975098, Learning Rate: 4.999999873689376e-05)
Epoch... (2/3 | Eval Loss: 5.625343322753906):  67% 2/3 [05:54<02:13, 133.72s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  7.18it/s][A[A

Evaluating...: 4it [00:00, 14.36it/s][A[A

Evaluating...: 6it [00:00, 15.37it/s][A[A

Evaluating...: 8it [00:00, 13.07it/s][A[A

                                     [A[A                                                                                 Epoch... (2/3 | Eval Loss: 5.625343322753906):  67% 2/3 [05:54<02:13, 133.72s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr5_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr5_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr5_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.547205924987793): 100% 3/3 [06:01<00:00, 99.83s/it] Epoch... (3/3 | Eval Loss: 5.547205924987793): 100% 3/3 [06:01<00:00, 120.45s/it]
Epoch... (3/3 | Eval Loss: 5.547205924987793)
wandb: Waiting for W&B process to finish, PID 2060124... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▃▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss █▅▅▂▂▂▁▁
wandb:         train/samples ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/step ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/time ▁▅▆▆▆▆▆▇▇▇▇███
wandb:   train/time_per_step █▁▁█▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.54721
wandb:           train/epoch 0
wandb:   train/learning_rate 5e-05
wandb:            train/loss 5.5445
wandb:         train/samples 12768
wandb:            train/step 57
wandb:            train/time 351.77496
wandb:   train/time_per_step 2.40144
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced avid-puddle-37: https://wandb.ai/gxucla/dalle-mini/runs/zee3ny6l
wandb: Find logs at: ./wandb/run-20220307_220626-zee3ny6l/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../subset_finetuned_model_lr4_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.0005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=7)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 133028.30it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data_5k-6141c8f43c53f37f
INFO:__main__:Local TPUs: 7
INFO:__main__:Global TPUs: 7
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run crisp-spaceship-38
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/hopdqfws
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220307_222702-hopdqfws
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmp9ljgjgez/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmpyrxi86ss/added_tokens.json. We won't load it.
loading file /tmp/tmpyrxi86ss/vocab.json
loading file /tmp/tmpyrxi86ss/merges.txt
loading file /tmp/tmpyrxi86ss/tokenizer.json
loading file None
loading file /tmp/tmpyrxi86ss/special_tokens_map.json
loading file /tmp/tmpyrxi86ss/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 7
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 224
INFO:__main__:  Model parameters = 773,580,544

/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")
Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-07 22:30:28.954727: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-07 22:30:47.063303: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:07, 187.75s/it][A
Training...: 2it [03:10, 78.72s/it] [A
Training...: 3it [03:12, 43.86s/it][A
Training...: 4it [03:15, 27.65s/it][A
Training...: 5it [03:17, 18.57s/it][A
Training...: 6it [03:20, 13.06s/it][A
Training...: 7it [03:22,  9.57s/it][A
Training...: 8it [03:24,  7.28s/it][A
Training...: 9it [03:27,  5.78s/it][A
Training...: 10it [03:29,  4.73s/it][A
Training...: 11it [03:32,  4.01s/it][A
Training...: 12it [03:34,  3.51s/it][A
Training...: 13it [03:36,  3.16s/it][A
Training...: 14it [03:39,  2.96s/it][A
Training...: 15it [03:41,  2.78s/it][A
Training...: 16it [03:44,  2.66s/it][A
Training...: 17it [03:46,  2.57s/it][A
Training...: 18it [03:48,  2.54s/it][A
Training...: 19it [03:51,  2.49s/it][A
                                    [A                                          Epoch ... (1/3):   0% 0/3 [03:51<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.8643174, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(7.983007, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(6.1859465, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.9003735, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.80405, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.8282056, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.9022274, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7063203, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7334924, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7568774, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7548265, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.757412, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.734037, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.673983, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6721134, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.701326, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6296506, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6902547, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7075987, dtype=float32)}
Epoch... (1/3 | Loss: 5.707598686218262, Learning Rate: 0.0005000000237487257)


Evaluating...: 1it [00:12, 12.47s/it][A[A

Evaluating...: 3it [00:12,  3.27s/it][A[A

Evaluating...: 5it [00:12,  1.62s/it][A[A

Evaluating...: 7it [00:13,  1.04s/it][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:04<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr4_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 5.64149284362793):  33% 1/3 [04:09<08:19, 249.56s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (1/3 | Eval Loss: 5.64149284362793)

Training...: 1it [00:02,  2.83s/it][A
Training...: 2it [00:05,  2.57s/it][A
Training...: 3it [00:07,  2.50s/it][A
Training...: 4it [00:10,  2.46s/it][A
Training...: 5it [00:12,  2.47s/it][A
Training...: 6it [00:14,  2.43s/it][A
Training...: 7it [00:17,  2.42s/it][A
Training...: 8it [00:19,  2.40s/it][A
Training...: 9it [00:22,  2.43s/it][A
Training...: 10it [00:24,  2.41s/it][A
Training...: 11it [00:26,  2.40s/it][A
Training...: 12it [00:29,  2.39s/it][A
Training...: 13it [00:31,  2.39s/it][A
Training...: 14it [00:34,  2.42s/it][A
Training...: 15it [00:36,  2.41s/it][A
Training...: 16it [00:38,  2.40s/it][A
Training...: 17it [00:41,  2.39s/it][A
Training...: 18it [00:43,  2.40s/it][A
Training...: 19it [00:46,  2.39s/it][A
                                    [A                                                                                Epoch... (1/3 | Eval Loss: 5.64149284362793):  33% 1/3 [04:55<08:19, 249.56s/it]train_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6439996, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7248287, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7390685, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6681976, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5406194, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.609521, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.610503, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7808104, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.7135043, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.695478, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.608247, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.560647, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5445757, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.582227, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.633096, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6184464, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.652696, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.581643, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.61408, dtype=float32)}
Epoch... (2/3 | Loss: 5.61407995223999, Learning Rate: 0.0005000000237487257)


Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  7.06it/s][A[A

Evaluating...: 3it [00:00, 13.24it/s][A[A

Evaluating...: 5it [00:00, 15.19it/s][A[A

Evaluating...: 7it [00:00, 15.55it/s][A[A

                                     [A[A                                                                                Epoch... (1/3 | Eval Loss: 5.64149284362793):  33% 1/3 [04:56<08:19, 249.56s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr4_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 5.5891432762146):  67% 2/3 [05:01<02:13, 133.08s/it] 
Training...: 0it [00:00, ?it/s]Epoch... (2/3 | Eval Loss: 5.5891432762146)
[A
Training...: 1it [00:02,  2.84s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr4_adafactor/special_tokens_map.json

Training...: 2it [00:11,  6.36s/it][A
Training...: 3it [00:14,  4.54s/it][A
Training...: 4it [00:16,  3.68s/it][A
Training...: 5it [00:18,  3.25s/it][A
Training...: 6it [00:21,  2.95s/it][A
Training...: 7it [00:23,  2.76s/it][A
Training...: 8it [00:26,  2.64s/it][A
Training...: 9it [00:28,  2.59s/it][A
Training...: 10it [00:30,  2.53s/it][A
Training...: 11it [00:33,  2.48s/it][A
Training...: 12it [00:35,  2.45s/it][A
Training...: 13it [00:38,  2.43s/it][A
Training...: 14it [00:40,  2.45s/it][A
Training...: 15it [00:42,  2.42s/it][A
Training...: 16it [00:45,  2.41s/it][A
Training...: 17it [00:47,  2.40s/it][A
Training...: 18it [00:50,  2.42s/it][A
Training...: 19it [00:52,  2.41s/it][A
                                    [A                                                                               train_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.568132, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.586751, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5420055, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.567885, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.570252, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.528577, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.548156, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.6997666, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.654252, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.59215, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.562869, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5148263, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.582602, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.559647, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.615736, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5432587, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5262995, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.637026, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.0005, dtype=float32), 'loss': ShardedDeviceArray(5.5676374, dtype=float32)}
Epoch... (3/3 | Loss: 5.5676374435424805, Learning Rate: 0.0005000000237487257)
Epoch... (2/3 | Eval Loss: 5.5891432762146):  67% 2/3 [05:53<02:13, 133.08s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  6.96it/s][A[A

Evaluating...: 3it [00:00, 12.85it/s][A[A

Evaluating...: 5it [00:00, 14.55it/s][A[A

Evaluating...: 7it [00:00, 15.47it/s][A[A

                                     [A[A                                                                               Epoch... (2/3 | Eval Loss: 5.5891432762146):  67% 2/3 [05:54<02:13, 133.08s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr4_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr4_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr4_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 5.543471336364746): 100% 3/3 [06:00<00:00, 99.50s/it]Epoch... (3/3 | Eval Loss: 5.543471336364746): 100% 3/3 [06:00<00:00, 120.21s/it]
Epoch... (3/3 | Eval Loss: 5.543471336364746)
wandb: Waiting for W&B process to finish, PID 2065463... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▄▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss █▇▅▄▄▃▁▃
wandb:         train/samples ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/step ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/time ▁▅▆▆▆▆▆▇▇▇▇███
wandb:   train/time_per_step █▁▁█▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 5.54347
wandb:           train/epoch 0
wandb:   train/learning_rate 0.0005
wandb:            train/loss 5.56764
wandb:         train/samples 12768
wandb:            train/step 57
wandb:            train/time 351.20645
wandb:   train/time_per_step 2.40505
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced crisp-spaceship-38: https://wandb.ai/gxucla/dalle-mini/runs/hopdqfws
wandb: Find logs at: ./wandb/run-20220307_222702-hopdqfws/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='../subset_finetuned_model_lr3_adafactor', overwrite_output_dir=False, do_train=True, do_eval=True, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, gradient_checkpointing=False, learning_rate=0.005, optim='adafactor', beta1=0.95, beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, block_size=1024, start_preconditioning_step=10, preconditioning_compute_steps=10, skip_preconditioning_dim_size_gt=4096, optim_quantized=False, num_train_epochs=3, warmup_steps=1, lr_decay=None, lr_transition_steps=None, lr_decay_rate=None, lr_staircase=False, logging_steps=10, eval_steps=400, save_steps=40, log_model=False, seed_model=42, wandb_entity=None, wandb_project='dalle-mini', wandb_job_type='Seq2Seq', assert_TPU_available=False, mp_devices=1, dp_devices=7)
Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 233016.89it/s]
WARNING:datasets.builder:Using custom data configuration encoded_data_5k-6141c8f43c53f37f
INFO:__main__:Local TPUs: 7
INFO:__main__:Global TPUs: 7
wandb: Currently logged in as: gxucla (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run icy-cosmos-39
wandb: ⭐️ View project at https://wandb.ai/gxucla/dalle-mini
wandb: 🚀 View run at https://wandb.ai/gxucla/dalle-mini/runs/34ojjr3u
wandb: Run data is saved locally in /home/gxu21/dalle-mini/tools/train/wandb/run-20220307_225406-34ojjr3u
wandb: Run `wandb offline` to turn off syncing.
[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
loading configuration file /tmp/tmphd9zr1ri/config.json
Model config DalleBartConfig {
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "eBart"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 16385,
  "classifier_dropout": 0.0,
  "d_model": 1408,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 14,
  "decoder_start_token_id": 16384,
  "do_sample": true,
  "dropout": 0.0,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 14,
  "encoder_vocab_size": 50264,
  "eos_token_id": 16385,
  "gradient_checkpointing": false,
  "image_length": 256,
  "image_vocab_size": 16384,
  "init_std": 0.01,
  "is_encoder_decoder": true,
  "max_length": 257,
  "max_text_length": 64,
  "min_length": 257,
  "model_type": "dallebart",
  "normalize_text": true,
  "pad_token_id": 16385,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "transformers_version": "4.16.2",
  "use_cache": true
}

[34m[1mwandb[0m: Downloading large artifact model-1reghx5l:latest, 2954.22MB. 7 files... Done. 0:0:0
Didn't find file /tmp/tmphxzh0gvd/added_tokens.json. We won't load it.
loading file /tmp/tmphxzh0gvd/vocab.json
loading file /tmp/tmphxzh0gvd/merges.txt
loading file /tmp/tmphxzh0gvd/tokenizer.json
loading file None
loading file /tmp/tmphxzh0gvd/special_tokens_map.json
loading file /tmp/tmphxzh0gvd/tokenizer_config.json
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = None
INFO:__main__:  Num Epochs = 3
INFO:__main__:  Batch size per device = 8
INFO:__main__:  Number of devices = 7
INFO:__main__:  Gradient accumulation steps = 4
INFO:__main__:  Batch size per update = 224
INFO:__main__:  Model parameters = 773,580,544
/home/gxu21/anaconda3/envs/minidalle/lib/python3.7/site-packages/jax/experimental/pjit.py:183: UserWarning: pjit is an experimental feature and probably has bugs!
  warn("pjit is an experimental feature and probably has bugs!")

Epoch ... (1/3):   0% 0/3 [00:00<?, ?it/s]
Training...: 0it [00:00, ?it/s][A2022-03-07 22:57:34.516824: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck:
2022-03-07 22:57:52.407376: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:147] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.

Training...: 1it [03:06, 186.45s/it][A
Training...: 2it [03:08, 78.17s/it] [A
Training...: 3it [03:11, 43.56s/it][A
Training...: 4it [03:13, 27.46s/it][A
Training...: 5it [03:16, 18.45s/it][A
Training...: 6it [03:18, 12.99s/it][A
Training...: 7it [03:21,  9.51s/it][A
Training...: 8it [03:23,  7.24s/it][A
Training...: 9it [03:26,  5.75s/it][A
Training...: 10it [03:28,  4.71s/it][A
Training...: 11it [03:30,  3.99s/it][A
Training...: 12it [03:33,  3.50s/it][A
Training...: 13it [03:35,  3.16s/it][A
Training...: 14it [03:38,  2.96s/it][A
Training...: 15it [03:40,  2.78s/it][A
Training...: 16it [03:42,  2.66s/it][A
Training...: 17it [03:45,  2.57s/it][A
Training...: 18it [03:47,  2.54s/it][A
Training...: 19it [03:49,  2.49s/it][A
                                    [A                                          train_step here is:   1
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.176915, dtype=float32)}
train_step here is:   2
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.077932, dtype=float32)}
train_step here is:   3
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.069047, dtype=float32)}
train_step here is:   4
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(11.449896, dtype=float32)}
train_step here is:   5
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.43811, dtype=float32)}
train_step here is:   6
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.376467, dtype=float32)}
train_step here is:   7
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.682924, dtype=float32)}
train_step here is:   8
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.7248464, dtype=float32)}
train_step here is:   9
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2976027, dtype=float32)}
train_step here is:   10
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.484949, dtype=float32)}
train_step here is:   11
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.457878, dtype=float32)}
train_step here is:   12
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9504538, dtype=float32)}
train_step here is:   13
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7474766, dtype=float32)}
train_step here is:   14
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7283754, dtype=float32)}
train_step here is:   15
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8834763, dtype=float32)}
train_step here is:   16
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.862356, dtype=float32)}
train_step here is:   17
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.690444, dtype=float32)}
train_step here is:   18
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1495156, dtype=float32)}
train_step here is:   19
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.5428524, dtype=float32)}
Epoch... (1/3 | Loss: 7.542852401733398, Learning Rate: 0.004999999888241291)
Epoch ... (1/3):   0% 0/3 [03:50<?, ?it/s]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:12, 12.42s/it][A[A

Evaluating...: 4it [00:12,  2.39s/it][A[A

Evaluating...: 6it [00:13,  1.47s/it][A[A

Evaluating...: 8it [00:13,  1.07it/s][A[A

                                     [A[A                                          Epoch ... (1/3):   0% 0/3 [04:03<?, ?it/s]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr3_adafactor/special_tokens_map.json
Epoch... (1/3 | Eval Loss: 7.416356563568115):  33% 1/3 [04:08<08:16, 248.25s/it]
Training...: 0it [00:00, ?it/s][AEpoch... (1/3 | Eval Loss: 7.416356563568115)

Training...: 1it [00:02,  2.83s/it][A
Training...: 2it [00:05,  2.57s/it][A
Training...: 3it [00:07,  2.50s/it][A
Training...: 4it [00:10,  2.46s/it][A
Training...: 5it [00:12,  2.48s/it][A
Training...: 6it [00:14,  2.44s/it][A
Training...: 7it [00:17,  2.42s/it][A
Training...: 8it [00:19,  2.40s/it][A
Training...: 9it [00:22,  2.43s/it][A
Training...: 10it [00:24,  2.41s/it][A
Training...: 11it [00:26,  2.40s/it][A
Training...: 12it [00:29,  2.39s/it][A
Training...: 13it [00:31,  2.39s/it][A
Training...: 14it [00:34,  2.41s/it][A
Training...: 15it [00:36,  2.40s/it][A
Training...: 16it [00:38,  2.39s/it][A
Training...: 17it [00:41,  2.38s/it][A
Training...: 18it [00:43,  2.40s/it][A
Training...: 19it [00:46,  2.39s/it][A
                                    [A                                                                                 Epoch... (1/3 | Eval Loss: 7.416356563568115):  33% 1/3 [04:54<08:16, 248.25s/it]

Evaluating...: 0it [00:00, ?it/s][A[Atrain_step here is:   20
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.4231663, dtype=float32)}
train_step here is:   21
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.566035, dtype=float32)}
train_step here is:   22
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.920227, dtype=float32)}
train_step here is:   23
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.540495, dtype=float32)}
train_step here is:   24
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.9453235, dtype=float32)}
train_step here is:   25
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.673535, dtype=float32)}
train_step here is:   26
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7431874, dtype=float32)}
train_step here is:   27
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.2512665, dtype=float32)}
train_step here is:   28
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0385923, dtype=float32)}
train_step here is:   29
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.913953, dtype=float32)}
train_step here is:   30
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0283685, dtype=float32)}
train_step here is:   31
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8189116, dtype=float32)}
train_step here is:   32
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.56213, dtype=float32)}
train_step here is:   33
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(8.664288, dtype=float32)}
train_step here is:   34
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.235082, dtype=float32)}
train_step here is:   35
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.89435, dtype=float32)}
train_step here is:   36
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.610946, dtype=float32)}
train_step here is:   37
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.585363, dtype=float32)}
train_step here is:   38
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1846046, dtype=float32)}
Epoch... (2/3 | Loss: 7.184604644775391, Learning Rate: 0.004999999888241291)


Evaluating...: 1it [00:00,  6.96it/s][A[A

Evaluating...: 3it [00:00, 13.00it/s][A[A

Evaluating...: 5it [00:00, 10.41it/s][A[A

Evaluating...: 7it [00:00,  9.75it/s][A[A

                                     [A[A                                                                                 Epoch... (1/3 | Eval Loss: 7.416356563568115):  33% 1/3 [04:55<08:16, 248.25s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr3_adafactor/special_tokens_map.json
Epoch... (2/3 | Eval Loss: 7.0547285079956055):  67% 2/3 [05:01<02:13, 133.63s/it]
Training...: 0it [00:00, ?it/s]Epoch... (2/3 | Eval Loss: 7.0547285079956055)
[A
Training...: 1it [00:02,  2.86s/it][AConfiguration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr3_adafactor/special_tokens_map.json

Training...: 2it [00:11,  6.18s/it][A
Training...: 3it [00:13,  4.45s/it][A
Training...: 4it [00:16,  3.63s/it][A
Training...: 5it [00:18,  3.23s/it][A
Training...: 6it [00:21,  2.94s/it][A
Training...: 7it [00:23,  2.75s/it][A
Training...: 8it [00:25,  2.63s/it][A
Training...: 9it [00:28,  2.59s/it][A
Training...: 10it [00:30,  2.52s/it][A
Training...: 11it [00:33,  2.48s/it][A
Training...: 12it [00:35,  2.45s/it][A
Training...: 13it [00:37,  2.42s/it][A
Training...: 14it [00:40,  2.44s/it][A
Training...: 15it [00:42,  2.42s/it][A
Training...: 16it [00:44,  2.41s/it][A
Training...: 17it [00:47,  2.40s/it][A
Training...: 18it [00:49,  2.42s/it][A
Training...: 19it [00:52,  2.40s/it][A
                                    [Atrain_step here is:   39
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0353613, dtype=float32)}
train_step here is:   40
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5703163, dtype=float32)}
train_step here is:   41
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3730807, dtype=float32)}
train_step here is:   42
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.748635, dtype=float32)}
train_step here is:   43
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.97246, dtype=float32)}
train_step here is:   44
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.66823, dtype=float32)}
train_step here is:   45
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3428507, dtype=float32)}
train_step here is:   46
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.0024223, dtype=float32)}
train_step here is:   47
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.4286222, dtype=float32)}
train_step here is:   48
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(7.1429, dtype=float32)}
train_step here is:   49
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7727623, dtype=float32)}
train_step here is:   50
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.5137677, dtype=float32)}
train_step here is:   51
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.4578857, dtype=float32)}
train_step here is:   52
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7308435, dtype=float32)}
train_step here is:   53
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.6368475, dtype=float32)}
train_step here is:   54
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3706436, dtype=float32)}
train_step here is:   55
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.3954196, dtype=float32)}
train_step here is:   56
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.8083467, dtype=float32)}
train_step here is:   57
the learning rate here is:   {'learning_rate': ShardedDeviceArray(0.005, dtype=float32), 'loss': ShardedDeviceArray(6.7230806, dtype=float32)}
Epoch... (3/3 | Loss: 6.723080635070801, Learning Rate: 0.004999999888241291)
                                                                                  Epoch... (2/3 | Eval Loss: 7.0547285079956055):  67% 2/3 [05:53<02:13, 133.63s/it]

Evaluating...: 0it [00:00, ?it/s][A[A

Evaluating...: 1it [00:00,  5.64it/s][A[A

Evaluating...: 4it [00:00, 13.07it/s][A[A

Evaluating...: 6it [00:00, 14.42it/s][A[A

Evaluating...: 8it [00:00, 15.25it/s][A[A

                                     [A[A                                                                                  Epoch... (2/3 | Eval Loss: 7.0547285079956055):  67% 2/3 [05:54<02:13, 133.63s/it]Configuration saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/config.json
Model weights saved in /home/gxu21/dalle-mini/tools/subset_finetuned_model_lr3_adafactor/flax_model.msgpack
tokenizer config file saved in ../subset_finetuned_model_lr3_adafactor/tokenizer_config.json
Special tokens file saved in ../subset_finetuned_model_lr3_adafactor/special_tokens_map.json
Epoch... (3/3 | Eval Loss: 6.60442590713501): 100% 3/3 [05:59<00:00, 99.00s/it]   Epoch... (3/3 | Eval Loss: 6.60442590713501): 100% 3/3 [05:59<00:00, 119.81s/it]Epoch... (3/3 | Eval Loss: 6.60442590713501)

wandb: Waiting for W&B process to finish, PID 2072047... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:             eval/loss █▅▁
wandb:           train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   train/learning_rate ▁▁▁▁▁▁▁▁
wandb:            train/loss ██▇▅▆▁▁▂
wandb:         train/samples ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/step ▁▂▃▃▃▃▅▆▆▆▆▇██
wandb:            train/time ▁▅▆▆▆▆▆▇▇▇▇███
wandb:   train/time_per_step █▁▁█▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:             eval/loss 6.60443
wandb:           train/epoch 0
wandb:   train/learning_rate 0.005
wandb:            train/loss 6.72308
wandb:         train/samples 12768
wandb:            train/step 57
wandb:            train/time 351.47824
wandb:   train/time_per_step 2.40301
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced icy-cosmos-39: https://wandb.ai/gxucla/dalle-mini/runs/34ojjr3u
wandb: Find logs at: ./wandb/run-20220307_225406-34ojjr3u/logs/debug.log
wandb: 

WARNING:absl:Initialized persistent compilation cache at ./jax_cache
2022-03-15 13:32:45.562663: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create StreamExecutor for CUDA:1: failed initializing StreamExecutor for CUDA device ordinal 1: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 51050315776
2022-03-15 13:32:45.757079: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_transfer_manager.cc:68] GpuTransferManager couldn't get StreamExecutor for device 1.  As a result, all dynamic shape copies from that device will be unpinned (and therefore potentially slow).
2022-03-15 13:32:45.766577: F external/org_tensorflow/tensorflow/core/platform/statusor.cc:33] Attempting to fetch value instead of handling error INVALID_ARGUMENT: device CUDA:1 not supported by XLA service

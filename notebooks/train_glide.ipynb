{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd655203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189c57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/data/andrewbai/Natural-Disaster-Image-Generation-to-raise-Environmental-Awareness/src\n",
      "/nfs/data/andrewbai/Natural-Disaster-Image-Generation-to-raise-Environmental-Awareness/guided-diffusion\n"
     ]
    }
   ],
   "source": [
    "paths = ['../src/', '../guided-diffusion/']\n",
    "for path in paths:\n",
    "    if os.path.abspath(path) not in sys.path:\n",
    "        sys.path.insert(0, os.path.abspath(path))\n",
    "        print(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b205689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7,8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d63f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f714a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glide_text2im.gaussian_diffusion import GaussianDiffusion\n",
    "from glide_text2im.model_creation import model_and_diffusion_defaults, create_model\n",
    "from glide_text2im.gaussian_diffusion import get_named_beta_schedule\n",
    "from glide_text2im.respace import space_timesteps\n",
    "\n",
    "from guided_diffusion.train_util import TrainLoop\n",
    "from guided_diffusion.gaussian_diffusion import ModelMeanType, ModelVarType, LossType\n",
    "from guided_diffusion.nn import mean_flat\n",
    "from guided_diffusion.losses import normal_kl, discretized_gaussian_log_likelihood\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from guided_diffusion import dist_util, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0da56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ee5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = model_and_diffusion_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a765ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableGaussianDiffusion(GaussianDiffusion):\n",
    "    '''\n",
    "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/gaussian_diffusion.py#L709\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        *,\n",
    "        betas,\n",
    "        model_mean_type,\n",
    "        model_var_type,\n",
    "        loss_type,\n",
    "        rescale_timesteps=False,\n",
    "    ):\n",
    "        self.model_mean_type = model_mean_type\n",
    "        self.model_var_type = model_var_type\n",
    "        self.loss_type = loss_type\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        \n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def _vb_terms_bpd(\n",
    "        self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get a term for the variational lower-bound.\n",
    "        The resulting units are bits (rather than nats, as one might expect).\n",
    "        This allows for comparison to other papers.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'output': a shape [N] tensor of NLLs or KLs.\n",
    "                 - 'pred_xstart': the x_0 predictions.\n",
    "        \"\"\"\n",
    "        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(\n",
    "            x_start=x_start, x_t=x_t, t=t\n",
    "        )\n",
    "        out = self.p_mean_variance(\n",
    "            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n",
    "        )\n",
    "        kl = normal_kl(\n",
    "            true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"]\n",
    "        )\n",
    "        kl = mean_flat(kl) / np.log(2.0)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(\n",
    "            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n",
    "        )\n",
    "        assert decoder_nll.shape == x_start.shape\n",
    "        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n",
    "\n",
    "        # At the first timestep return the decoder NLL,\n",
    "        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = torch.where((t == 0), decoder_nll, kl)\n",
    "        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def training_losses(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_t = self.q_sample(x_start, t, noise=noise)\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        if self.loss_type == LossType.KL or self.loss_type == LossType.RESCALED_KL:\n",
    "            terms[\"loss\"] = self._vb_terms_bpd(\n",
    "                model=model,\n",
    "                x_start=x_start,\n",
    "                x_t=x_t,\n",
    "                t=t,\n",
    "                clip_denoised=False,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )[\"output\"]\n",
    "            if self.loss_type == LossType.RESCALED_KL:\n",
    "                terms[\"loss\"] *= self.num_timesteps\n",
    "        elif self.loss_type == LossType.MSE or self.loss_type == LossType.RESCALED_MSE:\n",
    "            model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "\n",
    "            if self.model_var_type in [\n",
    "                ModelVarType.LEARNED,\n",
    "                ModelVarType.LEARNED_RANGE,\n",
    "            ]:\n",
    "                B, C = x_t.shape[:2]\n",
    "                assert model_output.shape == (B, C * 2, *x_t.shape[2:])\n",
    "                model_output, model_var_values = torch.split(model_output, C, dim=1)\n",
    "                # Learn the variance using the variational bound, but don't let\n",
    "                # it affect our mean prediction.\n",
    "                frozen_out = torch.cat([model_output.detach(), model_var_values], dim=1)\n",
    "                terms[\"vb\"] = self._vb_terms_bpd(\n",
    "                    model=lambda *args, r=frozen_out: r,\n",
    "                    x_start=x_start,\n",
    "                    x_t=x_t,\n",
    "                    t=t,\n",
    "                    clip_denoised=False,\n",
    "                )[\"output\"]\n",
    "                if self.loss_type == LossType.RESCALED_MSE:\n",
    "                    # Divide by 1000 for equivalence with initial implementation.\n",
    "                    # Without a factor of 1/1000, the VB term hurts the MSE term.\n",
    "                    terms[\"vb\"] *= self.num_timesteps / 1000.0\n",
    "\n",
    "            target = {\n",
    "                ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(\n",
    "                    x_start=x_start, x_t=x_t, t=t\n",
    "                )[0],\n",
    "                ModelMeanType.START_X: x_start,\n",
    "                ModelMeanType.EPSILON: noise,\n",
    "            }[self.model_mean_type]\n",
    "            assert model_output.shape == target.shape == x_start.shape\n",
    "            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "            if \"vb\" in terms:\n",
    "                terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n",
    "            else:\n",
    "                terms[\"loss\"] = terms[\"mse\"]\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def _prior_bpd(self, x_start):\n",
    "        \"\"\"\n",
    "        Get the prior KL term for the variational lower-bound, measured in\n",
    "        bits-per-dim.\n",
    "        This term can't be optimized, as it only depends on the encoder.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :return: a batch of [N] KL values (in bits), one per batch element.\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
    "        kl_prior = normal_kl(\n",
    "            mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0\n",
    "        )\n",
    "        return mean_flat(kl_prior) / np.log(2.0)\n",
    "\n",
    "    def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute the entire variational lower-bound, measured in bits-per-dim,\n",
    "        as well as other related quantities.\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :param clip_denoised: if True, clip denoised samples.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - total_bpd: the total variational lower-bound, per batch element.\n",
    "                 - prior_bpd: the prior term in the lower-bound.\n",
    "                 - vb: an [N x T] tensor of terms in the lower-bound.\n",
    "                 - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.\n",
    "                 - mse: an [N x T] tensor of epsilon MSEs for each timestep.\n",
    "        \"\"\"\n",
    "        device = x_start.device\n",
    "        batch_size = x_start.shape[0]\n",
    "\n",
    "        vb = []\n",
    "        xstart_mse = []\n",
    "        mse = []\n",
    "        for t in list(range(self.num_timesteps))[::-1]:\n",
    "            t_batch = torch.tensor([t] * batch_size, device=device)\n",
    "            noise = torch.randn_like(x_start)\n",
    "            x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)\n",
    "            # Calculate VLB term at the current timestep\n",
    "            with th.no_grad():\n",
    "                out = self._vb_terms_bpd(\n",
    "                    model,\n",
    "                    x_start=x_start,\n",
    "                    x_t=x_t,\n",
    "                    t=t_batch,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                )\n",
    "            vb.append(out[\"output\"])\n",
    "            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_start) ** 2))\n",
    "            eps = self._predict_eps_from_xstart(x_t, t_batch, out[\"pred_xstart\"])\n",
    "            mse.append(mean_flat((eps - noise) ** 2))\n",
    "\n",
    "        vb = torch.stack(vb, dim=1)\n",
    "        xstart_mse = torch.stack(xstart_mse, dim=1)\n",
    "        mse = torch.stack(mse, dim=1)\n",
    "\n",
    "        prior_bpd = self._prior_bpd(x_start)\n",
    "        total_bpd = vb.sum(dim=1) + prior_bpd\n",
    "        return {\n",
    "            \"total_bpd\": total_bpd,\n",
    "            \"prior_bpd\": prior_bpd,\n",
    "            \"vb\": vb,\n",
    "            \"xstart_mse\": xstart_mse,\n",
    "            \"mse\": mse,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41782ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableSpacedDiffusion(TrainableGaussianDiffusion):\n",
    "    \"\"\"\n",
    "    A diffusion process which can skip steps in a base diffusion process.\n",
    "    :param use_timesteps: a collection (sequence or set) of timesteps from the\n",
    "                          original diffusion process to retain.\n",
    "    :param kwargs: the kwargs to create the base diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_timesteps, **kwargs):\n",
    "        self.use_timesteps = set(use_timesteps)\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = len(kwargs[\"betas\"])\n",
    "\n",
    "        base_diffusion = TrainableGaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n",
    "        last_alpha_cumprod = 1.0\n",
    "        new_betas = []\n",
    "        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            if i in self.use_timesteps:\n",
    "                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "                last_alpha_cumprod = alpha_cumprod\n",
    "                self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.array(new_betas)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def condition_mean(self, cond_fn, *args, **kwargs):\n",
    "        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n",
    "\n",
    "    def condition_score(self, cond_fn, *args, **kwargs):\n",
    "        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel):\n",
    "            return model\n",
    "        return _WrappedModel(\n",
    "            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n",
    "        )\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        # Scaling is done by the wrapped model.\n",
    "        return t\n",
    "\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.original_num_steps = original_num_steps\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        map_tensor = torch.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        if self.rescale_timesteps:\n",
    "            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n",
    "        return self.model(x, new_ts, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee85e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_diffusion(\n",
    "    *,\n",
    "    steps=1000,\n",
    "    learn_sigma=False,\n",
    "    sigma_small=False,\n",
    "    noise_schedule=\"linear\",\n",
    "    use_kl=False,\n",
    "    predict_xstart=False,\n",
    "    rescale_timesteps=False,\n",
    "    rescale_learned_sigmas=False,\n",
    "    timestep_respacing=\"\",\n",
    "):\n",
    "    betas = get_named_beta_schedule(noise_schedule, steps)\n",
    "    if use_kl:\n",
    "        loss_type = LossType.RESCALED_KL\n",
    "    elif rescale_learned_sigmas:\n",
    "        loss_type = LossType.RESCALED_MSE\n",
    "    else:\n",
    "        loss_type = LossType.MSE\n",
    "    if not timestep_respacing:\n",
    "        timestep_respacing = [steps]\n",
    "    return TrainableSpacedDiffusion(\n",
    "        use_timesteps=space_timesteps(steps, timestep_respacing),\n",
    "        betas=betas,\n",
    "        model_mean_type=(\n",
    "            ModelMeanType.EPSILON if not predict_xstart else ModelMeanType.START_X\n",
    "        ),\n",
    "        model_var_type=(\n",
    "            (\n",
    "                ModelVarType.FIXED_LARGE\n",
    "                if not sigma_small\n",
    "                else ModelVarType.FIXED_SMALL\n",
    "            )\n",
    "            if not learn_sigma\n",
    "            else ModelVarType.LEARNED_RANGE\n",
    "        ),\n",
    "        loss_type=loss_type,\n",
    "        rescale_timesteps=rescale_timesteps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ac4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_diffusion(\n",
    "    image_size,\n",
    "    num_channels,\n",
    "    num_res_blocks,\n",
    "    channel_mult,\n",
    "    num_heads,\n",
    "    num_head_channels,\n",
    "    num_heads_upsample,\n",
    "    attention_resolutions,\n",
    "    dropout,\n",
    "    text_ctx,\n",
    "    xf_width,\n",
    "    xf_layers,\n",
    "    xf_heads,\n",
    "    xf_final_ln,\n",
    "    xf_padding,\n",
    "    diffusion_steps,\n",
    "    noise_schedule,\n",
    "    timestep_respacing,\n",
    "    use_scale_shift_norm,\n",
    "    resblock_updown,\n",
    "    use_fp16,\n",
    "    cache_text_emb,\n",
    "    inpaint,\n",
    "    super_res,\n",
    "):\n",
    "    '''\n",
    "    https://github.com/openai/glide-text2im/blob/9cc8e563851bd38f5ddb3e305127192cb0f02f5c/glide_text2im/model_creation.py#L54\n",
    "    '''\n",
    "    model = create_model(\n",
    "        image_size,\n",
    "        num_channels,\n",
    "        num_res_blocks,\n",
    "        channel_mult=channel_mult,\n",
    "        attention_resolutions=attention_resolutions,\n",
    "        num_heads=num_heads,\n",
    "        num_head_channels=num_head_channels,\n",
    "        num_heads_upsample=num_heads_upsample,\n",
    "        use_scale_shift_norm=use_scale_shift_norm,\n",
    "        dropout=dropout,\n",
    "        text_ctx=text_ctx,\n",
    "        xf_width=xf_width,\n",
    "        xf_layers=xf_layers,\n",
    "        xf_heads=xf_heads,\n",
    "        xf_final_ln=xf_final_ln,\n",
    "        xf_padding=xf_padding,\n",
    "        resblock_updown=resblock_updown,\n",
    "        use_fp16=use_fp16,\n",
    "        cache_text_emb=cache_text_emb,\n",
    "        inpaint=inpaint,\n",
    "        super_res=super_res,\n",
    "    )\n",
    "    diffusion = create_gaussian_diffusion(\n",
    "        steps=diffusion_steps,\n",
    "        noise_schedule=noise_schedule,\n",
    "        timestep_respacing=timestep_respacing,\n",
    "    )\n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8332dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "    **options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c95dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.Resize(img_size),\n",
    "     transforms.RandomCrop(img_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    ")\n",
    "valid_transform = transforms.Compose(\n",
    "    [transforms.Resize(img_size),\n",
    "     transforms.CenterCrop(img_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
    ")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# dataloader args\n",
    "parser.add_argument('--data-dir', type=str, default=\"../data\")\n",
    "parser.add_argument('--train-filename', type=str, default=\"../data/train.txt\")\n",
    "parser.add_argument('--valid-filename', type=str, default=\"../data/validation.txt\")\n",
    "parser.add_argument('--img-key', type=str, default=\"images\")\n",
    "parser.add_argument('--caption-key', type=str, default=\"captions\")\n",
    "parser.add_argument(\"--csv-separator\", type=str, default=\" \")\n",
    "parser.add_argument('--train-batch_size', type=int, default=4)\n",
    "parser.add_argument('--valid-batch-size', type=int, default=4)\n",
    "# transforms.ToTensor() changes input format from H x W x C to C x H x W\n",
    "# don't use below argument if input format required is C x H x W\n",
    "# use it to convert back from C x H x W\n",
    "# vqgan_jax requires H x W x C format, so set it to run this script\n",
    "parser.add_argument('--permute', action='store_true')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = model.tokenizer.encode(text)\n",
    "    tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
    "        tokens, options['text_ctx']\n",
    "    )\n",
    "    cond = {'tokens': tokens, 'mask': mask}\n",
    "    return cond\n",
    "\n",
    "data = get_data(args, (train_transform, valid_transform), 'glide', tokenize=tokenize)\n",
    "data.setup()\n",
    "data = iter(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ba422cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_sampler = create_named_schedule_sampler(\"uniform\", diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d52084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    data_dir=\"\",\n",
    "    schedule_sampler=\"uniform\",\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.0,\n",
    "    lr_anneal_steps=0,\n",
    "    batch_size=1,\n",
    "    microbatch=-1,  # -1 disables microbatches\n",
    "    ema_rate=\"0.9999\",  # comma-separated list of EMA values\n",
    "    log_interval=10,\n",
    "    save_interval=10000,\n",
    "    resume_checkpoint=\"\",\n",
    "    use_fp16=options['use_fp16'],\n",
    "    fp16_scale_growth=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b6a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2022-02-26-06-33-26-272237\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.05 GiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 753.44 MiB free; 8.84 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39mconfigure()\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(dist_util\u001b[38;5;241m.\u001b[39mdev())\n\u001b[0;32m----> 6\u001b[0m \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmicrobatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmicrobatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mema_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_interval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresume_checkpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_fp16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfp16_scale_growth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_anneal_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/data/andrewbai/Natural-Disaster-Image-Generation-to-raise-Environmental-Awareness/guided-diffusion/guided_diffusion/train_util.py:159\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_anneal_steps\n\u001b[1;32m    157\u001b[0m ):\n\u001b[1;32m    158\u001b[0m     batch, cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    161\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdumpkvs()\n",
      "File \u001b[0;32m/nfs/data/andrewbai/Natural-Disaster-Image-Generation-to-raise-Environmental-Awareness/guided-diffusion/guided_diffusion/train_util.py:173\u001b[0m, in \u001b[0;36mTrainLoop.run_step\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, cond):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     took_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp_trainer\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m took_step:\n",
      "File \u001b[0;32m/nfs/data/andrewbai/Natural-Disaster-Image-Generation-to-raise-Environmental-Awareness/guided-diffusion/guided_diffusion/train_util.py:203\u001b[0m, in \u001b[0;36mTrainLoop.forward_backward\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mddp_model\u001b[38;5;241m.\u001b[39mno_sync():\n\u001b[0;32m--> 203\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_sampler, LossAwareSampler):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_sampler\u001b[38;5;241m.\u001b[39mupdate_with_local_losses(\n\u001b[1;32m    207\u001b[0m         t, losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    208\u001b[0m     )\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mTrainableSpacedDiffusion.training_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_losses\u001b[39m(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     32\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mTrainableGaussianDiffusion.training_losses\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m    120\u001b[0m         terms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mMSE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mRESCALED_MSE:\n\u001b[0;32m--> 122\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_var_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    125\u001b[0m         ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED,\n\u001b[1;32m    126\u001b[0m         ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED_RANGE,\n\u001b[1;32m    127\u001b[0m     ]:\n\u001b[1;32m    128\u001b[0m         B, C \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m_WrappedModel.__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale_timesteps:\n\u001b[1;32m     64\u001b[0m     new_ts \u001b[38;5;241m=\u001b[39m new_ts\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1000.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_num_steps)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:799\u001b[0m, in \u001b[0;36mDistributedDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m    798\u001b[0m     inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_kwargs(inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 799\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/glide_text2im/text2im_model.py:134\u001b[0m, in \u001b[0;36mText2ImUNet.forward\u001b[0;34m(self, x, timesteps, tokens, mask)\u001b[0m\n\u001b[1;32m    132\u001b[0m h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[0;32m--> 134\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxf_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    136\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(h, emb, xf_out)\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/glide_text2im/unet.py:35\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, encoder_out)\u001b[0m\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, emb)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, AttentionBlock):\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/glide_text2im/unet.py:245\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x, encoder_out)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_kv(encoder_out)\n\u001b[0;32m--> 245\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(qkv)\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/data/andrewbai/miniconda3/envs/t2i/lib/python3.8/site-packages/glide_text2im/unet.py:281\u001b[0m, in \u001b[0;36mQKVAttention.forward\u001b[0;34m(self, qkv, encoder_kv)\u001b[0m\n\u001b[1;32m    277\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(math\u001b[38;5;241m.\u001b[39msqrt(ch))\n\u001b[1;32m    278\u001b[0m weight \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbct,bcs->bts\u001b[39m\u001b[38;5;124m\"\u001b[39m, q \u001b[38;5;241m*\u001b[39m scale, k \u001b[38;5;241m*\u001b[39m scale\n\u001b[1;32m    280\u001b[0m )  \u001b[38;5;66;03m# More stable with f16 than dividing afterwards\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m weight \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(weight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    282\u001b[0m a \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbts,bcs->bct\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight, v)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m.\u001b[39mreshape(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, length)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.05 GiB (GPU 0; 10.76 GiB total capacity; 8.64 GiB already allocated; 753.44 MiB free; 8.84 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "model = model.to(dist_util.dev())\n",
    "\n",
    "TrainLoop(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    data=data,\n",
    "    batch_size=defaults['batch_size'],\n",
    "    microbatch=defaults['microbatch'],\n",
    "    lr=defaults['lr'],\n",
    "    ema_rate=defaults['ema_rate'],\n",
    "    log_interval=defaults['log_interval'],\n",
    "    save_interval=defaults['save_interval'],\n",
    "    resume_checkpoint=defaults['resume_checkpoint'],\n",
    "    use_fp16=defaults['use_fp16'],\n",
    "    fp16_scale_growth=defaults['fp16_scale_growth'],\n",
    "    schedule_sampler=schedule_sampler,\n",
    "    weight_decay=defaults['weight_decay'],\n",
    "    lr_anneal_steps=defaults['lr_anneal_steps'],\n",
    ").run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f89b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
